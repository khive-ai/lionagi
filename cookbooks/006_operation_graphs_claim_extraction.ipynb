{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 006: Operation Graphs - Academic Claim Validation\n",
    "\n",
    "Building on ReAct patterns from tutorial 005, this demonstrates sequential coordination using Operation Graphs to orchestrate ReaderTool workflows for academic claim validation.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **Sequential Coordination**: Building context step-by-step through dependent operations\n",
    "2. **ReaderTool Integration**: Document chunking and progressive reading strategies  \n",
    "3. **Structured Extraction**: Using Pydantic models for reliable claim extraction\n",
    "4. **Operation Dependencies**: How operations build on previous results\n",
    "\n",
    "## Use Case: Validating a Theoretical Framework Paper\n",
    "\n",
    "We'll validate claims in an academic paper about capability-based security by:\n",
    "- Reading document chunks progressively with ReaderTool\n",
    "- Building understanding through sequential analysis\n",
    "- Extracting verifiable claims with structured output formats\n",
    "- Demonstrating how each operation builds on the previous one's results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Environment setup complete\n",
      "ðŸ“„ Target: 006_lion_proof_ch2.md\n",
      "ðŸŽ¯ Goal: Validate academic claims using coordinated ReAct workflows\n"
     ]
    }
   ],
   "source": [
    "# Setup and imports\n",
    "from typing import Literal\n",
    "from pathlib import Path\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from lionagi import Branch, Session, Builder, types, iModel\n",
    "from lionagi.tools.types import ReaderTool\n",
    "\n",
    "# Target document - complex theoretical framework\n",
    "here = Path().cwd()\n",
    "document_path = here / \"data\" / \"006_lion_proof_ch2.md\"\n",
    "\n",
    "print(\"âœ… Environment setup complete\")\n",
    "print(f\"ðŸ“„ Target: {document_path.name}\")\n",
    "print(\"ðŸŽ¯ Goal: Validate academic claims using coordinated ReAct workflows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Data models defined\n"
     ]
    }
   ],
   "source": [
    "# Data models for structured responses\n",
    "class Claim(BaseModel):\n",
    "    claim: str\n",
    "    type: Literal[\"citation\", \"performance\", \"technical\", \"other\"]\n",
    "    location: str = Field(..., description=\"Section/paragraph reference\")\n",
    "    verifiability: Literal[\"high\", \"medium\", \"low\"]\n",
    "    search_strategy: str = Field(..., description=\"How to verify this claim\")\n",
    "\n",
    "\n",
    "class ClaimExtraction(BaseModel):\n",
    "    claims: list[Claim]\n",
    "\n",
    "\n",
    "print(\"âœ… Data models defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern 1: Sequential Document Analysis\n",
    "\n",
    "Build understanding step-by-step: Open â†’ Analyze â†’ Extract claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”— Executing sequential analysis...\n",
      "Pre-allocated 2 branches\n",
      "Warning: Operation e9e8faff using default branch (not pre-allocated)\n",
      "Executing operation: e9e8faff\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-15 07:50:15,795 - INFO - detected formats: [<InputFormat.MD: 'md'>]\n",
      "2025-10-15 07:50:15,796 - INFO - Going to convert document batch...\n",
      "2025-10-15 07:50:15,796 - INFO - Initializing pipeline for SimplePipeline with options hash 995a146ad601044538e6a923bea22f4e\n",
      "2025-10-15 07:50:15,804 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-10-15 07:50:15,805 - INFO - Registered picture descriptions: ['vlm', 'api']\n",
      "2025-10-15 07:50:15,806 - INFO - Processing document 006_lion_proof_ch2.md\n",
      "2025-10-15 07:50:17,017 - INFO - Finished converting document 006_lion_proof_ch2.md in 1.22 sec.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### ReAct Round No.1 Analysis:\n",
       "\n",
       "```yaml\n",
       "analysis: I will first open the specified markdown document so I can read its contents. After opening, I will read the document in sequential slices (start-to-end or by sections) to understand its structure (headings, subsections, theorems/claims, proofs, citations). For each major section and subsection I will identify and list statements that are verifiable (e.g., explicit theorems, lemmas, propositions, empirical claims with citations, or factual assertions referencing external results). I will also note where claims lack citations or appear to be definitions/conventions (which are not verifiable in the same way). This stepwise approach reduces I/O size and lets me confirm findings incrementally.\n",
       "planned_actions:\n",
       "  - action_type: reader_tool\n",
       "    description: Open the file /Users/lion/projects/lionagi/cookbooks/data/006_lion_proof_ch2.md so its text is available for subsequent partial reads. Rationale: must load the document before reading and analyzing its structure and veri...\n",
       "\n",
       "[Truncated output]\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### ReAct Round No.2 Analysis:\n",
       "\n",
       "```yaml\n",
       "analysis: I will read the opened document in sequential slices and extract its structure (headings, subsections, theorems/lemmas/definitions, proofs, citations) and identify candidate verifiable claims (theorems, propositions, lemmas, empirical/factual assertions that can be checked against literature or computation). To start, I'll read the initial portion of the document (characters 0â€“10000) to detect top-level headings and the first set of claims. After that chunk is analyzed, I'll request further reads until the full document is covered or until all verifiable claims are collected.\n",
       "planned_actions:\n",
       "  - action_type: reader_tool\n",
       "    description: Read the first 10,000 characters of the opened document DOC_6453347028123389634 to identify headings and initial verifiable claims. Rationale: large document (53006 chars) â€” chunked reads reduce I/O and let me extract structure incrementally.\n",
       "extension_needed: True\n",
       "milestone: Read initial chunk (0-10000) and extract headings and candi...\n",
       "\n",
       "[Truncated output]\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### ReAct Round No.3 Analysis:\n",
       "\n",
       "```yaml\n",
       "analysis: |\n",
       "  I opened and read the first 10,000 characters of the document DOC_6453347028123389634. In this chunk I located the chapter title, author/date, Introduction (2.1), System Model and Formal Definitions (2.2), and the beginning of Theorem 2.1 (2.3) with its proof structure. Key structural elements found so far:\n",
       "  \n",
       "  - Top-level headings: 2.1 Introduction, 2.2 System Model and Formal Definitions, 2.3 Theorem 2.1\n",
       "  - Subsections: 2.1.1 Motivation, 2.1.2 Contribution Overview, 2.2.1 Lion Ecosystem Architecture, 2.2.2 Formal System Definition, 2.3.1 Theorem Statement, 2.3.2 Proof Structure\n",
       "  - Formal definitions: Definition 2.1 (Lion Capability System as 7-tuple), Definition 2.2 (Cross-Component Capability 5-tuple), Definition 2.3 (Capability Authority), Definition 2.4 (Component Composition), Definition 2.5 (Security Properties)\n",
       "  - Theorems/lemmas encountered: Theorem 2.1 (Cross-Component Capability Flow), Lemma 2.1.1 (WebAssembly Isolation Preserves Capability Referenc...\n",
       "\n",
       "[Truncated output]\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### ReAct Final Answer:\n",
       "\n",
       "```yaml\n",
       "answer: |\n",
       "  Summary of analysis (reads 0â€“20000):\n",
       "  \n",
       "  - Document structure (observed so far):\n",
       "    - Metadata: Author and date\n",
       "    - 2.1 Introduction (motivation, contributions)\n",
       "    - 2.2 System Model and Formal Definitions\n",
       "      - 2.2.1 Lion Ecosystem Architecture\n",
       "      - 2.2.2 Formal System Definition (Definitions 2.1â€“2.5)\n",
       "    - 2.3 Theorem 2.1: Cross-Component Capability Flow\n",
       "      - Lemma 2.1.1: WebAssembly isolation preserves capability references\n",
       "      - Lemma 2.1.2: Capability transfer protocol preserves authority\n",
       "      - Lemma 2.1.3: Policy compliance during transfer (sketch)\n",
       "      - Mechanization note: TLA+ model (Appendix A.2)\n",
       "    - 2.4 Theorem 2.2: Security Composition\n",
       "      - Lemmas 2.2.1 and 2.2.2\n",
       "      - Mechanization note: Lean encoding\n",
       "    - 2.5 Theorem 2.3: Confused Deputy Prevention (beginning)\n",
       "    - (Remaining content, including Theorem 2.4, appendices, references, and mechanized artifacts, likely beyond offset 20000)\n",
       "  \n",
       "  - Sections that contain verifiable cl...\n",
       "\n",
       "[Truncated output]\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed operation: e9e8faff\n",
      "Operation 8642dd92 waiting for e9e8faff\n",
      "Operation 8642dd92 inherited context from e9e8faff\n",
      "Executing operation: 8642dd92\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### ReAct Round No.1 Analysis:\n",
       "\n",
       "```yaml\n",
       "analysis: |\n",
       "  To identify citations, technical claims, and performance metrics that can be verified, I must focus on key theoretical and technical sections within the document. The previous reading has covered the initial half of the document, including formal definitions and initial theorems with proofs. To gather citations and verify technical claims, especially those related to cryptographic guarantees, protocol correctness, and system performance, it is necessary to read sections that likely contain references (e.g., introduction, related work, appendices, or bibliography) and the parts discussing mechanized proofs or performance evaluation.\n",
       "  \n",
       "  Given the document is about a capability-based security framework with mechanized proofs (TLA+, Lean) and Rust implementation, the key verifiable claims will include theorem formalizations, cryptographic assumptions, and possibly performance benchmarks.\n",
       "  \n",
       "  I plan to:\n",
       "  1. Read the document's latter part, focusing on sections like...\n",
       "\n",
       "[Truncated output]\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### ReAct Round No.2 Analysis:\n",
       "\n",
       "```yaml\n",
       "analysis: |\n",
       "  I have read the last section (offsets 40000â€“53006) including 2.10.3 through 2.11.6 and the bibliography. This section contains detailed citations, explicit technical claims, and performance metrics that can be verified to support the theoretical framework.\n",
       "  \n",
       "  Key findings:\n",
       "  1. Citations: The document extensively cites foundational and modern works in capability-based security, formal verification, cryptographic systems, and implementation case studies. These citations include seminal papers by Dennis & Van Horn (1966), Saltzer & Schroeder (1975), Lampson (1974), Hardy (1988), modern works such as Miller (2006), Klein et al. (2009), and several recent security papers. These references provide verifiable context and foundational support.\n",
       "  \n",
       "  2. Technical Claims: The text claims a strong correspondence between formal theorems and the Rust implementation via phantom types and type-level constraints that enforce security properties at compile time. The cryptographi...\n",
       "\n",
       "[Truncated output]\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### ReAct Final Answer:\n",
       "\n",
       "```yaml\n",
       "topic: Capability-Based Security Framework: Citations, Technical Claims, and Performance Metrics\n",
       "items:\n",
       "  - heading: Foundational Citations\n",
       "    summary: Includes seminal works in capability security and OS protection like Dennis & Van Horn (1966), Saltzer & Schroeder (1975), Lampson (1974), and Hardy (1988), providing the theoretical basis.\n",
       "  - heading: Modern Capability Systems References\n",
       "    summary: Citations of recent advances and implementations including Miller (2006), Shapiro et al. (1999), and Mettler & Wagner (2008, 2010), contextualizing the framework within current research.\n",
       "  - heading: Formal Verification Literature\n",
       "    summary: Key formal methods works are cited, such as Klein et al. (2009) on seL4, and Lean/TLA+ mechanizations supporting the theorems proved in the document.\n",
       "  - heading: Technical Claims in Implementation\n",
       "    summary: Claims on cryptographic integrity (HMAC-SHA256 providing unforgeability and authentication), type-level enforcement of least privil...\n",
       "\n",
       "[Truncated output]\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed operation: 8642dd92\n",
      "Operation 9a6a2769 waiting for 8642dd92\n",
      "Operation 9a6a2769 inherited context from 8642dd92\n",
      "Executing operation: 9a6a2769\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### ReAct Round No.1 Analysis:\n",
       "\n",
       "```yaml\n",
       "analysis: |\n",
       "  Based on the previously read content of the theoretical framework document, I identify the following 6 specific, verifiable claims prioritized by citations, technical assertions, and performance metrics:\n",
       "  \n",
       "  1. Theorem 2.1 states that the authority of capability tokens is preserved across component boundaries and tokens remain cryptographically unforgeable during inter-component communication. This is supported by both formal proofs and a TLA+ mechanized model (Appendix A.2).\n",
       "  \n",
       "  2. The Rust implementation enforces capability integrity and least privilege through phantom types, type-level constraints, and runtime WebAssembly isolation that prevents forgery and enforces authority attenuation safely.\n",
       "  \n",
       "  3. Cryptographic integrity of capabilities is provided by HMAC-SHA256 signatures with 128-bit security, ensuring only Lion core can generate valid capabilities. The implementation includes timestamped signatures and validation code verifiable via cryptographic au...\n",
       "\n",
       "[Truncated output]\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### ReAct Final Answer:\n",
       "\n",
       "```yaml\n",
       "claims:\n",
       "  - claim: Theorem 2.1 states that capability tokens preserve their authority and remain cryptographically unforgeable across component boundaries during inter-component communication, as formalized and mechanized in TLA+ (Appendix A.2).\n",
       "    type: citation\n",
       "    location: Section 2.3 Theorem 2.1 and Appendix A.2\n",
       "    verifiability: high\n",
       "    search_strategy: Review the formal proof and TLA+ mechanization in Appendix A.2; verify correspondence between the model and the theorem statement, and validate assumptions about cryptographic unforgeability.\n",
       "  - claim: The Rust implementation enforces capability integrity and least privilege at compile time using phantom types and type-level constraints, complemented by runtime WebAssembly isolation which prevents forgery of capabilities.\n",
       "    type: technical\n",
       "    location: Section 2.10.4 and 2.11.1 Rust Implementation Architecture\n",
       "    verifiability: high\n",
       "    search_strategy: Examine the Rust code snippets provided, verify the use of pha...\n",
       "\n",
       "[Truncated output]\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed operation: 9a6a2769\n"
     ]
    }
   ],
   "source": [
    "async def sequential_analysis():\n",
    "    \"\"\"Sequential workflow: open â†’ analyze structure â†’ extract claims.\"\"\"\n",
    "\n",
    "    # Create branch with ReaderTool\n",
    "    branch = Branch(\n",
    "        tools=[ReaderTool], chat_model=iModel(model=\"openai/gpt-5-mini\")\n",
    "    )\n",
    "    session = Session(default_branch=branch)\n",
    "    builder = Builder()\n",
    "\n",
    "    # Step 1: Open and understand document\n",
    "    doc_reader = builder.add_operation(\n",
    "        \"ReAct\",\n",
    "        node_id=\"open_document\",\n",
    "        instruct=types.Instruct(\n",
    "            instruction=\"Use ReaderTool to open and analyze the theoretical framework document. Understand its structure and identify sections containing verifiable claims.\",\n",
    "            context={\"document_path\": str(document_path)},\n",
    "        ),\n",
    "        tools=[\"reader_tool\"],\n",
    "        max_extensions=2,\n",
    "        verbose=True,\n",
    "        verbose_length=1000,\n",
    "    )\n",
    "\n",
    "    # Step 2: Progressive content analysis\n",
    "    content_analyzer = builder.add_operation(\n",
    "        \"ReAct\",\n",
    "        node_id=\"analyze_content\",\n",
    "        depends_on=[doc_reader],\n",
    "        instruct=types.Instruct(\n",
    "            instruction=\"Read through key sections to identify citations, technical claims, and performance metrics that can be verified.\"\n",
    "        ),\n",
    "        response_format=types.Outline,\n",
    "        tools=[\"reader_tool\"],\n",
    "        max_extensions=3,\n",
    "        verbose=True,\n",
    "        verbose_length=1000,\n",
    "        inherit_context=True,\n",
    "    )\n",
    "\n",
    "    # Step 3: Extract specific claims\n",
    "    claim_extractor = builder.add_operation(\n",
    "        \"ReAct\",\n",
    "        node_id=\"extract_claims\",\n",
    "        depends_on=[content_analyzer],\n",
    "        instruct=types.Instruct(\n",
    "            instruction=\"Extract 5-7 specific, verifiable claims. Prioritize citations, performance metrics, and technical assertions.\"\n",
    "        ),\n",
    "        response_format=ClaimExtraction,\n",
    "        tools=[\"reader_tool\"],\n",
    "        max_extensions=3,\n",
    "        verbose=True,\n",
    "        verbose_length=1000,\n",
    "        inherit_context=True,\n",
    "    )\n",
    "\n",
    "    # Execute workflow\n",
    "    graph = builder.get_graph()\n",
    "    print(\"ðŸ”— Executing sequential analysis...\")\n",
    "\n",
    "    result = await session.flow(graph, parallel=False, verbose=True)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# Execute sequential analysis\n",
    "result = await sequential_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "## ðŸ“„ Document Structure (8642dd92-099d-494b-8c0d-280943d8096f)\n",
       "\n",
       "**Topic:** Capability-Based Security Framework: Citations, Technical Claims, and Performance Metrics\n",
       "\n",
       "### Key Sections:\n",
       "- **Foundational Citations**: Includes seminal works in capability security and OS protection like Dennis & Van Horn (1966), Saltzer & Schroeder (1975), Lampson (1974), and Hardy (1988), providing the theoretical basis.\n",
       "- **Modern Capability Systems References**: Citations of recent advances and implementations including Miller (2006), Shapiro et al. (1999), and Mettler & Wagner (2008, 2010), contextualizing the framework within current research.\n",
       "- **Formal Verification Literature**: Key formal methods works are cited, such as Klein et al. (2009) on seL4, and Lean/TLA+ mechanizations supporting the theorems proved in the document.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "## ðŸ“‘ Extracted Claims (9a6a2769-c760-4a4d-bd2d-f60fdbb74d47)\n",
       "\n",
       "Found **6** verifiable claims:\n",
       "\n",
       "\n",
       "### 1. [CITATION] Theorem 2.1 states that capability tokens preserve their authority and remain cryptographically unforgeable across component boundaries during inter-component communication, as formalized and mechanized in TLA+ (Appendix A.2).\n",
       "\n",
       "- **Location:** Section 2.3 Theorem 2.1 and Appendix A.2  \n",
       "- **Verifiability:** high\n",
       "- **Search Strategy:** Review the formal proof and TLA+ mechanization in Appendix A.2; verify correspondence between the model and the theorem statement, and validate assumptions about cryptographic unforgeability.\n",
       "\n",
       "\n",
       "### 2. [TECHNICAL] The Rust implementation enforces capability integrity and least privilege at compile time using phantom types and type-level constraints, complemented by runtime WebAssembly isolation which prevents forgery of capabilities.\n",
       "\n",
       "- **Location:** Section 2.10.4 and 2.11.1 Rust Implementation Architecture  \n",
       "- **Verifiability:** high\n",
       "- **Search Strategy:** Examine the Rust code snippets provided, verify the use of phantom types and type constraints, and audit the WebAssembly isolation design and implementation within the lion_core project.\n",
       "\n",
       "\n",
       "### 3. [TECHNICAL] Capability cryptographic integrity is ensured via HMAC-SHA256 signatures providing 128-bit security, with only the Lion core able to generate valid signatures, including timestamped verification to prevent forgery.\n",
       "\n",
       "- **Location:** Section 2.11.2 Cryptographic Implementation Details  \n",
       "- **Verifiability:** high\n",
       "- **Search Strategy:** Audit the HMAC-SHA256 implementation and usage in the Rust code, review cryptographic key management practices, and confirm timestamp usage for replay protection.\n",
       "\n",
       "\n",
       "### 4. [PERFORMANCE] Capability creation, authority verification, and cross-component capability transfer operate with constant-time (O(1)) complexity and microsecond-level latencies, e.g., capability creation latency of 2.3 microseconds and authority verification latency of 0.8 microseconds.\n",
       "\n",
       "- **Location:** Section 2.11.3 Performance Characteristics (performance table)  \n",
       "- **Verifiability:** high\n",
       "- **Search Strategy:** Reproduce performance benchmarks using the Lion implementation environment, confirm measurement methodology aligns with stated complexity and latency values.\n",
       "\n",
       "\n",
       "### 5. [CITATION] Theorem 2.2 proves that secure components composed via compatible interfaces preserve their security properties in the composite system, supported by a mechanized proof in the Lean theorem prover.\n",
       "\n",
       "- **Location:** Section 2.4 Theorem 2.2 and Lean mechanization reference  \n",
       "- **Verifiability:** high\n",
       "- **Search Strategy:** Review the formal proof in the document and inspect the Lean mechanized proof files; verify the assumptions on component interfaces and the preservation of security predicates.\n",
       "\n",
       "\n",
       "### 6. [TECHNICAL] The capability system architecture prevents confused deputy attacks by eliminating ambient authority and requiring explicit capability transfer before privileged actions, formalized in Theorem 2.3.\n",
       "\n",
       "- **Location:** Section 2.5 Theorem 2.3  \n",
       "- **Verifiability:** medium\n",
       "- **Search Strategy:** Examine the formal statement and proof of Theorem 2.3; cross-reference related literature on confused deputy prevention to confirm claimâ€™s theoretical validity.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## âœ… Sequential analysis completed"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Display results\n",
    "for node_id, data in result[\"operation_results\"].items():\n",
    "    if isinstance(data, types.Outline):\n",
    "        md_content = f\"\"\"\n",
    "## ðŸ“„ Document Structure ({node_id})\n",
    "\n",
    "**Topic:** {data.topic}\n",
    "\n",
    "### Key Sections:\n",
    "\"\"\"\n",
    "        for item in data.items[:3]:  # Show first 3\n",
    "            md_content += f\"- **{item.heading}**: {item.summary}\\n\"\n",
    "\n",
    "        display(Markdown(md_content))\n",
    "\n",
    "    elif isinstance(data, ClaimExtraction):\n",
    "        md_content = f\"\"\"\n",
    "## ðŸ“‘ Extracted Claims ({node_id})\n",
    "\n",
    "Found **{len(data.claims)}** verifiable claims:\n",
    "\n",
    "\"\"\"\n",
    "        for i, claim in enumerate(data.claims, 1):\n",
    "            md_content += f\"\"\"\n",
    "### {i}. [{claim.type.upper()}] {claim.claim}\n",
    "\n",
    "- **Location:** {claim.location}  \n",
    "- **Verifiability:** {claim.verifiability}\n",
    "- **Search Strategy:** {claim.search_strategy}\n",
    "\n",
    "\"\"\"\n",
    "        display(Markdown(md_content))\n",
    "\n",
    "display(Markdown(\"## âœ… Sequential analysis completed\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lionagi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
