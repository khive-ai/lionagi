# Copyright (c) 2023 - 2025, HaiyangLi <quantocean.li at gmail dot com>
#
# SPDX-License-Identifier: Apache-2.0
from typing import Literal
from pydantic import Field, BaseModel
from collections.abc import AsyncGenerator
from pydantic import BaseModel
from lionagi.service.endpoints.base import EndPoint





















required_kwargs = {"messages", "model"}

optional_kwargs = {
    "store",
    "reasoning_effort",
    "metadata",
    "frequency_penalty",
    "logit_bias",
    "logprobs",
    "top_logprobs",
    "max_completion_tokens",
    "n",
    "modalities",
    "prediction",
    "audio",
    "presence_penalty",
    "response_format",
    "seed",
    "service_tier",
    "stop",
    "stream",
    "stream_options",
    "temperature",
    "top_p",
    "tools",
    "tool_choice",
    "parallel_tool_calls",
    "user",
}

deprecated_kwargs = {
    "max_tokens",
    "function_call",
    "functions",
}


class OpenAIChatCompletionRequestOptions(BaseModel):

    messages: list[dict] = Field(
        ...,
        description="A list of messages comprising the conversation so far. Depending on the model you use, different message types (modalities) are supported, like text, images, and audio.",
    )

    model: str = Field(
        description="ID of the model to use. See the [model endpoint compatibility table](https://platform.openai.com/docs/models#model-endpoint-compatibility) for details on which models work with the Chat API."
    )
    response_format: type | None = Field(
        None,
        description="An object specifying the format that the model must output.",
    )
    frequency_penalty: float | None = None
    logit_bias: dict | None = Field(
        None,
        description="Modify the likelihood of specified tokens appearing in the completion. Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.",
    )
    logprobs: bool | None = None
    max_completion_tokens: int | None = Field(
        None, validation_alias="max_tokens"
    )
    metadata: dict | None = Field(
        None,
        description="Developer-defined tags and values used for filtering completions in the dashboard.",
    )
    modalities: list[Literal["audio", "text"]] = ["text"]
    parallel_tool_calls: bool = Field(
        True,
        description="Whether to enable parallel function calling during tool use.",
    )
    prediction: dict | None = None
    presence_penalty = None
    reasoning_effort: Literal["low", "medium", "high"] | None = None
    seed: int | None = Field(
        None,
        description=" If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result.",
    )
    stop = None
    store = None
    stream_options = None
    temperature: float | None = None
    tool_choice: str | dict | None = Field(
        None,
        description="""Controls which (if any) tool is called by the model. none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools. Specifying a particular tool via {"type": "function", "function": {"name": "my_function"}} forces the model to call that tool. none is the default when no tools are present. auto is the default if tools are present.""",
    )

    tools = None
    top_logprobs = None
    top_p = None
    user: str = Field(
        None,
        description="A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse",
    )
    extra_headers = None
    extra_query = None
    extra_body: dict | None = None
    timeout: int | None = None


class OpenAIChatCompletionConfig(BaseModel):

    name: str = "openai_chat_completion"
    provider: str = "openai"
    base_url: str = "https://api.openai.com/v1"
    endpoint: str = "chat/completions"
    method: Literal["get", "post", "put", "delete"] = Field("post")
    openai_compatible: bool = True
    required_kwargs: set[str] = required_kwargs
    optional_kwargs: set[str] = optional_kwargs
    deprecated_kwargs: set[str] = deprecated_kwargs
    is_invokeable: bool = True
    is_streamable: bool = True
    requires_tokens: bool = False
    allowed_roles: set[str] = {
        "user",
        "assistant",
        "system",
        "developer",
        "tool",
    }
    request_options: type | None = Field(None, exclude=True)
    requires_api_key: bool = True


CHAT_COMPLETION_CONFIG = {
    "endpoint": "chat/completions",
    "method": "post",
    "requires_tokens": True,
    "openai_compatible": True,
    "is_invokeable": True,
    "is_streamable": True,
}


class OpenAIChatCompletionEndPoint(EndPoint):

    def __init__(
        self,
        config: dict | EndpointConfig,
        api_key: str | None = None,
        organization: str | None = None,
        project: str | None = None,
        base_url: str | None = None,
        websocket_base_url: str | None = None,
        timeout: float = None,
        max_retries: int = 3,
        default_headers: dict[str, str] | None = None,
        default_query: dict[str, object] | None = None,
    ):
        try:
            from openai import AsyncOpenAI
        except ImportError:
            from lionagi.libs.package.imports import check_import

            check_import("openai")

            from openai import AsyncOpenAI

        super().__init__(config)

        self.client = AsyncOpenAI(
            api_key=api_key,
            organization=organization,
            project=project,
            base_url=base_url,
            websocket_base_url=websocket_base_url,
            timeout=timeout,
            max_retries=max_retries,
            default_headers=default_headers,
            default_query=default_query,
        )

    async def _invoke(
        self,
        messages=None,
        model=None,
        audio=None,
        response_format=None,
        frequency_penalty=None,
        function_call=None,
        functions=None,
        logit_bias=None,
        logprobs=None,
        max_completion_tokens=None,
        max_tokens=None,
        metadata=None,
        modalities=None,
        n=None,
        parallel_tool_calls=True,
        prediction=None,
        presence_penalty=None,
        reasoning_effort=None,
        seed=None,
        service_tier=None,
        stop=None,
        store=None,
        stream_options=None,
        temperature=None,
        tool_choice=None,
        tools=None,
        top_logprobs=None,
        top_p=None,
        user=None,
        extra_headers=None,
        extra_query=None,
        extra_body=None,
        timeout=None,
    ) -> BaseModel | dict:
        params = {
            k: v for k, v in locals().items() if v is not None and k != "self"
        }
        if response_format:
            return await self.client.beta.chat.completions.parse(**params)
        return await self.client.chat.completions.create(**params)

    async def _stream(
        self,
        messages=None,
        model=None,
        audio=None,
        response_format=None,
        frequency_penalty=None,
        function_call=None,
        functions=None,
        logit_bias=None,
        logprobs=None,
        max_completion_tokens=None,
        max_tokens=None,
        metadata=None,
        modalities=None,
        n=None,
        parallel_tool_calls=True,
        prediction=None,
        presence_penalty=None,
        reasoning_effort=None,
        seed=None,
        service_tier=None,
        stop=None,
        store=None,
        stream_options=None,
        temperature=None,
        tool_choice=None,
        tools=None,
        top_logprobs=None,
        top_p=None,
        user=None,
        extra_headers=None,
        extra_query=None,
        extra_body=None,
        timeout=None,
    ) -> AsyncGenerator[BaseModel | dict, None]:
        params = {
            k: v for k, v in locals().items() if v is not None and k != "self"
        }
        async with self.client.beta.chat.completions.stream(
            **params
        ) as stream:
            async for chunk in stream:
                yield chunk







class OpenAIChatCompletionEndPoint(ChatCompletionEndPoint):
    """
    Documentation: https://platform.openai.com/docs/api-reference/chat/create
    """

    def __init__(self, config: dict = CHAT_COMPLETION_CONFIG):
        super().__init__(config)

    def create_payload(self, **kwargs) -> dict:
        """Generates a request payload (and headers) for this endpoint.

        Args:
            **kwargs:
                Arbitrary parameters passed by the caller.

        Returns:
            dict:
                A dictionary containing:
                - "payload": A dict with filtered parameters for the request.
                - "headers": A dict of additional headers (e.g., `Authorization`).
                - "is_cached": Whether the request is to be cached.
        """
        payload = {}
        is_cached = kwargs.get("is_cached", False)
        headers = kwargs.get("headers", {})
        for k, v in kwargs.items():
            if k in self.acceptable_kwargs:
                payload[k] = v
        if "api_key" in kwargs:
            headers["Authorization"] = f"Bearer {kwargs['api_key']}"

        if payload.get("model") in ["o1", "o1-2024-12-17"]:
            payload.pop("temperature", None)
            payload.pop("top_p", None)
            if payload["messages"][0].get("role") == "system":
                payload["messages"][0]["role"] = "developer"
        else:
            payload.pop("reasoning_effort", None)

        return {
            "payload": payload,
            "headers": headers,
            "is_cached": is_cached,
        }
