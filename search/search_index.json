{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"LionAGI","text":"<p>Build AI workflows you can trust by coordinating multiple agents</p>"},{"location":"#the-problem","title":"The Problem","text":"<p>AI reasoning is a black box, but AI workflows don't have to be.</p> <p>When you ask an AI agent a complex question, you get one answer. But how do you know it's right? How do you know it considered all angles? Those \"reasoning traces\" you see? They're just generated text, not actual thinking.</p> <p>LionAGI solves this by making AI workflows observable. Instead of trusting what one model tells you about its thinking, you orchestrate multiple specialists and see exactly what each one does.</p> <p>Read our full problem statement \u2192</p>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install lionagi\n</code></pre> <p>Set your API keys (use any or all): <pre><code># OpenAI for GPT models\nexport OPENAI_API_KEY=your-key\n\n# NVIDIA NIM for Llama, Mistral (1000 free credits)\nexport NVIDIA_NIM_API_KEY=nvapi-your-key  # Get at build.nvidia.com\n\n# Claude Code for workspace-aware agents\n# Configured via Claude Code desktop app\n</code></pre></p>"},{"location":"#your-first-observable-workflow","title":"Your First Observable Workflow","text":"<p>Here's the simplest example - getting multiple perspectives using different providers:</p> <pre><code>from lionagi import Branch, iModel\nimport asyncio\n\nasync def main():\n    # Use different models for different strengths\n\n    # OpenAI GPT-4.1 for analysis (1M token context)\n    analyst = Branch(\n        system=\"You analyze business opportunities\",\n        chat_model=iModel(provider=\"openai\", model=\"gpt-4.1\")\n    )\n\n    # NVIDIA NIM with DeepSeek V3.1 for risk assessment (latest preview model)\n    critic = Branch(\n        system=\"You identify risks and challenges\",\n        chat_model=iModel(provider=\"nvidia_nim\", model=\"deepseek-ai/deepseek-v3.1\")\n    )\n\n    # Claude Code for implementation planning (workspace-aware)\n    planner = Branch(\n        system=\"You create actionable implementation plans\",\n        chat_model=iModel(provider=\"claude_code\", endpoint=\"query_cli\")\n    )\n\n    # Ask all three about the same decision\n    question = \"Should our startup expand to Europe?\"\n\n    # Parallel execution - all models work simultaneously\n    analysis, risks, plan = await asyncio.gather(\n        analyst.chat(question),\n        critic.chat(question),\n        planner.chat(question)\n    )\n\n    print(\"Analysis (GPT-4.1):\", analysis)\n    print(\"Risks (DeepSeek V3.1):\", risks)\n    print(\"Plan (Claude Code):\", plan)\n    # Every perspective is visible, using the best model for each task\n\nasyncio.run(main())\n</code></pre>"},{"location":"#available-models","title":"Available Models","text":"<p>OpenAI: GPT-5, GPT-4.1, GPT-4.1-mini, GPT-4o, GPT-4o-mini NVIDIA NIM: DeepSeek V3.1 (latest preview), Llama 3.2 Vision, Mistral Large, Mixtral 8x22B Claude Code: Workspace-aware development with file access Also supported: Anthropic Claude, Google Gemini, Ollama (local), Groq, Perplexity</p>"},{"location":"#why-observable-workflows-matter","title":"Why Observable Workflows Matter","text":"<ul> <li>Trust through transparency: See every step, not just the final answer</li> <li>Multiple perspectives: Different agents catch different issues</li> <li>Audit trails: Every decision is logged and reproducible</li> <li>No black boxes: You control the workflow, not agent conversations</li> </ul>"},{"location":"#when-to-use-lionagi","title":"When to Use LionAGI","text":"<p>\u2705 Perfect for: - Complex decisions needing multiple perspectives - Production systems requiring audit trails - Workflows where you need to see the reasoning - Coordinating different models for different tasks</p> <p>\u274c Not for: - Simple chatbots - Basic Q&amp;A - Prototypes where you want agents to chat freely</p>"},{"location":"#core-concepts-made-simple","title":"Core Concepts Made Simple","text":"<p>Branches = Agents Each Branch is an independent agent with its own context</p> <p>Explicit &gt; Implicit You control the workflow, not agent conversations</p> <p>Observable &gt; Explainable See what happened, don't trust what models claim</p>"},{"location":"#quick-patterns","title":"Quick Patterns","text":""},{"location":"#get-multiple-perspectives","title":"Get Multiple Perspectives","text":"<pre><code># Parallel analysis from different angles\nresults = await asyncio.gather(\n    technical_agent.chat(\"Technical implications?\"),\n    business_agent.chat(\"Business impact?\"),\n    legal_agent.chat(\"Legal considerations?\")\n)\n# See all perspectives at once\n</code></pre>"},{"location":"#use-the-right-model-for-each-job","title":"Use the Right Model for Each Job","text":"<pre><code># Complex analysis needs powerful model\nresearcher = Branch(chat_model=iModel(provider=\"anthropic\", model=\"claude-3-opus\"))\nresearch = await researcher.chat(\"Deep dive into quantum computing\")\n\n# Simple summary can use cheaper model\nsummarizer = Branch(chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"))\nsummary = await summarizer.chat(f\"Three key points: {research}\")\n</code></pre>"},{"location":"#learning-path","title":"Learning Path","text":"<ol> <li>Start Here \u2192 Your First Flow</li> <li>Understand Why \u2192 Why LionAGI?</li> <li>Learn Basics \u2192 Sessions and Branches</li> <li>Apply Patterns \u2192 Common Workflows</li> <li>Go Deeper \u2192 Advanced Topics</li> </ol>"},{"location":"#the-key-difference","title":"The Key Difference","text":"<pre><code># \u274c Other frameworks: Agents figure it out themselves\nresult = agent_conversation(agent1, agent2, agent3, problem)\n# Who knows what happened?\n\n# \u2705 LionAGI: You orchestrate, agents execute\nstep1 = await analyst.analyze(problem)\nstep2 = await critic.review(step1)\nstep3 = await synthesizer.combine(step1, step2)\n# Every step visible and verifiable\n</code></pre>"},{"location":"#get-started","title":"Get Started","text":"<p>Ready to build? Start with Your First Flow \u2192</p> <p>Have questions? Check our Problem Statement to understand our philosophy</p> <p>Need help? GitHub Issues | Discord</p> <p>LionAGI: Observable workflows for trustworthy AI</p> <p>Apache 2.0 License</p>"},{"location":"DOCUMENTATION_STANDARDS/","title":"LionAGI Documentation Standards","text":"<p>Practical guidelines for writing docs that actually help.</p>"},{"location":"DOCUMENTATION_STANDARDS/#core-principle","title":"Core Principle","text":"<p>Show, don't tell. Every page should have working code that solves a real problem.</p>"},{"location":"DOCUMENTATION_STANDARDS/#page-types-templates","title":"Page Types &amp; Templates","text":""},{"location":"DOCUMENTATION_STANDARDS/#1-pattern-pages-patterns","title":"1. Pattern Pages (<code>/patterns/</code>)","text":"<pre><code># [Pattern Name]\n\nWhen you need to [problem this solves].\n\n## The Pattern\n\n```python\n# Complete working example\nfrom lionagi import Session, Branch, Builder\n\nasync def pattern_name():\n    # Implementation\n    pass\n\n# Usage\nresult = await pattern_name()\n```\n</code></pre>"},{"location":"DOCUMENTATION_STANDARDS/#when-it-works","title":"When It Works","text":"<ul> <li>Scenario 1: [specific use case]</li> <li>Scenario 2: [another use case]</li> </ul>"},{"location":"DOCUMENTATION_STANDARDS/#success-rate","title":"Success Rate","text":"<p>~95% based on [context]</p> <pre><code>### 2. Cookbook Pages (`/cookbook/`)\n\n```markdown\n# [Solution Name]\n\n[One sentence: what this builds]\n\n## Problem\n\n[2-3 sentences on the specific challenge]\n\n## Solution\n\n```python\n# Full implementation\n# Can be 50-200 lines\n# Must be copy-paste ready\n</code></pre>"},{"location":"DOCUMENTATION_STANDARDS/#results","title":"Results","text":"<pre><code>[Actual output from running the code]\n</code></pre>"},{"location":"DOCUMENTATION_STANDARDS/#customization","title":"Customization","text":"<ul> <li>To adapt for X: [change this]</li> <li>To scale up: [modify that]</li> </ul> <pre><code>### 3. Concept Pages (`/core-concepts/`)\n\n```markdown\n# [Concept Name]\n\n[One sentence definition]\n\n## Quick Example\n\n```python\n# Minimal example showing the concept\n</code></pre>"},{"location":"DOCUMENTATION_STANDARDS/#key-points","title":"Key Points","text":"<ul> <li>Point 1: [essential info]</li> <li>Point 2: [essential info]</li> <li>Point 3: [essential info]</li> </ul>"},{"location":"DOCUMENTATION_STANDARDS/#common-usage","title":"Common Usage","text":"<pre><code># Realistic example\n</code></pre> <pre><code>### 4. Quickstart Pages\n\n```markdown\n# [Getting Started with X]\n\n## Install\n\n```bash\nuv add lionagi\n</code></pre>"},{"location":"DOCUMENTATION_STANDARDS/#first-example","title":"First Example","text":"<pre><code># Simplest possible working example\n</code></pre>"},{"location":"DOCUMENTATION_STANDARDS/#next-steps","title":"Next Steps","text":"<ul> <li>Try [pattern]</li> <li>Read about [concept]</li> <li>See [cookbook example]</li> </ul> <pre><code>## Code Standards\n\n### Every Code Block Must:\n\n1. **Run without modification** - Include all imports\n2. **Show realistic usage** - Not just toy examples\n3. **Handle errors gracefully** - At least try/except where it matters\n\n```python\n# GOOD: Complete and runnable\nfrom lionagi import Branch, iModel\nimport asyncio\n\nasync def example():\n    branch = Branch(chat_model=iModel(provider=\"openai\", model=\"gpt-4\"))\n    try:\n        result = await branch.chat(\"Analyze this\")\n        return result\n    except Exception as e:\n        print(f\"Error: {e}\")\n        return None\n\n# Run it\n# result = asyncio.run(example())\n</code></pre> <pre><code># BAD: Fragment without context\nbranch.chat(\"Analyze this\")  # What's branch? How to run?\n</code></pre>"},{"location":"DOCUMENTATION_STANDARDS/#writing-style","title":"Writing Style","text":""},{"location":"DOCUMENTATION_STANDARDS/#keep-it-simple","title":"Keep It Simple","text":"<ul> <li>Short sentences (max 20 words)</li> <li>Active voice (\"Use X to...\" not \"X can be used to...\")</li> <li>Direct instructions (\"Do this\" not \"You might want to consider\")</li> <li>Skip the fluff (No \"In this section we will explore...\")</li> </ul>"},{"location":"DOCUMENTATION_STANDARDS/#show-success-metrics","title":"Show Success Metrics","text":"<p>When claiming something works, show evidence:</p> <ul> <li>\"95% success rate\" not \"usually works\"</li> <li>\"2.3 second average\" not \"fast\"</li> <li>\"Handles 1000 req/sec\" not \"scalable\"</li> </ul>"},{"location":"DOCUMENTATION_STANDARDS/#for-ai-agents","title":"For AI Agents","text":""},{"location":"DOCUMENTATION_STANDARDS/#pattern-recognition-format","title":"Pattern Recognition Format","text":"<p>Help AI agents understand when to use patterns:</p> <pre><code>## When to Use\n\nIF task requires parallel analysis: USE fan-out-in pattern ELIF task needs\nstep-by-step building: USE sequential-analysis pattern ELSE: USE single-branch\nReAct\n</code></pre>"},{"location":"DOCUMENTATION_STANDARDS/#executable-templates","title":"Executable Templates","text":"<p>Provide parameterized code AI can modify:</p> <pre><code>async def orchestrate(roles: list[str], task: str):\n    \"\"\"Template AI agents can adapt.\"\"\"\n    branches = [Branch(system=f\"You are a {role}\") for role in roles]\n    results = await asyncio.gather(*[b.chat(task) for b in branches])\n    return synthesize(results)\n</code></pre>"},{"location":"DOCUMENTATION_STANDARDS/#documentation-workflow","title":"Documentation Workflow","text":""},{"location":"DOCUMENTATION_STANDARDS/#adding-new-docs","title":"Adding New Docs","text":"<ol> <li>Check if needed - Does this solve a new problem?</li> <li>Pick the right type - Pattern, cookbook, concept, or quickstart?</li> <li>Use the template - Don't reinvent the structure</li> <li>Test the code - Every example must run</li> <li>Get it merged - Perfect is the enemy of done</li> </ol>"},{"location":"DOCUMENTATION_STANDARDS/#updating-docs","title":"Updating Docs","text":"<ul> <li>Fix errors immediately - Don't wait</li> <li>Update metrics quarterly - Keep data fresh</li> <li>Add examples from issues - Real problems, real solutions</li> </ul>"},{"location":"DOCUMENTATION_STANDARDS/#quality-checklist","title":"Quality Checklist","text":"<p>Before merging any doc:</p> <ul> <li> Code runs without errors</li> <li> Solves a real problem</li> <li> Uses appropriate template</li> <li> Includes actual output/metrics</li> <li> Links to related content</li> </ul>"},{"location":"DOCUMENTATION_STANDARDS/#what-not-to-document","title":"What NOT to Document","text":"<ul> <li>Obvious things - We have good docstrings</li> <li>Every parameter - API reference handles that</li> <li>Theory without practice - This isn't an academic paper</li> <li>Features not in main - Document what's shipped</li> </ul>"},{"location":"DOCUMENTATION_STANDARDS/#examples-of-good-docs","title":"Examples of Good Docs","text":""},{"location":"DOCUMENTATION_STANDARDS/#good-pattern-doc","title":"Good Pattern Doc","text":"<ul> <li>Clear problem statement</li> <li>Complete working code</li> <li>Success metrics</li> <li>When to use/not use</li> </ul>"},{"location":"DOCUMENTATION_STANDARDS/#good-cookbook-entry","title":"Good Cookbook Entry","text":"<ul> <li>Specific real-world scenario</li> <li>Full implementation</li> <li>Actual results</li> <li>How to customize</li> </ul>"},{"location":"DOCUMENTATION_STANDARDS/#good-concept-page","title":"Good Concept Page","text":"<ul> <li>Simple definition</li> <li>Minimal example</li> <li>Key points only</li> <li>Practical usage</li> </ul>"},{"location":"DOCUMENTATION_STANDARDS/#maintenance","title":"Maintenance","text":""},{"location":"DOCUMENTATION_STANDARDS/#quarterly-review","title":"Quarterly Review","text":"<ul> <li>Update success metrics</li> <li>Fix broken examples</li> <li>Remove outdated patterns</li> <li>Add new proven patterns</li> </ul>"},{"location":"DOCUMENTATION_STANDARDS/#continuous","title":"Continuous","text":"<ul> <li>Fix errors when found</li> <li>Add clarifications from support questions</li> <li>Update for API changes</li> </ul> <p>Remember: If you wouldn't copy-paste it into your own project, don't put it in the docs.</p>"},{"location":"code-of-conduct/","title":"Code of Conduct","text":""},{"location":"code-of-conduct/#our-pledge","title":"Our Pledge","text":"<p>We are committed to providing a welcoming and inclusive environment for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, nationality, personal appearance, race, religion, or sexual identity and orientation.</p>"},{"location":"code-of-conduct/#our-standards","title":"Our Standards","text":"<p>Positive behaviors include:</p> <ul> <li>Using welcoming and inclusive language</li> <li>Being respectful of differing viewpoints and experiences</li> <li>Gracefully accepting constructive criticism</li> <li>Focusing on what is best for the community</li> <li>Showing empathy towards other community members</li> </ul> <p>Unacceptable behaviors include:</p> <ul> <li>Harassment of any kind</li> <li>Discriminatory language or actions</li> <li>Personal attacks or insults</li> <li>Trolling, spamming, or disruptive behavior</li> <li>Publishing others' private information without permission</li> </ul>"},{"location":"code-of-conduct/#our-responsibilities","title":"Our Responsibilities","text":"<p>Community leaders are responsible for clarifying standards of acceptable behavior and will take appropriate action in response to unacceptable behavior.</p>"},{"location":"code-of-conduct/#scope","title":"Scope","text":"<p>This Code of Conduct applies to all community spaces, including:</p> <ul> <li>GitHub repositories and issues</li> <li>Discord channels</li> <li>Community forums</li> <li>Social media interactions</li> <li>In-person events</li> </ul>"},{"location":"code-of-conduct/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement. All complaints will be reviewed and investigated promptly and fairly.</p> <p>Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned with this Code of Conduct.</p>"},{"location":"code-of-conduct/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 2.1.</p> <p>For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq.</p>"},{"location":"contributing/","title":"Contributing to LionAGI","text":"<p>Thank you for your interest in contributing to LionAGI!</p>"},{"location":"contributing/#ways-to-contribute","title":"Ways to Contribute","text":"<ul> <li>Bug Reports: Found an issue? Create a GitHub issue</li> <li>Feature Requests: Have ideas for improvements? Let us know</li> <li>Documentation: Help improve our docs and examples</li> <li>Code: Submit pull requests for bug fixes and features</li> <li>Community: Help others in discussions and forums</li> </ul>"},{"location":"contributing/#development-setup","title":"Development Setup","text":"<ol> <li>Fork the repository</li> <li>Clone your fork</li> <li>Install dependencies: <code>uv sync</code></li> <li>Create a branch: <code>git checkout -b feature/your-feature</code></li> <li>Make changes and add tests</li> <li>Run tests: <code>uv run pytest</code></li> <li>Submit a pull request</li> </ol>"},{"location":"contributing/#code-standards","title":"Code Standards","text":"<ul> <li>Follow Python PEP 8 style guidelines</li> <li>Add type hints to all functions</li> <li>Include docstrings for public APIs</li> <li>Write tests for new functionality</li> <li>Keep pull requests focused and atomic</li> </ul>"},{"location":"contributing/#documentation","title":"Documentation","text":"<ul> <li>Update docs when adding features</li> <li>Include examples in docstrings</li> <li>Test code examples to ensure they work</li> <li>Follow our documentation standards</li> </ul>"},{"location":"contributing/#community-guidelines","title":"Community Guidelines","text":"<ul> <li>Be respectful and inclusive</li> <li>Help newcomers learn</li> <li>Focus on constructive feedback</li> <li>Follow our Code of Conduct</li> </ul>"},{"location":"contributing/#questions","title":"Questions?","text":"<ul> <li>GitHub Discussions for questions</li> <li>GitHub Issues for bugs and features</li> <li>Discord community for real-time chat</li> </ul> <p>We appreciate all contributions, no matter how small!</p>"},{"location":"problem-statement/","title":"The LionAGI Problem Statement","text":""},{"location":"problem-statement/#ai-reasoning-is-a-black-box-but-ai-workflows-dont-have-to-be","title":"AI reasoning is a black box, but AI workflows don't have to be.","text":"<p>Everyone's racing to build agents, but we're solving the wrong problem. The industry is obsessed with making models \"explain their reasoning\" - but those reasoning traces are just generated text, not actual thought processes. They're probabilistic outputs, not deterministic explanations.</p> <p>Meanwhile, enterprises need AI automation but can't get it past security audits. They need to trust AI systems with critical decisions, but they can't see or verify what's happening inside.</p>"},{"location":"problem-statement/#the-reality-check","title":"The Reality Check","text":"<ol> <li>No single model will achieve AGI - Complex intelligence requires multiple specialized models working together</li> <li>\"Reasoning\" models are theater - Those traces don't show actual thinking, just prolonged inference time</li> <li>The biggest model isn't the best model - Different tasks need different capabilities at different costs</li> <li>Agent demos \u2260 Production systems - What works in demos breaks when facing real complexity</li> </ol>"},{"location":"problem-statement/#the-lionagi-insight","title":"The LionAGI Insight","text":"<p>\"Even if we can't explain LLM reasoning, the workflow itself is explainable.\"</p> <p>Trust doesn't come from models explaining themselves (they can't). Trust comes from observing structured workflows where you can: - See every decision point - Verify every data transformation - Audit every agent interaction - Reproduce every outcome</p>"},{"location":"problem-statement/#what-everyone-wants-vs-what-they-need","title":"What Everyone Wants vs What They Need","text":"<p>Want: AI automation that handles complex tasks Need: AI systems they can trust in production</p> <p>Current \"solutions\": - LangChain/LlamaIndex: Kitchen sink frameworks with everything but clarity - AutoGen/CrewAI: Agents chatting in unpredictable conversations - \"Reasoning\" models: Self-reported thinking that's just more generated text</p> <p>What's missing: Observable, deterministic, production-ready orchestration</p>"},{"location":"problem-statement/#the-market-timing","title":"The Market Timing","text":"<ul> <li>2022-2023: \"What's an agent?\" (too early)</li> <li>2024-2025: \"Agents are cool but how do we trust them in production?\" (perfect timing)</li> <li>Enterprise pain: Need AI automation but can't pass security audits with current solutions</li> </ul>"},{"location":"problem-statement/#lionagis-answer","title":"LionAGI's Answer","text":"<p>Instead of trying to make AI explain itself (impossible), make AI workflows observable (achievable).</p> <p>Simple patterns that work: - Parallel specialists with different perspectives - Mandatory critics to catch errors - Artifact coordination for transparency - Cognitive limits that prevent chaos</p> <p>Not \"complexity theater\": - No Byzantine fault tolerance when you don't need it - No category theory abstractions for their own sake - No formal verification overkill - Just observable, reliable workflows</p>"},{"location":"problem-statement/#the-philosophical-shift","title":"The Philosophical Shift","text":""},{"location":"problem-statement/#old-way-trust-the-model","title":"Old way: Trust the model","text":"<p>\"This model says it considered X, Y, and Z in its reasoning\"</p>"},{"location":"problem-statement/#lionagi-way-trust-the-workflow","title":"LionAGI way: Trust the workflow","text":"<p>\"We asked three specialists, had a critic review, and here's exactly what each one did\"</p>"},{"location":"problem-statement/#who-this-is-for","title":"Who This Is For","text":"<ul> <li>Enterprises who need AI automation but can't deploy black boxes</li> <li>Developers tired of agent chaos and unpredictable outcomes</li> <li>Teams who know one model isn't enough for complex problems</li> <li>Organizations who need to explain AI decisions to auditors and regulators</li> </ul>"},{"location":"problem-statement/#the-bottom-line","title":"The Bottom Line","text":"<p>LionAGI doesn't make AI models explainable (nobody can). LionAGI makes AI workflows observable (everybody needs).</p> <p>In a world racing toward AGI with black box models, we're building the glass box that lets you see - and trust - what's actually happening.</p> <p>The best orchestration is embarrassingly simple: parallel specialists + mandatory critics + artifact coordination + cognitive limits. Everything else is complexity theater.</p>"},{"location":"advanced/","title":"Advanced Topics","text":"<p>For Experienced Users</p> <p>This section is designed for developers building production systems, optimizing performance, or extending LionAGI's capabilities. </p> <p>Deep dive into LionAGI's advanced features and production-ready capabilities.</p>"},{"location":"advanced/#what-youll-learn","title":"What You'll Learn","text":"<p>These advanced concepts will help you build robust, scalable, and observable multi-agent systems:</p> <ul> <li>Custom Operations - Build specialized operations   for your workflows</li> <li>Flow Composition - Compose complex multi-agent   flows</li> <li>Performance - Optimize your LionAGI workflows</li> <li>Error Handling - Handle failures gracefully</li> <li>Observability - Monitor and debug your workflows</li> </ul>"},{"location":"advanced/#prerequisites","title":"Prerequisites","text":"<p>Required Knowledge</p> <p>Before diving into advanced topics, make sure you have:</p> <ul> <li>\u2705 Completed the Core Concepts section</li> <li>\u2705 Built workflows using Patterns </li> <li>\u2705 Experience with Python async/await programming</li> <li>\u2705 Understanding of multi-agent coordination concepts</li> </ul>"},{"location":"advanced/#when-to-use-advanced-features","title":"When to Use Advanced Features","text":"<p>You Need Advanced Topics If:</p> <ul> <li>Building production systems with specific performance requirements</li> <li>Need custom operations beyond the built-in types</li> <li>Want detailed monitoring and observability </li> <li>Handling complex error scenarios and failures</li> <li>Integrating with enterprise systems and databases</li> </ul>"},{"location":"advanced/#next-steps","title":"Next Steps","text":"<p>After Mastering Advanced Topics</p> <ul> <li>Integrations - Connect with databases, tools, and services</li> <li>Migration Guides - If you're coming from other frameworks  </li> <li>For AI Agents - Special guidance for AI-powered development</li> </ul>"},{"location":"advanced/custom-operations/","title":"Custom Operations","text":"<p>Creating specialized operations for LionAGI workflows.</p>"},{"location":"advanced/custom-operations/#core-concept","title":"Core Concept","text":"<p>Operations are building blocks that branches execute. LionAGI provides built-in operations like <code>chat</code>, <code>communicate</code>, <code>operate</code>, and <code>ReAct</code>, but you can create custom ones for specialized tasks.</p>"},{"location":"advanced/custom-operations/#built-in-operations","title":"Built-in Operations","text":"<ul> <li>chat: Basic conversation</li> <li>communicate: Stateful conversation</li> <li>operate: Structured output with Pydantic</li> <li>ReAct: Reasoning with tools</li> </ul>"},{"location":"advanced/custom-operations/#creating-custom-operations","title":"Creating Custom Operations","text":""},{"location":"advanced/custom-operations/#function-based-operations","title":"Function-Based Operations","text":"<p>Define async functions for custom behavior:</p> <pre><code>from lionagi import Branch, Builder, Session\n\nasync def summarize_with_keywords(branch: Branch, instruction: str, keywords: list = None, **kwargs):\n    \"\"\"Custom operation that emphasizes specific keywords.\"\"\"\n    keyword_text = f\"Focus on: {', '.join(keywords or [])}\"\n    enhanced_instruction = f\"{instruction}\\n\\n{keyword_text}\"\n\n    return await branch.chat(enhanced_instruction, **kwargs)\n\n# Usage in workflow\nsession = Session()\nbuilder = Builder(\"custom_workflow\")\n\nnode = builder.add_operation(\n    operation=summarize_with_keywords,\n    instruction=\"Summarize this research paper\",\n    keywords=[\"machine learning\", \"performance\"]\n)\n\nresult = await session.flow(builder.get_graph())\n</code></pre>"},{"location":"advanced/custom-operations/#class-based-operations","title":"Class-Based Operations","text":"<p>For stateful operations, use classes:</p> <pre><code>class DataAnalysisOperation:\n    def __init__(self, analysis_type: str = \"descriptive\"):\n        self.analysis_type = analysis_type\n\n    async def __call__(self, branch: Branch, data_context: str, **kwargs):\n        instruction = f\"\"\"\n        Perform {self.analysis_type} analysis on: {data_context}\n\n        Provide:\n        1. Key findings\n        2. Statistical insights  \n        3. Recommendations\n        \"\"\"\n\n        return await branch.chat(instruction, **kwargs)\n\n# Usage\nanalyzer = DataAnalysisOperation(\"predictive\")\n\nbuilder.add_operation(\n    operation=analyzer,\n    data_context=\"Sales data Q1-Q3 2024\"\n)\n</code></pre>"},{"location":"advanced/custom-operations/#integration-patterns","title":"Integration Patterns","text":""},{"location":"advanced/custom-operations/#sequential-dependencies","title":"Sequential Dependencies","text":"<pre><code># Operations run in order\ndata_load = builder.add_operation(\n    operation=\"communicate\",\n    instruction=\"Load and validate dataset\"\n)\n\nanalyze = builder.add_operation(\n    operation=analyzer,\n    depends_on=[data_load],\n    data_context=\"Use loaded data\"\n)\n</code></pre>"},{"location":"advanced/custom-operations/#parallel-with-aggregation","title":"Parallel with Aggregation","text":"<pre><code># Multiple analyses run in parallel\nanalysis_nodes = []\nfor analysis_type in [\"descriptive\", \"predictive\"]:\n    node = builder.add_operation(\n        operation=DataAnalysisOperation(analysis_type),\n        data_context=\"Sales data\"\n    )\n    analysis_nodes.append(node)\n\n# Combine results\nsummary = builder.add_aggregation(\n    operation=\"communicate\",\n    source_node_ids=analysis_nodes,\n    instruction=\"Combine analyses into executive summary\"\n)\n</code></pre>"},{"location":"advanced/custom-operations/#best-practices","title":"Best Practices","text":"<ul> <li>Keep operations focused on single responsibilities</li> <li>Use existing LionAGI operations (<code>chat</code>, <code>communicate</code>) when possible</li> <li>Handle errors gracefully with try/catch in custom logic</li> <li>Test operations independently before integrating into workflows</li> </ul>"},{"location":"advanced/error-handling/","title":"Error Handling","text":"<p>Handle failures gracefully in LionAGI workflows.</p>"},{"location":"advanced/error-handling/#basic-error-handling","title":"Basic Error Handling","text":"<p>Catch and handle common errors:</p> <pre><code>from lionagi import Branch, iModel\n\ntry:\n    # Invalid model configuration\n    branch = Branch(\n        chat_model=iModel(provider=\"invalid\", model=\"gpt-4\")\n    )\n    result = await branch.communicate(\"Analyze market trends\")\n\nexcept Exception as e:\n    print(f\"Error: {e}\")\n    # Fallback to working model\n    branch = Branch(\n        chat_model=iModel(provider=\"openai\", model=\"gpt-4\")\n    )\n    result = await branch.communicate(\"Analyze market trends\")\n</code></pre>"},{"location":"advanced/error-handling/#retry-pattern","title":"Retry Pattern","text":"<p>Retry failed operations with backoff:</p> <pre><code>import asyncio\nfrom lionagi import Branch, iModel\n\nbranch = Branch(chat_model=iModel(provider=\"openai\", model=\"gpt-4\"))\n\nasync def safe_communicate(prompt: str, max_retries: int = 3):\n    for attempt in range(max_retries):\n        try:\n            return await branch.communicate(prompt)\n        except Exception as e:\n            print(f\"Attempt {attempt + 1} failed: {e}\")\n            if attempt == max_retries - 1:\n                raise\n            await asyncio.sleep(2 ** attempt)  # Exponential backoff\n\nresult = await safe_communicate(\"Analyze market trends\")\n</code></pre>"},{"location":"advanced/error-handling/#fallback-pattern","title":"Fallback Pattern","text":"<p>Try multiple models as fallbacks:</p> <pre><code>from lionagi import Branch, iModel\n\nmodels = [\n    {\"provider\": \"openai\", \"model\": \"gpt-4\"},\n    {\"provider\": \"anthropic\", \"model\": \"claude-3-5-sonnet-20240620\"}\n]\n\nprompt = \"What are current AI trends?\"\n\nfor i, config in enumerate(models):\n    try:\n        branch = Branch(chat_model=iModel(**config))\n        result = await branch.communicate(prompt)\n        print(f\"\u2713 Success with model {i+1}\")\n        break\n    except Exception as e:\n        print(f\"\u2717 Model {i+1} failed: {e}\")\n        if i == len(models) - 1:\n            raise Exception(\"All models failed\")\n</code></pre>"},{"location":"advanced/error-handling/#workflow-fallbacks","title":"Workflow Fallbacks","text":"<p>Simplify workflows when complex ones fail:</p> <pre><code>from lionagi import Session, Builder, Branch, iModel\n\nasync def complex_workflow():\n    session = Session()\n    builder = Builder(\"analysis\")\n\n    branch = Branch(chat_model=iModel(provider=\"openai\", model=\"gpt-4\"))\n    session.include_branches([branch])\n\n    step1 = builder.add_operation(\n        \"communicate\", branch=branch,\n        instruction=\"Research market trends\"\n    )\n    step2 = builder.add_operation(\n        \"communicate\", branch=branch,\n        instruction=\"Analyze competition\",\n        depends_on=[step1]\n    )\n\n    return await session.flow(builder.get_graph())\n\nasync def simple_workflow():\n    branch = Branch(chat_model=iModel(provider=\"openai\", model=\"gpt-4\"))\n    result = await branch.communicate(\"Provide market and competitive analysis\")\n    return {\"operation_results\": {\"analysis\": result}}\n\n# Try complex, fallback to simple\ntry:\n    result = await complex_workflow()\nexcept Exception as e:\n    print(f\"Complex workflow failed: {e}\")\n    result = await simple_workflow()\n</code></pre>"},{"location":"advanced/error-handling/#partial-results","title":"Partial Results","text":"<p>Accept partial success when some operations fail:</p> <pre><code>from lionagi import Branch, iModel\n\nbranch = Branch(chat_model=iModel(provider=\"openai\", model=\"gpt-4\"))\n\ntasks = [\"market analysis\", \"competitor research\", \"risk assessment\"]\nresults = []\n\nfor task in tasks:\n    try:\n        result = await branch.communicate(f\"Brief {task}\")\n        results.append({\"task\": task, \"result\": result, \"status\": \"success\"})\n        print(f\"\u2713 {task} completed\")\n    except Exception as e:\n        results.append({\"task\": task, \"error\": str(e), \"status\": \"failed\"})\n        print(f\"\u2717 {task} failed: {e}\")\n\n# Use partial results if sufficient\nsuccessful = [r for r in results if r[\"status\"] == \"success\"]\nif len(successful) &gt;= 2:\n    print(f\"Using {len(successful)} successful results\")\nelse:\n    print(\"Too many failures to proceed\")\n</code></pre>"},{"location":"advanced/error-handling/#error-tracking","title":"Error Tracking","text":"<p>Track errors for monitoring:</p> <pre><code>from lionagi import Branch, iModel\n\nbranch = Branch(chat_model=iModel(provider=\"openai\", model=\"gpt-4\"))\n\noperations = [\"market analysis\", \"competitor research\", \"risk assessment\"]\nerrors = []\nsuccesses = 0\n\nfor operation in operations:\n    try:\n        result = await branch.communicate(f\"Brief {operation}\")\n        successes += 1\n        print(f\"\u2713 {operation} completed\")\n    except Exception as e:\n        errors.append({\"operation\": operation, \"error\": str(e)})\n        print(f\"\u2717 {operation} failed: {e}\")\n\ntotal = len(operations)\nerror_rate = len(errors) / total * 100\nprint(f\"Results: {successes}/{total} successful ({error_rate:.1f}% error rate)\")\n</code></pre>"},{"location":"advanced/error-handling/#best-practices","title":"Best Practices","text":"<p>Handle errors at appropriate levels:</p> <pre><code># Operation level: basic try/catch\ntry:\n    result = await branch.communicate(instruction)\nexcept Exception as e:\n    print(f\"Operation failed: {e}\")\n\n# Workflow level: fallbacks\ntry:\n    result = await complex_workflow()\nexcept Exception:\n    result = await simple_fallback()\n</code></pre> <p>Use simple retry logic:</p> <pre><code>for attempt in range(3):\n    try:\n        return await operation()\n    except Exception as e:\n        if attempt == 2:\n            raise\n        await asyncio.sleep(2 ** attempt)\n</code></pre> <p>Accept partial results:</p> <pre><code>results = []\nfor task in tasks:\n    try:\n        result = await process_task(task)\n        results.append(result)\n    except Exception:\n        continue\n\nif len(results) &gt;= minimum_required:\n    return results\n</code></pre> <p>Error handling focuses on practical patterns: basic try/catch blocks, retry logic, fallback strategies, and accepting partial results.</p>"},{"location":"advanced/flow-composition/","title":"Flow Composition","text":"<p>Building complex multi-phase workflows with LionAGI.</p>"},{"location":"advanced/flow-composition/#sequential-flows","title":"Sequential Flows","text":"<p>Chain operations with dependencies:</p> <pre><code>from lionagi import Session, Builder, Branch, iModel\n\nsession = Session()\nbuilder = Builder(\"sequential_analysis\")\n\nbranch = Branch(chat_model=iModel(provider=\"openai\", model=\"gpt-4\"))\nsession.include_branches([branch])\n\n# Sequential steps\nresearch = builder.add_operation(\n    \"communicate\",\n    branch=branch,\n    instruction=\"Research AI market trends\"\n)\n\nanalysis = builder.add_operation(\n    \"communicate\", \n    branch=branch,\n    instruction=\"Analyze the research findings\",\n    depends_on=[research]\n)\n\nrecommendations = builder.add_operation(\n    \"communicate\",\n    branch=branch,\n    instruction=\"Provide recommendations\",\n    depends_on=[analysis]\n)\n\nresult = await session.flow(builder.get_graph())\n</code></pre>"},{"location":"advanced/flow-composition/#parallel-flows","title":"Parallel Flows","text":"<p>Execute independent operations simultaneously:</p> <pre><code>session = Session()\nbuilder = Builder(\"parallel_analysis\")\n\nbranch = Branch(chat_model=iModel(provider=\"openai\", model=\"gpt-4\"))\nsession.include_branches([branch])\n\n# Parallel operations\nmarket_op = builder.add_operation(\n    \"communicate\",\n    branch=branch,\n    instruction=\"Analyze market conditions\"\n)\n\ncompetitor_op = builder.add_operation(\n    \"communicate\",\n    branch=branch, \n    instruction=\"Analyze competitors\"\n)\n\ntech_op = builder.add_operation(\n    \"communicate\",\n    branch=branch,\n    instruction=\"Analyze technology trends\"\n)\n\n# Aggregate results\nsynthesis = builder.add_aggregation(\n    \"communicate\",\n    branch=branch,\n    source_node_ids=[market_op, competitor_op, tech_op],\n    instruction=\"Synthesize all analyses\"\n)\n\nresult = await session.flow(builder.get_graph(), max_concurrent=3)\n</code></pre>"},{"location":"advanced/flow-composition/#multi-phase-workflows","title":"Multi-Phase Workflows","text":"<p>Mix sequential and parallel patterns:</p> <pre><code># Phase 1: Initial research\ninitial_research = builder.add_operation(\n    \"communicate\",\n    branch=branch,\n    instruction=\"Initial market research\"\n)\n\n# Phase 2: Parallel analysis (depends on Phase 1)\nfinancial_analysis = builder.add_operation(\n    \"communicate\",\n    branch=branch,\n    instruction=\"Financial analysis based on research\",\n    depends_on=[initial_research]\n)\n\nmarket_analysis = builder.add_operation(\n    \"communicate\", \n    branch=branch,\n    instruction=\"Market analysis based on research\",\n    depends_on=[initial_research]\n)\n\n# Phase 3: Synthesis (depends on Phase 2)\nfinal_report = builder.add_aggregation(\n    \"communicate\",\n    branch=branch,\n    source_node_ids=[financial_analysis, market_analysis],\n    instruction=\"Create comprehensive report\"\n)\n\nresult = await session.flow(builder.get_graph())\n</code></pre>"},{"location":"advanced/flow-composition/#context-inheritance","title":"Context Inheritance","text":"<p>Pass context between operations:</p> <pre><code># Parent operation\nparent_op = builder.add_operation(\n    \"communicate\",\n    branch=branch,\n    instruction=\"Analyze business requirements\"\n)\n\n# Child operation inherits context\nchild_op = builder.add_operation(\n    \"communicate\",\n    branch=branch,\n    instruction=\"Provide implementation recommendations\",\n    depends_on=[parent_op],\n    inherit_context=True\n)\n\nresult = await session.flow(builder.get_graph())\n</code></pre>"},{"location":"advanced/flow-composition/#multi-branch-flows","title":"Multi-Branch Flows","text":"<p>Use specialized branches for different tasks:</p> <pre><code># Specialized branches\nresearcher = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4\"),\n    system=\"Research specialist\"\n)\n\nanalyst = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4\"),\n    system=\"Data analyst\"\n)\n\nwriter = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4\"),\n    system=\"Report writer\"\n)\n\nsession.include_branches([researcher, analyst, writer])\n\n# Sequential workflow with different branches\nresearch_op = builder.add_operation(\n    \"communicate\",\n    branch=researcher,\n    instruction=\"Research market trends\"\n)\n\nanalysis_op = builder.add_operation(\n    \"communicate\",\n    branch=analyst,\n    instruction=\"Analyze research data\",\n    depends_on=[research_op]\n)\n\nreport_op = builder.add_operation(\n    \"communicate\",\n    branch=writer,\n    instruction=\"Write executive summary\",\n    depends_on=[analysis_op]\n)\n\nresult = await session.flow(builder.get_graph())\n</code></pre>"},{"location":"advanced/flow-composition/#best-practices","title":"Best Practices","text":"<p>Start simple with linear flows before adding complexity:</p> <pre><code># Good: Clear progression\nresearch -&gt; analysis -&gt; report\n\n# Avoid: Over-complex initial design\nresearch -&gt; [analysis1, analysis2, analysis3] -&gt; synthesis -&gt; validation -&gt; report\n</code></pre> <p>Use aggregation to combine parallel results:</p> <pre><code>synthesis = builder.add_aggregation(\n    \"communicate\",\n    source_node_ids=[op1, op2, op3],\n    instruction=\"Combine all results\"\n)\n</code></pre> <p>Control concurrency to manage resource usage:</p> <pre><code>result = await session.flow(\n    builder.get_graph(),\n    max_concurrent=3\n)\n</code></pre> <p>Flow composition enables sophisticated workflows by combining sequential dependencies, parallel execution, and multi-branch coordination.</p>"},{"location":"advanced/observability/","title":"Observability","text":"<p>Monitor and debug LionAGI workflows.</p>"},{"location":"advanced/observability/#verbose-mode","title":"Verbose Mode","text":"<p>Enable detailed logging during development:</p> <pre><code>from lionagi import Session, Builder, Branch, iModel\n\nsession = Session()\nbuilder = Builder(\"debug_test\")\n\nbranch = Branch(chat_model=iModel(provider=\"openai\", model=\"gpt-4\"))\nsession.include_branches([branch])\n\n# Build workflow\nfor topic in [\"research\", \"analysis\", \"synthesis\"]:\n    builder.add_operation(\n        \"communicate\",\n        branch=branch,\n        instruction=f\"Brief {topic} on AI trends\"\n    )\n\n# Execute with verbose logging\nresult = await session.flow(\n    builder.get_graph(),\n    verbose=True  # Shows execution details\n)\n\nprint(f\"Completed: {len(result['completed_operations'])}\")\nprint(f\"Skipped: {len(result['skipped_operations'])}\")\n</code></pre>"},{"location":"advanced/observability/#message-inspection","title":"Message Inspection","text":"<p>Track conversation history:</p> <pre><code>branch = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4\"),\n    system=\"Helpful assistant\"\n)\n\ntopics = [\"AI trends\", \"market analysis\", \"predictions\"]\n\nfor topic in topics:\n    response = await branch.communicate(f\"Brief analysis of {topic}\")\n\n    print(f\"After {topic}:\")\n    print(f\"Total messages: {len(branch.messages)}\")\n\n    # Show recent exchange\n    if len(branch.messages) &gt;= 2:\n        user_msg = branch.messages[-2]\n        assistant_msg = branch.messages[-1]\n        print(f\"User: {user_msg.content[:50]}...\")\n        print(f\"Assistant: {assistant_msg.content[:50]}...\")\n</code></pre>"},{"location":"advanced/observability/#performance-tracking","title":"Performance Tracking","text":"<p>Monitor execution time:</p> <pre><code>import time\n\nsession = Session()\nbuilder = Builder(\"perf_test\")\n\nbranch = Branch(chat_model=iModel(provider=\"openai\", model=\"gpt-4\"))\nsession.include_branches([branch])\n\n# Build operations\nfor i in range(3):\n    builder.add_operation(\n        \"communicate\",\n        branch=branch,\n        instruction=f\"Quick analysis {i+1}\"\n    )\n\n# Execute with timing\nstart_time = time.time()\nresult = await session.flow(builder.get_graph(), verbose=True)\nexecution_time = time.time() - start_time\n\nprint(f\"Execution time: {execution_time:.2f}s\")\nprint(f\"Completed: {len(result['completed_operations'])}\")\n</code></pre>"},{"location":"advanced/observability/#error-monitoring","title":"Error Monitoring","text":"<p>Track failures during execution:</p> <pre><code>branch = Branch(chat_model=iModel(provider=\"openai\", model=\"gpt-4\"))\n\ntest_prompts = [\"Normal request\", \"\", \"Another request\"]\nresults = {\"success\": 0, \"failed\": 0}\n\nfor i, prompt in enumerate(test_prompts):\n    try:\n        result = await branch.communicate(prompt)\n        results[\"success\"] += 1\n        print(f\"\u2713 Request {i+1} succeeded\")\n    except Exception as e:\n        results[\"failed\"] += 1\n        print(f\"\u2717 Request {i+1} failed: {e}\")\n\nprint(f\"Summary: {results['success']} success, {results['failed']} failed\")\n</code></pre>"},{"location":"advanced/observability/#best-practices","title":"Best Practices","text":"<p>Use verbose mode during development:</p> <pre><code># Development\nresult = await session.flow(graph, verbose=True)\n\n# Production  \nresult = await session.flow(graph, verbose=False)\n</code></pre> <p>Track key metrics:</p> <ul> <li>Execution times and success rates</li> <li>Message history and memory usage</li> <li>Error patterns and frequencies</li> </ul> <p>Simple monitoring:</p> <pre><code>print(f\"Completed: {len(result['completed_operations'])}\")\nprint(f\"Skipped: {len(result['skipped_operations'])}\")\n</code></pre> <p>LionAGI provides observability through verbose logging, message inspection, performance tracking, and error monitoring.</p>"},{"location":"advanced/performance/","title":"Performance Optimization","text":"<p>Making LionAGI workflows fast and efficient.</p>"},{"location":"advanced/performance/#parallel-execution","title":"Parallel Execution","text":"<p>Execute multiple branches simultaneously:</p> <pre><code>from lionagi import Session, Branch, iModel\nimport asyncio\n\nsession = Session()\n\n# Multiple branches for parallel processing\nresearcher = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4\"),\n    system=\"Research specialist\"\n)\nanalyst = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4\"),\n    system=\"Data analyst\"\n)\n\nsession.include_branches([researcher, analyst])\n\n# Execute in parallel\nresults = await asyncio.gather(\n    researcher.communicate(\"Research market trends\"),\n    analyst.communicate(\"Analyze competitive landscape\")\n)\n</code></pre>"},{"location":"advanced/performance/#concurrency-control","title":"Concurrency Control","text":"<p>Control parallel execution with <code>max_concurrent</code>:</p> <pre><code>from lionagi import Session, Builder, Branch, iModel\n\nsession = Session()\nbuilder = Builder(\"performance_test\")\n\nbranch = Branch(chat_model=iModel(provider=\"openai\", model=\"gpt-4\"))\nsession.include_branches([branch])\n\n# Create multiple operations\ntopics = [\"AI trends\", \"market analysis\", \"competition\"]\nfor topic in topics:\n    builder.add_operation(\n        \"communicate\",\n        branch=branch,\n        instruction=f\"Brief analysis of {topic}\"\n    )\n\n# Execute with controlled concurrency\nresult = await session.flow(\n    builder.get_graph(),\n    max_concurrent=2  # Only 2 operations at once\n)\n</code></pre>"},{"location":"advanced/performance/#token-efficiency","title":"Token Efficiency","text":"<p>Use appropriate models for different tasks:</p> <pre><code># Light model for simple tasks\nclassifier = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4\"),\n    system=\"Classify content briefly\"\n)\n\n# Powerful model for complex analysis\nanalyzer = Branch(\n    chat_model=iModel(provider=\"anthropic\", model=\"claude-3-5-sonnet-20240620\"),\n    system=\"Provide detailed analysis\"\n)\n\ncontent = \"Sample content to process\"\n\n# Step 1: Quick classification\ncategory = await classifier.communicate(f\"Category (simple/complex): {content}\")\n\n# Step 2: Use appropriate model based on complexity\nif \"complex\" in category.lower():\n    analysis = await analyzer.communicate(f\"Detailed analysis: {content}\")\nelse:\n    analysis = await classifier.communicate(f\"Brief analysis: {content}\")\n</code></pre>"},{"location":"advanced/performance/#batch-processing","title":"Batch Processing","text":"<p>Process multiple items efficiently:</p> <pre><code>branch = Branch(chat_model=iModel(provider=\"openai\", model=\"gpt-4\"))\n\nitems = [f\"item_{i}\" for i in range(20)]\nbatch_size = 5\nresults = []\n\nfor i in range(0, len(items), batch_size):\n    batch = items[i:i + batch_size]\n\n    # Process batch in parallel\n    batch_results = await asyncio.gather(*[\n        branch.communicate(f\"Process: {item}\")\n        for item in batch\n    ])\n\n    results.extend(batch_results)\n</code></pre>"},{"location":"advanced/performance/#memory-management","title":"Memory Management","text":"<p>Clear message history in long workflows:</p> <pre><code>branch = Branch(chat_model=iModel(provider=\"openai\", model=\"gpt-4\"))\n\nlarge_dataset = [f\"document_{i}\" for i in range(100)]\nchunk_size = 10\nresults = []\n\nfor i in range(0, len(large_dataset), chunk_size):\n    chunk = large_dataset[i:i + chunk_size]\n\n    # Process chunk\n    chunk_result = await branch.communicate(\n        f\"Summarize these {len(chunk)} documents: {chunk}\"\n    )\n    results.append(chunk_result)\n\n    # Clear message history to free memory\n    branch.messages.clear()\n</code></pre>"},{"location":"advanced/performance/#best-practices","title":"Best Practices","text":"<p>Choose appropriate patterns for your use case:</p> <pre><code># Simple parallel tasks: Use asyncio.gather()\nresults = await asyncio.gather(*[branch.communicate(task) for task in tasks])\n\n# Complex workflows: Use Builder + session.flow()\nresult = await session.flow(builder.get_graph(), max_concurrent=5)\n</code></pre> <p>Control concurrency based on your needs:</p> <ul> <li>Start with <code>max_concurrent=3-5</code></li> <li>Adjust based on API rate limits</li> <li>Monitor for optimal settings</li> </ul> <p>Optimize token usage:</p> <ul> <li>Use appropriate models for task complexity</li> <li>Clear message history when context not needed</li> <li>Batch similar operations together</li> </ul> <p>Monitor performance:</p> <pre><code># Use verbose mode for insights\nresult = await session.flow(graph, verbose=True)\n\n# Track metrics\nprint(f\"Completed: {len(result['completed_operations'])}\")\n</code></pre> <p>Performance optimization focuses on parallel execution, concurrency control, and efficient resource usage.</p>"},{"location":"comparisons/langgraph/","title":"Migrating from LangGraph to LionAGI","text":"<p>A practical guide for LangGraph users to leverage LionAGI's superior orchestration capabilities.</p>"},{"location":"comparisons/langgraph/#why-consider-migration","title":"Why Consider Migration?","text":"<p>LangGraph is powerful but can be complex for multi-agent workflows. LionAGI offers:</p> <ul> <li>Simpler abstractions: Less boilerplate, cleaner code</li> <li>Parallel by default: Automatic concurrency without complex state   management</li> <li>Better production features: Built-in monitoring, error handling,   performance control</li> <li>Framework agnostic: Orchestrate LangGraph alongside other tools</li> </ul>"},{"location":"comparisons/langgraph/#migration-approaches","title":"Migration Approaches","text":""},{"location":"comparisons/langgraph/#option-1-gradual-migration-recommended","title":"Option 1: Gradual Migration (Recommended)","text":"<p>Keep your existing LangGraph workflows and orchestrate them with LionAGI:</p> <pre><code>from lionagi import Session, Builder\nfrom your_langgraph_code import existing_workflow\n\n# Keep your LangGraph workflow unchanged\nasync def langgraph_research_workflow(branch, query: str, **kwargs):\n    \"\"\"Wrap your existing LangGraph workflow\"\"\"\n    # Your existing LangGraph code - no changes needed!\n    result = await existing_workflow.invoke({\"query\": query})\n    return result[\"output\"]\n\n# Orchestrate with LionAGI\nsession = Session()\nbuilder = Builder(\"hybrid_workflow\")\n\n# Use existing LangGraph workflow as custom operation\nresearch_op = builder.add_operation(\n    operation=langgraph_research_workflow,\n    query=\"Market analysis request\"\n)\n\n# Add pure LionAGI operations\nanalysis_branch = session.new_branch(system=\"Analysis specialist\")\nanalysis_op = builder.add_operation(\n    \"communicate\",\n    branch=analysis_branch,\n    instruction=\"Analyze the research findings\",\n    depends_on=[research_op]\n)\n\nresult = await session.flow(builder.get_graph())\n</code></pre>"},{"location":"comparisons/langgraph/#option-2-direct-translation","title":"Option 2: Direct Translation","text":"<p>Translate LangGraph patterns to LionAGI equivalents:</p>"},{"location":"comparisons/langgraph/#langgraph-supervisor-pattern","title":"LangGraph Supervisor Pattern","text":"<pre><code># LangGraph approach\nclass AgentState(TypedDict):\n    messages: Annotated[Sequence[BaseMessage], add_messages]\n    next: str\n\ndef supervisor_agent(state):\n    # Complex routing logic with manual state management\n    response = llm.invoke(state[\"messages\"])\n    return {\"next\": response.content, \"messages\": [response]}\n\nworkflow = StateGraph(AgentState)\nworkflow.add_node(\"supervisor\", supervisor_agent)\nworkflow.add_conditional_edges(\"supervisor\", route_function, routing_map)\n</code></pre>"},{"location":"comparisons/langgraph/#lionagi-equivalent","title":"LionAGI Equivalent","text":"<pre><code># LionAGI approach - much simpler\nsession = Session()\nbuilder = Builder(\"coordinated_workflow\")\n\n# Create specialized agents\nresearcher = session.new_branch(system=\"Research specialist\")\nanalyst = session.new_branch(system=\"Analysis specialist\") \nwriter = session.new_branch(system=\"Report writer\")\n\n# Sequential workflow with automatic coordination\nresearch_op = builder.add_operation(\n    \"communicate\", branch=researcher,\n    instruction=\"Research the topic thoroughly\"\n)\n\nanalysis_op = builder.add_operation(\n    \"communicate\", branch=analyst,\n    instruction=\"Analyze research findings\", \n    depends_on=[research_op]\n)\n\nreport_op = builder.add_operation(\n    \"communicate\", branch=writer,\n    instruction=\"Write executive summary\",\n    depends_on=[analysis_op]\n)\n\nresult = await session.flow(builder.get_graph())\n</code></pre> <p>Analysis: The LionAGI version eliminates state classes, routing functions, and manual edge configuration. Instead of 15+ lines of setup code, you get a natural workflow that reads like business logic. The <code>depends_on</code> parameter automatically handles execution order, while LangGraph requires explicit edge configuration between every node.</p>"},{"location":"comparisons/langgraph/#common-migration-patterns","title":"Common Migration Patterns","text":""},{"location":"comparisons/langgraph/#1-state-management-memory-management","title":"1. State Management \u2192 Memory Management","text":""},{"location":"comparisons/langgraph/#langgraph-state-handling","title":"LangGraph State Handling","text":"<pre><code>class AgentState(TypedDict):\n    messages: List[BaseMessage]\n    research_data: str\n    analysis_result: str\n\ndef research_node(state: AgentState):\n    # Manual state updates\n    result = research_function()\n    return {\"research_data\": result, \"messages\": state[\"messages\"] + [result]}\n</code></pre>"},{"location":"comparisons/langgraph/#lionagi-memory-management","title":"LionAGI Memory Management","text":"<pre><code># Automatic memory management per branch\nresearcher = Branch(system=\"Research specialist\")\n\n# Memory is handled automatically\nresearch_result = await researcher.communicate(\"Research market trends\")\nfollow_up = await researcher.communicate(\"What are the key risks?\")  # Has context\n</code></pre>"},{"location":"comparisons/langgraph/#2-conditional-routing-dependencies","title":"2. Conditional Routing \u2192 Dependencies","text":""},{"location":"comparisons/langgraph/#langgraph-conditional-logic","title":"LangGraph Conditional Logic","text":"<pre><code>def should_continue(state):\n    if state[\"confidence\"] &gt; 0.8:\n        return \"high_confidence_path\"\n    else:\n        return \"low_confidence_path\"\n\nworkflow.add_conditional_edges(\"analysis\", should_continue, {\n    \"high_confidence_path\": \"final_report\",\n    \"low_confidence_path\": \"additional_research\"\n})\n</code></pre>"},{"location":"comparisons/langgraph/#lionagi-dependency-management","title":"LionAGI Dependency Management","text":"<pre><code># Use simple dependencies and aggregation\ninitial_analysis = builder.add_operation(\"communicate\", instruction=\"Initial analysis\")\n\n# Alternative paths based on results  \ndetailed_research = builder.add_operation(\n    \"communicate\",\n    instruction=\"Conduct detailed research if needed\",\n    depends_on=[initial_analysis]\n)\n\nfinal_report = builder.add_aggregation(\n    \"communicate\",\n    source_node_ids=[initial_analysis, detailed_research],\n    instruction=\"Create final report based on all analysis\"\n)\n</code></pre>"},{"location":"comparisons/langgraph/#3-manual-parallelism-automatic-parallelism","title":"3. Manual Parallelism \u2192 Automatic Parallelism","text":""},{"location":"comparisons/langgraph/#langgraph-parallel-execution","title":"LangGraph Parallel Execution","text":"<pre><code># Complex parallel setup\ndef run_parallel_tasks(state):\n    tasks = [research_task, analysis_task, review_task]\n    # Manual coordination required\n    results = asyncio.gather(*tasks)\n    return {\"results\": results}\n</code></pre>"},{"location":"comparisons/langgraph/#lionagi-automatic-parallelism","title":"LionAGI Automatic Parallelism","text":"<pre><code># Automatic parallel execution - no dependencies = parallel\nresearch_op = builder.add_operation(\"communicate\", instruction=\"Research\")\nanalysis_op = builder.add_operation(\"communicate\", instruction=\"Analyze\") \nreview_op = builder.add_operation(\"communicate\", instruction=\"Review\")\n\n# These run in parallel automatically\n# Synthesis waits for all to complete\nsynthesis = builder.add_aggregation(\n    \"communicate\",\n    source_node_ids=[research_op, analysis_op, review_op],\n    instruction=\"Combine all findings\"\n)\n</code></pre>"},{"location":"comparisons/langgraph/#migration-benefits","title":"Migration Benefits","text":""},{"location":"comparisons/langgraph/#1-reduced-complexity","title":"1. Reduced Complexity","text":"<p>Before (LangGraph):</p> <ul> <li>Manual state management</li> <li>Complex routing logic</li> <li>Verbose graph setup</li> </ul> <p>After (LionAGI):</p> <ul> <li>Automatic memory management</li> <li>Simple dependencies</li> <li>Clean abstractions</li> </ul>"},{"location":"comparisons/langgraph/#2-better-performance","title":"2. Better Performance","text":"<pre><code># LionAGI: Built-in performance controls\nresult = await session.flow(\n    builder.get_graph(),\n    max_concurrent=5,  # Control parallelism\n    verbose=True      # Built-in monitoring\n)\n</code></pre>"},{"location":"comparisons/langgraph/#3-production-features","title":"3. Production Features","text":"<pre><code># LionAGI: Built-in error handling and monitoring\ntry:\n    result = await session.flow(builder.get_graph())\n    print(f\"Completed: {len(result['completed_operations'])}\")\nexcept Exception as e:\n    print(f\"Workflow failed: {e}\")\n    # Built-in fallback handling\n</code></pre>"},{"location":"comparisons/langgraph/#step-by-step-migration-guide","title":"Step-by-Step Migration Guide","text":""},{"location":"comparisons/langgraph/#step-1-analyze-your-langgraph-workflow","title":"Step 1: Analyze Your LangGraph Workflow","text":"<ol> <li>Identify your agents/nodes</li> <li>Map out state dependencies</li> <li>Note any parallel operations</li> <li>Identify conditional logic</li> </ol>"},{"location":"comparisons/langgraph/#step-2-choose-migration-strategy","title":"Step 2: Choose Migration Strategy","text":"<p>For complex workflows: Start with gradual migration (wrap existing code) For simple workflows: Direct translation to LionAGI</p>"},{"location":"comparisons/langgraph/#step-3-create-lionagi-equivalent","title":"Step 3: Create LionAGI Equivalent","text":"<pre><code># Template for most LangGraph migrations\nsession = Session()\nbuilder = Builder(\"migrated_workflow\")\n\n# Create branches for each LangGraph node\nagent1 = session.new_branch(system=\"Agent 1 role\")\nagent2 = session.new_branch(system=\"Agent 2 role\")\n\n# Convert nodes to operations with dependencies\nop1 = builder.add_operation(\"communicate\", branch=agent1, instruction=\"Task 1\")\nop2 = builder.add_operation(\"communicate\", branch=agent2, instruction=\"Task 2\", depends_on=[op1])\n\n# Execute with better performance and monitoring\nresult = await session.flow(builder.get_graph(), max_concurrent=3)\n</code></pre>"},{"location":"comparisons/langgraph/#step-4-test-and-optimize","title":"Step 4: Test and Optimize","text":"<ol> <li>Compare outputs between old and new systems</li> <li>Optimize concurrency settings</li> <li>Add error handling</li> <li>Monitor performance</li> </ol>"},{"location":"comparisons/langgraph/#migration-checklist","title":"Migration Checklist","text":"<ul> <li> Map LangGraph nodes to LionAGI branches</li> <li> Convert state management to dependencies</li> <li> Identify parallel operations</li> <li> Add error handling</li> <li> Test output equivalence</li> <li> Optimize performance settings</li> <li> Add monitoring/observability</li> </ul>"},{"location":"comparisons/langgraph/#when-not-to-migrate","title":"When Not to Migrate","text":"<p>Consider keeping LangGraph if:</p> <ul> <li>You have simple, working workflows that don't need orchestration</li> <li>Your use case doesn't require parallel execution</li> <li>You're heavily invested in LangChain ecosystem features</li> </ul>"},{"location":"comparisons/langgraph/#best-of-both-worlds","title":"Best of Both Worlds","text":"<p>Remember: You don't have to choose! LionAGI can orchestrate your existing LangGraph workflows alongside other tools:</p> <pre><code># Orchestrate everything together\nlanggraph_op = builder.add_operation(operation=existing_langgraph_workflow)\ncrewai_op = builder.add_operation(operation=existing_crewai_workflow)  \nlionagi_op = builder.add_operation(\"communicate\", instruction=\"Pure LionAGI task\")\n\n# LionAGI orchestrates all frameworks\nresult = await session.flow(builder.get_graph())\n</code></pre> <p>LionAGI enhances your existing investments rather than replacing them.</p>"},{"location":"comparisons/langgraph/#key-advantages","title":"Key Advantages","text":""},{"location":"comparisons/langgraph/#1-simplicity","title":"1. Simplicity","text":"<p>LangGraph: Complex state management, manual routing, verbose setup LionAGI: Clean abstractions, automatic orchestration, minimal boilerplate</p>"},{"location":"comparisons/langgraph/#2-parallel-execution","title":"2. Parallel Execution","text":"<p>LangGraph: Sequential by design, complex to parallelize LionAGI: Parallel by default, automatic dependency resolution</p> <pre><code># LionAGI: Automatic parallelization\nresearch = builder.add_operation(\"communicate\", branch=researcher, instruction=\"Research\")\nanalysis = builder.add_operation(\"communicate\", branch=analyst, instruction=\"Analyze\")\n# Both run in parallel automatically\n</code></pre>"},{"location":"comparisons/langgraph/#3-memory-management","title":"3. Memory Management","text":"<ul> <li>LangGraph: Manual state passing, complex message handling</li> <li>LionAGI: Automatic memory management per branch</li> </ul> <pre><code># LionAGI: Each branch maintains independent memory\nresearcher.communicate(\"First question\")\nresearcher.communicate(\"Follow up\")  # Automatically has context\n</code></pre>"},{"location":"comparisons/langgraph/#4-framework-agnostic","title":"4. Framework Agnostic","text":"<p>LangGraph: Locked into LangChain ecosystem LionAGI: Can orchestrate ANY framework as custom operations</p> <pre><code># Wrap existing LangGraph workflow as LionAGI operation\nasync def langgraph_operation(branch, **kwargs):\n    # Your existing LangGraph code here - no changes needed!\n    return await your_langgraph_workflow.invoke(kwargs)\n\n# Use in LionAGI orchestration\nbuilder.add_operation(operation=langgraph_operation, ...)\n</code></pre>"},{"location":"comparisons/langgraph/#5-production-ready","title":"5. Production Ready","text":"<p>LangGraph: Complex debugging, hard to monitor LionAGI: Built-in observability, error handling, performance control</p> <pre><code># LionAGI: Built-in monitoring and control\nresult = await session.flow(\n    builder.get_graph(),\n    max_concurrent=5,  # Control parallelism\n    verbose=True      # Built-in monitoring\n)\n</code></pre>"},{"location":"comparisons/langgraph/#migration-path","title":"Migration Path","text":"<p>Don't throw away your LangGraph investment! LionAGI can orchestrate your existing LangGraph workflows:</p> <pre><code>from lionagi import Session, Builder\n\n# Keep your existing LangGraph workflow\nasync def existing_langgraph_workflow(input_data):\n    # Your current LangGraph code - unchanged!\n    return langgraph_result\n\n# Orchestrate with LionAGI\nsession = Session()\nbuilder = Builder(\"hybrid_workflow\")\n\n# LangGraph workflow as custom operation\nlg_op = builder.add_operation(\n    operation=existing_langgraph_workflow,\n    input_data=\"market research\"\n)\n\n# Pure LionAGI operations\nanalysis_op = builder.add_operation(\n    \"communicate\",\n    branch=analyst,\n    instruction=\"Analyze the research results\",\n    depends_on=[lg_op]\n)\n\n# Get best of both worlds\nresult = await session.flow(builder.get_graph())\n</code></pre>"},{"location":"comparisons/langgraph/#performance-comparison","title":"Performance Comparison","text":"Feature LangGraph LionAGI Parallel Execution Manual/Complex Automatic Setup Complexity High Low State Management Manual Automatic Error Handling Manual Built-in Framework Lock-in Yes No Migration Cost High Zero Debugging Complex Simple Production Monitoring Manual Built-in"},{"location":"comparisons/langgraph/#when-to-choose-lionagi","title":"When to Choose LionAGI","text":"<ul> <li>Parallel workflows: LionAGI excels at concurrent execution</li> <li>Complex orchestration: Multiple agents, dependencies, synthesis</li> <li>Production systems: Built-in monitoring, error handling, performance   control</li> <li>Multi-framework: Orchestrate LangChain, CrewAI, AutoGen together</li> <li>Enterprise: Clean architecture, maintainable code, team scalability</li> </ul> <p>LionAGI doesn't replace your existing investments - it orchestrates them better.</p>"},{"location":"cookbook/","title":"Cookbook","text":"<p>You're in Step 5 of the Learning Path</p> <p>You've learned the patterns. Now grab these complete, working examples that you can copy, modify, and use in production.</p> <p>These recipes are production-ready implementations that demonstrate LionAGI patterns in real-world scenarios. Each includes full code, expected outputs, performance metrics, and customization guidance.</p>"},{"location":"cookbook/#available-recipes","title":"Available Recipes","text":""},{"location":"cookbook/#analysis-research","title":"Analysis &amp; Research","text":"<ul> <li>Claim Extraction - Extract and validate claims from   documents</li> <li>Research Synthesis - Aggregate multiple sources into   insights</li> </ul>"},{"location":"cookbook/#business-applications","title":"Business Applications","text":"<ul> <li>HR Automation - Multi-agent HR workflow system</li> <li>Code Review Crew - Parallel code analysis with quality   gates</li> </ul>"},{"location":"cookbook/#creative-work","title":"Creative Work","text":"<ul> <li>Brainstorming - Generate and refine ideas collaboratively</li> </ul>"},{"location":"cookbook/#technical","title":"Technical","text":"<ul> <li>Data Persistence - Save agent state to databases</li> </ul>"},{"location":"cookbook/#quick-templates","title":"Quick Templates","text":""},{"location":"cookbook/#basic-multi-agent-analysis","title":"Basic Multi-Agent Analysis","text":"<pre><code>from lionagi import Branch, iModel\nimport asyncio\n\nagents = {\n    \"analyst\": Branch(system=\"Analyze data\", chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\")),\n    \"critic\": Branch(system=\"Find issues\", chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\")),\n    \"advisor\": Branch(system=\"Give recommendations\", chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"))\n}\n\nasync def analyze(topic):\n    results = await asyncio.gather(*[\n        agent.chat(f\"Analyze: {topic}\") \n        for agent in agents.values()\n    ])\n    return dict(zip(agents.keys(), results))\n</code></pre>"},{"location":"cookbook/#sequential-pipeline","title":"Sequential Pipeline","text":"<pre><code>from lionagi import Session, Branch, Builder\n\nasync def pipeline(input_data):\n    session = Session()\n    builder = Builder(\"pipeline\")\n\n    extract = builder.add_operation(\"chat\", instruction=f\"Extract key points from: {input_data}\")\n    analyze = builder.add_operation(\"chat\", depends_on=[extract], instruction=\"Analyze the extracted points\")\n    summarize = builder.add_operation(\"chat\", depends_on=[analyze], instruction=\"Create executive summary\")\n\n    return await session.flow(builder.get_graph())\n</code></pre>"},{"location":"cookbook/#contributing-examples","title":"Contributing Examples","text":"<p>Have a useful pattern? Submit a PR with:</p> <ol> <li>Working code</li> <li>Clear use case</li> <li>Expected output</li> <li>Performance metrics</li> </ol> <p>Congratulations! \ud83c\udf89</p> <p>You've completed the LionAGI learning path! You now have:</p> <ul> <li>\u2705 Understanding of LionAGI's paradigm and advantages</li> <li>\u2705 Knowledge of core concepts and architecture  </li> <li>\u2705 Proven patterns for common workflows</li> <li>\u2705 Production-ready examples to build upon</li> </ul> <p>What's next? - Advanced Topics - Custom operations, performance tuning, observability - Integrations - Connect with databases, tools, and services - Migration Guides - If you're coming from other frameworks</p>"},{"location":"cookbook/brainstorming/","title":"Brainstorming Workflows","text":"<p>Creative ideation using parallel agents with divergent \u2192 convergent thinking patterns.</p>"},{"location":"cookbook/brainstorming/#basic-brainstorming-pattern","title":"Basic Brainstorming Pattern","text":"<pre><code>from lionagi import Branch, Session, Builder, iModel\n\nsession = Session()\nbuilder = Builder(\"brainstorming\")\n\n# Create diverse creative agents\ninnovator = Branch(\n    chat_model=iModel(provider=\"anthropic\", model=\"claude-3-sonnet-20240229\"),\n    system=\"Innovative thinker generating bold, unconventional ideas.\"\n)\n\npragmatist = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"),\n    system=\"Focus on practical, implementable solutions.\"\n)\n\ncontrarian = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"),\n    system=\"Challenge assumptions and think from opposite perspectives.\"\n)\n\nsynthesizer = Branch(\n    chat_model=iModel(provider=\"anthropic\", model=\"claude-3-sonnet-20240229\"),\n    system=\"Combine and refine ideas into coherent solutions.\"\n)\n\nsession.include_branches([innovator, pragmatist, contrarian, synthesizer])\n\nchallenge = \"How can we reduce plastic waste in urban environments?\"\n\n# Parallel ideation phase\ninnovator_ideas = builder.add_operation(\n    \"communicate\",\n    branch=innovator,\n    instruction=f\"Generate 3 innovative solutions: {challenge}\"\n)\n\npragmatist_ideas = builder.add_operation(\n    \"communicate\", \n    branch=pragmatist,\n    instruction=f\"Generate 3 practical solutions: {challenge}\"\n)\n\ncontrarian_ideas = builder.add_operation(\n    \"communicate\",\n    branch=contrarian,\n    instruction=f\"3 unconventional approaches: {challenge}\"\n)\n\n# Synthesis phase\nsynthesis = builder.add_aggregation(\n    \"communicate\",\n    branch=synthesizer,\n    source_node_ids=[innovator_ideas, pragmatist_ideas, contrarian_ideas],\n    instruction=\"Create 3 refined solutions combining best aspects\"\n)\n\nresult = await session.flow(builder.get_graph())\n</code></pre>"},{"location":"cookbook/brainstorming/#sequential-brainstorming-process","title":"Sequential Brainstorming Process","text":"<pre><code># Sequential stages: Explore \u2192 Generate \u2192 Evaluate \u2192 Refine\nexplorer = Branch(system=\"Explore problems deeply, identify root causes\")\ngenerator = Branch(system=\"Generate many diverse ideas quickly\")  \nevaluator = Branch(system=\"Evaluate ideas for feasibility and impact\")\nrefiner = Branch(system=\"Refine and improve promising ideas\")\n\nproblem = \"Remote team creative collaboration challenges\"\n\n# Chain dependent operations\nexplore = builder.add_operation(\"communicate\", branch=explorer, \n                                instruction=f\"Analyze problem: {problem}\")\n\ngenerate = builder.add_operation(\"communicate\", branch=generator,\n                                 instruction=\"Generate 10 solution approaches\",\n                                 depends_on=[explore])\n\nevaluate = builder.add_operation(\"communicate\", branch=evaluator,\n                                 instruction=\"Evaluate and rank top 5 ideas\",\n                                 depends_on=[generate])\n\nrefine = builder.add_operation(\"communicate\", branch=refiner,\n                               instruction=\"Develop top ideas into solutions\",\n                               depends_on=[evaluate])\n\nresult = await session.flow(builder.get_graph())\n</code></pre>"},{"location":"cookbook/brainstorming/#multiple-perspectives","title":"Multiple Perspectives","text":"<pre><code># Different viewpoint agents\nuser_advocate = Branch(system=\"Represent end-user needs and experiences\")\ntech_expert = Branch(system=\"Focus on technical feasibility\")\nbusiness_analyst = Branch(system=\"Consider business impact and ROI\") \ncreative_director = Branch(system=\"Focus on innovative user experiences\")\n\nchallenge = \"Design mobile app for sustainable living\"\n\n# Parallel perspective generation\nimport lionagi as ln\n\nresults = {}\n\nasync def get_perspective(name, agent):\n    prompt = f\"From {name} perspective, 3 key ideas for: {challenge}\"\n    results[name] = await agent.communicate(prompt)\n\nasync with ln.create_task_group() as tg:\n    tg.start_soon(get_perspective, \"user_advocate\", user_advocate)\n    tg.start_soon(get_perspective, \"tech_expert\", tech_expert)\n    tg.start_soon(get_perspective, \"business_analyst\", business_analyst)\n    tg.start_soon(get_perspective, \"creative_director\", creative_director)\n\n# Synthesize perspectives\nsynthesizer = Branch(system=\"Synthesize diverse perspectives into solutions\")\nall_perspectives = \"\\n\\n\".join([f\"{k}: {v}\" for k, v in results.items()])\nsynthesis = await synthesizer.communicate(f\"Synthesize: {all_perspectives}\")\n</code></pre>"},{"location":"cookbook/brainstorming/#rapid-ideation-sprint","title":"Rapid Ideation Sprint","text":"<pre><code># Quick parallel idea generation\ngenerators = [\n    Branch(system=\"Generate wild, unconventional ideas\"),\n    Branch(system=\"Focus on simple, elegant solutions\"),\n    Branch(system=\"Think scalable, systematic approaches\"),\n    Branch(system=\"Consider user-centered solutions\")\n]\n\ntopic = \"Make coding accessible to beginners\"\n\n# Parallel rapid generation\nall_ideas = []\n\nasync def quick_ideas(generator):\n    ideas = await generator.communicate(f\"5 quick ideas: {topic}\")\n    all_ideas.append(ideas)\n\nasync with ln.create_task_group() as tg:\n    for generator in generators:\n        tg.start_soon(quick_ideas, generator)\n\n# Curate best ideas\ncurator = Branch(system=\"Identify and combine best ideas\")\ncuration = await curator.communicate(f\"Top 7 from: {all_ideas}\")\n</code></pre>"},{"location":"cookbook/brainstorming/#best-practices","title":"Best Practices","text":""},{"location":"cookbook/brainstorming/#diverse-agent-personalities","title":"Diverse Agent Personalities","text":"<pre><code># Different thinking styles\nagents = [\n    Branch(system=\"Think analytically and systematically\"),\n    Branch(system=\"Think creatively and associatively\"), \n    Branch(system=\"Think practically and implementally\"),\n    Branch(system=\"Think critically and skeptically\")\n]\n</code></pre>"},{"location":"cookbook/brainstorming/#clear-ideation-prompts","title":"Clear Ideation Prompts","text":"<pre><code># Good: Specific, actionable\n\"Generate 5 solutions for X under $Y budget in Z time\"\n\n# Avoid: Vague\n\"Think of some ideas\"\n</code></pre>"},{"location":"cookbook/brainstorming/#structured-synthesis","title":"Structured Synthesis","text":"<pre><code>synthesis_prompt = f\"\"\"\nReview ideas: {all_ideas}\nCreate 3 refined concepts that:\n1. Combine best aspects\n2. Address concerns  \n3. Are actionable\n\"\"\"\n</code></pre>"},{"location":"cookbook/brainstorming/#balance-divergence-and-convergence","title":"Balance Divergence and Convergence","text":"<p>Pattern: Divergent (generate many) \u2192 Convergent (refine/combine) \u2192 Select (develop)</p>"},{"location":"cookbook/brainstorming/#when-to-use","title":"When to Use","text":"<p>Perfect for: Product development, problem solving, strategic planning, content creation, process improvement</p> <p>AI brainstorming leverages parallel processing and diverse perspectives for faster, higher-quality ideation through structured synthesis phases.</p>"},{"location":"cookbook/claim-extraction/","title":"Academic Claim Extraction","text":"<p>ReAct-based claim extraction with sequential document analysis and structured outputs.</p>"},{"location":"cookbook/claim-extraction/#basic-claim-extraction","title":"Basic Claim Extraction","text":"<pre><code>from typing import Literal\nfrom pathlib import Path\nfrom pydantic import BaseModel, Field\nfrom lionagi import Branch, Session, Builder, types, iModel\nfrom lionagi.tools.types import ReaderTool\n\n# Structured claim models\nclass Claim(BaseModel):\n    claim: str\n    type: Literal[\"citation\", \"performance\", \"technical\", \"other\"]\n    location: str = Field(..., description=\"Section/paragraph reference\")\n    verifiability: Literal[\"high\", \"medium\", \"low\"]\n    search_strategy: str = Field(..., description=\"How to verify this claim\")\n\nclass ClaimExtraction(BaseModel):\n    claims: list[Claim]\n\nasync def extract_claims_from_document(document_path: str):\n    \"\"\"Extract verifiable claims using ReAct pattern\"\"\"\n\n    # Create ReAct-enabled branch with ReaderTool\n    extractor = Branch(\n        tools=[ReaderTool],\n        chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"),\n        name=\"claim_extractor\"\n    )\n\n    # Use ReAct for systematic claim extraction\n    result = await extractor.ReAct(\n        instruct=types.Instruct(\n            instruction=(\n                f\"Use ReaderTool to analyze document at {document_path}. \"\n                \"Extract 5-7 specific, verifiable claims. Focus on citations, \"\n                \"performance metrics, and technical assertions.\"\n            ),\n            context={\"document_path\": document_path}\n        ),\n        response_format=ClaimExtraction,\n        tools=[\"reader_tool\"],\n        max_extensions=4,\n        verbose=True\n    )\n\n    return result\n\n# Usage\ndocument_path = \"data/research_paper.pdf\"\nclaims = await extract_claims_from_document(document_path)\n\nfor claim in claims.claims:\n    print(f\"[{claim.type.upper()}] {claim.claim}\")\n    print(f\"Location: {claim.location}\")\n    print(f\"Verifiability: {claim.verifiability}\\n\")\n</code></pre>"},{"location":"cookbook/claim-extraction/#sequential-document-analysis","title":"Sequential Document Analysis","text":"<pre><code>async def sequential_claim_analysis(document_path: str):\n    \"\"\"Progressive document analysis: open \u2192 analyze \u2192 extract claims\"\"\"\n\n    # Create branch with ReaderTool\n    analyzer = Branch(\n        tools=[ReaderTool],\n        chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\")\n    )\n    session = Session(default_branch=analyzer)\n    builder = Builder(\"ClaimAnalysis\")\n\n    # Step 1: Document exploration\n    doc_reader = builder.add_operation(\n        \"ReAct\",\n        node_id=\"explore_document\",\n        instruct=types.Instruct(\n            instruction=(\n                \"Open and explore the document structure. \"\n                \"Identify sections containing verifiable claims.\"\n            ),\n            context={\"document_path\": document_path}\n        ),\n        tools=[\"reader_tool\"],\n        max_extensions=2,\n        verbose=True\n    )\n\n    # Step 2: Content analysis\n    content_analyzer = builder.add_operation(\n        \"ReAct\",\n        node_id=\"analyze_content\",\n        depends_on=[doc_reader],\n        instruct=types.Instruct(\n            instruction=(\n                \"Analyze key sections for citations, technical claims, \"\n                \"and performance metrics that can be verified.\"\n            )\n        ),\n        response_format=types.Outline,\n        tools=[\"reader_tool\"],\n        max_extensions=3,\n        verbose=True\n    )\n\n    # Step 3: Claim extraction\n    claim_extractor = builder.add_operation(\n        \"ReAct\",\n        node_id=\"extract_claims\",\n        depends_on=[content_analyzer],\n        instruct=types.Instruct(\n            instruction=(\n                \"Extract specific verifiable claims based on analysis. \"\n                \"Prioritize citations, performance data, and technical assertions.\"\n            )\n        ),\n        response_format=ClaimExtraction,\n        tools=[\"reader_tool\"],\n        max_extensions=3,\n        verbose=True\n    )\n\n    # Execute sequential workflow\n    graph = builder.get_graph()\n    result = await session.flow(graph, parallel=False, verbose=True)\n\n    return result[\"operation_results\"][claim_extractor]\n\n# Usage\nclaims = await sequential_claim_analysis(\"data/ai_safety_paper.pdf\")\n</code></pre>"},{"location":"cookbook/claim-extraction/#multi-document-claim-extraction","title":"Multi-Document Claim Extraction","text":"<pre><code>from typing import Dict\nimport asyncio\n\nclass DocumentClaims(BaseModel):\n    document: str\n    claims: list[Claim]\n    summary: str\n\nasync def extract_from_multiple_documents(document_paths: list[str]):\n    \"\"\"Parallel claim extraction from multiple documents\"\"\"\n\n    async def process_document(doc_path: str) -&gt; DocumentClaims:\n        \"\"\"Process single document\"\"\"\n        extractor = Branch(\n            tools=[ReaderTool],\n            chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"),\n            name=f\"extractor_{Path(doc_path).stem}\"\n        )\n\n        # Extract claims using ReAct\n        result = await extractor.ReAct(\n            instruct=types.Instruct(\n                instruction=(\n                    f\"Analyze {doc_path} and extract verifiable claims. \"\n                    \"Focus on novel findings and key assertions.\"\n                ),\n                context={\"document\": doc_path}\n            ),\n            response_format=ClaimExtraction,\n            tools=[\"reader_tool\"],\n            max_extensions=3\n        )\n\n        # Generate summary\n        summary = await extractor.communicate(\n            \"Provide brief summary of the document's main contributions\"\n        )\n\n        return DocumentClaims(\n            document=doc_path,\n            claims=result.claims,\n            summary=summary\n        )\n\n    # Process documents in parallel\n    tasks = [process_document(doc) for doc in document_paths]\n    results = await asyncio.gather(*tasks)\n\n    return results\n\n# Usage\npapers = [\n    \"data/transformer_paper.pdf\",\n    \"data/bert_paper.pdf\", \n    \"data/gpt_paper.pdf\"\n]\nall_claims = await extract_from_multiple_documents(papers)\n</code></pre>"},{"location":"cookbook/claim-extraction/#claim-validation-pipeline","title":"Claim Validation Pipeline","text":"<pre><code>class ValidationResult(BaseModel):\n    claim: str\n    validation_status: Literal[\"verified\", \"disputed\", \"unclear\", \"unverifiable\"]\n    evidence: list[str]\n    confidence: float\n\nclass ClaimValidator(BaseModel):\n    validations: list[ValidationResult]\n\ndef search_evidence(claim: str) -&gt; str:\n    \"\"\"Mock search function - replace with actual search API\"\"\"\n    return f\"Search results for: {claim}\"\n\ndef cross_reference(claim: str, reference_docs: list[str]) -&gt; str:\n    \"\"\"Cross-reference claim against known sources\"\"\"\n    return f\"Cross-reference results for: {claim}\"\n\nasync def validate_claims(claims: list[Claim], reference_docs: list[str] = None):\n    \"\"\"Validate extracted claims using ReAct reasoning\"\"\"\n\n    validator = Branch(\n        tools=[search_evidence, cross_reference],\n        chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"),\n        name=\"claim_validator\"\n    )\n\n    validation_tasks = []\n\n    for claim in claims:\n        # Use ReAct to validate each claim\n        task = validator.ReAct(\n            instruct=types.Instruct(\n                instruction=(\n                    f\"Validate this claim: '{claim.claim}' \"\n                    \"Use available tools to search for evidence and cross-reference.\"\n                ),\n                context={\n                    \"claim\": claim.model_dump(),\n                    \"reference_docs\": reference_docs or []\n                }\n            ),\n            response_format=ValidationResult,\n            max_extensions=4,\n            verbose=True\n        )\n        validation_tasks.append(task)\n\n    # Execute validations in parallel\n    validations = await asyncio.gather(*validation_tasks)\n\n    return ClaimValidator(validations=validations)\n\n# Usage\nextracted_claims = claims.claims  # From previous extraction\nvalidation_results = await validate_claims(extracted_claims)\n\nfor validation in validation_results.validations:\n    print(f\"Claim: {validation.claim}\")\n    print(f\"Status: {validation.validation_status}\")\n    print(f\"Confidence: {validation.confidence}\\n\")\n</code></pre>"},{"location":"cookbook/claim-extraction/#citation-specific-extraction","title":"Citation-Specific Extraction","text":"<pre><code>class Citation(BaseModel):\n    text: str\n    authors: list[str]\n    year: int\n    title: str\n    venue: str\n    context: str = Field(..., description=\"Context where citation appears\")\n\nclass CitationExtraction(BaseModel):\n    citations: list[Citation]\n\nasync def extract_citations(document_path: str):\n    \"\"\"Extract and structure citations from academic papers\"\"\"\n\n    citation_extractor = Branch(\n        tools=[ReaderTool],\n        chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"),\n        system=\"You specialize in extracting citations from academic papers\",\n        name=\"citation_extractor\"\n    )\n\n    # Step 1: Identify citation patterns\n    result = await citation_extractor.ReAct(\n        instruct=types.Instruct(\n            instruction=(\n                \"Scan document for citations and references. \"\n                \"Extract complete citation information including context.\"\n            ),\n            context={\"document_path\": document_path}\n        ),\n        response_format=CitationExtraction,\n        tools=[\"reader_tool\"],\n        max_extensions=5,\n        verbose=True\n    )\n\n    return result\n\n# Usage\ncitations = await extract_citations(\"data/survey_paper.pdf\")\nprint(f\"Found {len(citations.citations)} citations\")\n</code></pre>"},{"location":"cookbook/claim-extraction/#performance-claims-analysis","title":"Performance Claims Analysis","text":"<pre><code>class PerformanceClaim(BaseModel):\n    metric: str\n    value: str\n    baseline: str = None\n    improvement: str = None\n    dataset: str\n    methodology: str\n    location: str\n\nclass PerformanceExtraction(BaseModel):\n    performance_claims: list[PerformanceClaim]\n\nasync def extract_performance_claims(document_path: str):\n    \"\"\"Extract performance metrics and benchmarks\"\"\"\n\n    performance_extractor = Branch(\n        tools=[ReaderTool],\n        chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"),\n        system=\"You extract performance metrics and experimental results from research papers\",\n        name=\"performance_extractor\"\n    )\n\n    result = await performance_extractor.ReAct(\n        instruct=types.Instruct(\n            instruction=(\n                \"Extract performance claims including metrics, values, \"\n                \"baselines, datasets, and methodology details.\"\n            ),\n            context={\"document_path\": document_path}\n        ),\n        response_format=PerformanceExtraction,\n        tools=[\"reader_tool\"],\n        max_extensions=4,\n        verbose=True\n    )\n\n    return result\n\n# Usage\nperformance_data = await extract_performance_claims(\"data/benchmark_paper.pdf\")\n</code></pre>"},{"location":"cookbook/claim-extraction/#production-pipeline","title":"Production Pipeline","text":"<pre><code>async def production_claim_extraction_pipeline(\n    document_paths: list[str],\n    validate_claims: bool = True,\n    extract_citations: bool = True\n):\n    \"\"\"Complete production pipeline for claim extraction\"\"\"\n\n    try:\n        results = {}\n\n        for doc_path in document_paths:\n            print(f\"Processing: {doc_path}\")\n\n            # Sequential analysis\n            claims = await sequential_claim_analysis(doc_path)\n            results[doc_path] = {\"claims\": claims}\n\n            # Optional citation extraction\n            if extract_citations:\n                citations = await extract_citations(doc_path)\n                results[doc_path][\"citations\"] = citations\n\n            # Optional claim validation\n            if validate_claims:\n                validations = await validate_claims(claims.claims)\n                results[doc_path][\"validations\"] = validations\n\n            print(f\"Completed: {doc_path}\")\n\n        return results\n\n    except Exception as e:\n        print(f\"Pipeline failed: {e}\")\n        import traceback\n        traceback.print_exc()\n        return None\n\n# Usage\ndocuments = [\n    \"data/paper1.pdf\",\n    \"data/paper2.pdf\",\n    \"data/paper3.pdf\"\n]\n\npipeline_results = await production_claim_extraction_pipeline(\n    documents,\n    validate_claims=True,\n    extract_citations=True\n)\n</code></pre>"},{"location":"cookbook/claim-extraction/#key-features","title":"Key Features","text":"<p>ReAct Integration:</p> <ul> <li>Systematic reasoning before extraction</li> <li>Tool-assisted document analysis</li> <li>Progressive understanding building</li> </ul> <p>Structured Outputs:</p> <ul> <li>Pydantic models for reliable parsing</li> <li>Type-safe claim categorization</li> <li>Consistent data formats</li> </ul> <p>Sequential Processing:</p> <ul> <li>Document exploration \u2192 analysis \u2192 extraction</li> <li>Context-aware claim identification</li> <li>Dependency-based operation flow</li> </ul> <p>Validation Pipeline:</p> <ul> <li>Evidence searching and cross-referencing</li> <li>Confidence scoring for claims</li> <li>Multi-source verification</li> </ul> <p>Production Ready:</p> <ul> <li>Error handling and recovery</li> <li>Parallel processing support</li> <li>Comprehensive logging and monitoring</li> </ul>"},{"location":"cookbook/code-review-crew/","title":"Multi-Agent Code Review","text":"<p>Use specialized agents to review code from multiple perspectives.</p>"},{"location":"cookbook/code-review-crew/#basic-multi-agent-review","title":"Basic Multi-Agent Review","text":"<pre><code>from lionagi import Session, Branch, iModel\n\nsession = Session()\n\n# Create specialized reviewers\nsecurity = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"),\n    system=\"Security expert. Focus only on security issues.\"\n)\n\nperformance = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"),\n    system=\"Performance expert. Focus only on performance issues.\"\n)\n\nmaintainability = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"),\n    system=\"Code quality expert. Focus on maintainability and readability.\"\n)\n\nsession.include_branches([security, performance, maintainability])\n\n# Code to review\ncode = '''\ndef login(user, pwd):\n    query = f\"SELECT * FROM users WHERE name='{user}' AND pass='{pwd}'\"\n    return db.execute(query).fetchone()\n'''\n\n# Parallel reviews using LionAGI TaskGroup\nimport lionagi as ln\n\nreviews = {}\n\nasync def security_review():\n    reviews[\"security\"] = await security.chat(f\"Security review: {code}\")\n\nasync def performance_review():\n    reviews[\"performance\"] = await performance.chat(f\"Performance review: {code}\")\n\nasync def maintainability_review():\n    reviews[\"maintainability\"] = await maintainability.chat(f\"Code quality review: {code}\")\n\nasync with ln.create_task_group() as tg:\n    tg.start_soon(security_review)\n    tg.start_soon(performance_review)\n    tg.start_soon(maintainability_review)\n\n# All tasks complete when TaskGroup context exits\nreview_results = reviews\n</code></pre>"},{"location":"cookbook/code-review-crew/#builder-pattern-review","title":"Builder Pattern Review","text":"<pre><code>from lionagi import Session, Builder, Branch, iModel\n\nsession = Session()\nbuilder = Builder(\"code_review\")\n\n# Reviewers\nsecurity_branch = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"),\n    system=\"Security code reviewer\"\n)\nquality_branch = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"),\n    system=\"Code quality reviewer\"\n)\n\nsession.include_branches([security_branch, quality_branch])\n\n# Code snippet to review (example with SQL injection vulnerability)\nuser_input = \"1 OR 1=1\"  # Example malicious input\ncode_snippet = \"SELECT * FROM users WHERE id=\" + user_input\n\n# Parallel review operations\nsecurity_review = builder.add_operation(\n    \"communicate\",\n    branch=security_branch,\n    instruction=f\"Review for security issues: {code_snippet}\"\n)\n\nquality_review = builder.add_operation(\n    \"communicate\", \n    branch=quality_branch,\n    instruction=f\"Review for code quality: {code_snippet}\"\n)\n\n# Synthesis\nsynthesis = builder.add_aggregation(\n    \"communicate\",\n    branch=security_branch,\n    source_node_ids=[security_review, quality_review],\n    instruction=\"Summarize all review findings\"\n)\n\nresult = await session.flow(builder.get_graph())\n</code></pre>"},{"location":"cookbook/code-review-crew/#review-with-final-decision","title":"Review with Final Decision","text":"<pre><code>session = Session()\nbuilder = Builder(\"comprehensive_review\")\n\n# Create multiple reviewers\nreview_types = [\"security\", \"performance\", \"maintainability\", \"correctness\"]\nreviewers = {}\nreview_ops = []\n\nfor review_type in review_types:\n    branch = Branch(\n        chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"),\n        system=f\"{review_type.title()} code reviewer\"\n    )\n    reviewers[review_type] = branch\n\n    op_id = builder.add_operation(\n        \"communicate\",\n        branch=branch,\n        instruction=f\"{review_type} review of submitted code\"\n    )\n    review_ops.append(op_id)\n\n# Senior reviewer for final decision\nsenior = Branch(\n    chat_model=iModel(provider=\"anthropic\", model=\"claude-3-sonnet-20240229\"),\n    system=\"Senior code reviewer who makes final approval decisions\"\n)\n\nsession.include_branches([*reviewers.values(), senior])\n\n# Final synthesis\nfinal_decision = builder.add_aggregation(\n    \"communicate\",\n    branch=senior,\n    source_node_ids=review_ops,\n    instruction=\"Based on all reviews, provide final APPROVE/REJECT decision\"\n)\n\nresult = await session.flow(builder.get_graph())\n</code></pre>"},{"location":"cookbook/code-review-crew/#best-practices","title":"Best Practices","text":""},{"location":"cookbook/code-review-crew/#specialized-systems","title":"Specialized Systems","text":"<pre><code># Good: Clear specialization\nsecurity = Branch(system=\"Focus only on security vulnerabilities\")\nperformance = Branch(system=\"Focus only on performance bottlenecks\")\n\n# Avoid: Generic reviewers\ngeneric = Branch(system=\"Review all aspects of code\")\n</code></pre>"},{"location":"cookbook/code-review-crew/#structured-output","title":"Structured Output","text":"<pre><code>instruction = \"\"\"\nReview this code for security issues:\n\nFormat response as:\n- Issues Found: [list]\n- Severity: [high/medium/low]  \n- Recommendations: [list]\n\"\"\"\n</code></pre>"},{"location":"cookbook/code-review-crew/#advanced-parallel-execution","title":"Advanced Parallel Execution","text":"<pre><code># LionAGI TaskGroup (recommended)\nasync with ln.create_task_group() as tg:\n    tg.start_soon(security_task)\n    tg.start_soon(performance_task)\n    tg.start_soon(quality_task)\n\n# Or asyncio.gather() for simple cases\nreviews = await asyncio.gather(\n    security.chat(prompt),\n    performance.chat(prompt)\n)\n</code></pre> <p>Multi-agent code review leverages specialized expertise in parallel, catching issues that single reviewers might miss.</p>"},{"location":"cookbook/data-persistence/","title":"Data Persistence","text":"<p>Direct Node-to-database patterns with automatic schema creation.</p>"},{"location":"cookbook/data-persistence/#basic-postgresql-setup","title":"Basic PostgreSQL Setup","text":"<pre><code># Install lionagi with postgres support\n# uv add \"lionagi[postgres]\"\n\nfrom pydantic import BaseModel\nfrom typing import Literal\nfrom lionagi import types\nfrom lionagi.adapters.async_postgres_adapter import LionAGIAsyncPostgresAdapter\n\n# Connection string format\ndsn = \"postgresql+asyncpg://postgres:postgres@127.0.0.1:54322/postgres\"\n\n# Define your data model\nclass StudentInfo(BaseModel):\n    name: str\n    age: int\n    grade: Literal[\"A\", \"B\", \"C\", \"D\", \"F\"]\n\n# Create Node with content\nclass Student(types.Node):\n    content: StudentInfo\n\n# Register the adapter\nStudent.register_async_adapter(LionAGIAsyncPostgresAdapter)\n</code></pre>"},{"location":"cookbook/data-persistence/#save-operations","title":"Save Operations","text":"<pre><code># Create student objects\nstudents = [\n    Student(content=StudentInfo(name=\"Adam Smith\", grade=\"A\", age=20)),\n    Student(content=StudentInfo(name=\"Bob Johnson\", grade=\"B\", age=22)),\n    Student(content=StudentInfo(name=\"Charlie Brown\", grade=\"C\", age=21)),\n]\n\n# Save to database (table created automatically)\nrecords = []\nfor student in students:\n    result = await student.adapt_to_async(\n        obj_key=\"lionagi_async_pg\",\n        dsn=dsn,\n        table=\"students\",\n    )\n    records.append(result)\n\nprint(f\"Saved {len(records)} records\")\n# Output: Saved 3 records\n</code></pre>"},{"location":"cookbook/data-persistence/#query-operations","title":"Query Operations","text":"<pre><code># Fetch single record\nstudent = await Student.adapt_from_async(\n    {\"dsn\": dsn, \"table\": \"students\"},\n    obj_key=\"lionagi_async_pg\",\n)\nprint(f\"Retrieved: {student.content.name}, Grade: {student.content.grade}\")\n\n# Fetch multiple with limit\nstudents = await Student.adapt_from_async(\n    {\"dsn\": dsn, \"table\": \"students\", \"limit\": 2},\n    obj_key=\"lionagi_async_pg\",\n    many=True,\n)\nprint(f\"Retrieved {len(students)} students\")\n\n# Fetch with conditions\nadam = await Student.adapt_from_async(\n    {\"dsn\": dsn, \"table\": \"students\", \"selectors\": {\"id\": str(student.id)}},\n    obj_key=\"lionagi_async_pg\",\n)\n</code></pre>"},{"location":"cookbook/data-persistence/#update-operations","title":"Update Operations","text":"<pre><code># Modify and update\nadam.content.age = 22\n\n# Update in database\nresult = await adam.adapt_to_async(\n    \"lionagi_async_pg\",\n    dsn=dsn,\n    table=\"students\",\n    operation=\"update\",\n    where={\"id\": str(adam.id)},\n)\n\n# Verify update\nupdated_adam = await Student.adapt_from_async(\n    {\"dsn\": dsn, \"table\": \"students\", \"selectors\": {\"id\": str(adam.id)}},\n    obj_key=\"lionagi_async_pg\",\n)\nprint(f\"Updated age: {updated_adam.content.age}\")\n</code></pre>"},{"location":"cookbook/data-persistence/#conversation-persistence","title":"Conversation Persistence","text":"<pre><code>from lionagi import Branch\n\n# Create conversation data model\nclass ConversationState(BaseModel):\n    branch_id: str\n    conversation_data: dict\n    message_count: int\n    last_updated: float\n\nclass ConversationNode(types.Node):\n    content: ConversationState\n\nConversationNode.register_async_adapter(LionAGIAsyncPostgresAdapter)\n\nasync def save_conversation(branch: Branch):\n    \"\"\"Save branch state to database\"\"\"\n    import time\n\n    conversation = ConversationNode(\n        content=ConversationState(\n            branch_id=str(branch.id),\n            conversation_data=branch.to_dict(),\n            message_count=len(branch.messages),\n            last_updated=time.time()\n        )\n    )\n\n    await conversation.adapt_to_async(\n        obj_key=\"lionagi_async_pg\",\n        dsn=dsn,\n        table=\"conversations\",\n    )\n    return conversation\n\nasync def load_conversation(branch_id: str) -&gt; Branch:\n    \"\"\"Load branch state from database\"\"\"\n    conversation = await ConversationNode.adapt_from_async(\n        {\"dsn\": dsn, \"table\": \"conversations\", \"selectors\": {\"branch_id\": branch_id}},\n        obj_key=\"lionagi_async_pg\",\n    )\n\n    return Branch.from_dict(conversation.content.conversation_data)\n\n# Usage\nbranch = Branch(system=\"You are a helpful assistant\")\nawait branch.communicate(\"Hello, how are you?\")\n\n# Save conversation\nsaved_conv = await save_conversation(branch)\nprint(f\"Saved conversation with {saved_conv.content.message_count} messages\")\n\n# Load conversation\nloaded_branch = await load_conversation(str(branch.id))\nprint(f\"Loaded conversation with {len(loaded_branch.messages)} messages\")\n</code></pre>"},{"location":"cookbook/data-persistence/#sqlite-pattern","title":"SQLite Pattern","text":"<pre><code># For local development with SQLite\nsqlite_dsn = \"sqlite+aiosqlite:///./conversations.db\"\n\n# Same Node patterns work with SQLite\nclass LocalStudent(types.Node):\n    content: StudentInfo\n\n# Use async SQLite adapter (if available) or regular sync operations\nasync def save_local(student: LocalStudent):\n    return await student.adapt_to_async(\n        obj_key=\"lionagi_async_sqlite\",  # Adapter key for SQLite\n        dsn=sqlite_dsn,\n        table=\"local_students\",\n    )\n</code></pre>"},{"location":"cookbook/data-persistence/#supabase-integration","title":"Supabase Integration","text":"<pre><code># For Supabase (managed PostgreSQL)\n# Set up project: supabase init &amp;&amp; supabase start\n\nsupabase_dsn = \"postgresql+asyncpg://postgres:postgres@127.0.0.1:54322/postgres\"\n\nclass SupabaseData(BaseModel):\n    title: str\n    content: str\n    created_by: str\n\nclass SupabaseNode(types.Node):\n    content: SupabaseData\n\nSupabaseNode.register_async_adapter(LionAGIAsyncPostgresAdapter)\n\nasync def supabase_operations():\n    # Create\n    node = SupabaseNode(\n        content=SupabaseData(\n            title=\"Research Paper\",\n            content=\"AI safety considerations...\",\n            created_by=\"researcher_001\"\n        )\n    )\n\n    # Save to Supabase\n    result = await node.adapt_to_async(\n        obj_key=\"lionagi_async_pg\",\n        dsn=supabase_dsn,\n        table=\"research_papers\",\n    )\n\n    # Query by creator\n    papers = await SupabaseNode.adapt_from_async(\n        {\n            \"dsn\": supabase_dsn, \n            \"table\": \"research_papers\",\n            \"selectors\": {\"created_by\": \"researcher_001\"}\n        },\n        obj_key=\"lionagi_async_pg\",\n        many=True,\n    )\n\n    return papers\n\n# Usage\npapers = await supabase_operations()\nprint(f\"Found {len(papers)} research papers\")\n</code></pre>"},{"location":"cookbook/data-persistence/#production-patterns","title":"Production Patterns","text":"<pre><code>import asyncio\nfrom contextlib import asynccontextmanager\n\nclass DatabaseManager:\n    def __init__(self, dsn: str):\n        self.dsn = dsn\n        self.pool = None\n\n    @asynccontextmanager\n    async def get_connection(self):\n        \"\"\"Connection pool management\"\"\"\n        try:\n            # Connection logic here\n            yield self.dsn\n        except Exception as e:\n            print(f\"Database error: {e}\")\n            raise\n        finally:\n            # Cleanup\n            pass\n\n# Error handling wrapper\nasync def safe_db_operation(operation, **kwargs):\n    \"\"\"Wrapper for database operations with error handling\"\"\"\n    try:\n        return await operation(**kwargs)\n    except Exception as e:\n        print(f\"Database operation failed: {e}\")\n        return None\n\n# Batch operations\nasync def batch_save(nodes: list[types.Node], table: str):\n    \"\"\"Save multiple nodes efficiently\"\"\"\n    tasks = []\n    for node in nodes:\n        task = safe_db_operation(\n            node.adapt_to_async,\n            obj_key=\"lionagi_async_pg\",\n            dsn=dsn,\n            table=table,\n        )\n        tasks.append(task)\n\n    results = await asyncio.gather(*tasks, return_exceptions=True)\n    successful = [r for r in results if r is not None]\n\n    print(f\"Saved {len(successful)}/{len(nodes)} records\")\n    return successful\n\n# Usage\nstudents = [\n    Student(content=StudentInfo(name=f\"Student {i}\", grade=\"A\", age=20+i))\n    for i in range(10)\n]\nresults = await batch_save(students, \"batch_students\")\n</code></pre>"},{"location":"cookbook/data-persistence/#key-patterns","title":"Key Patterns","text":"<p>Node Creation:</p> <ol> <li>Define Pydantic model for data structure</li> <li>Create Node class with content field</li> <li>Register appropriate async adapter</li> </ol> <p>Database Operations:</p> <ul> <li><code>adapt_to_async()</code> for save/update operations</li> <li><code>adapt_from_async()</code> for query operations</li> <li>Automatic table creation and schema management</li> <li>Support for PostgreSQL, SQLite, and Supabase</li> </ul> <p>Production Considerations:</p> <ul> <li>Connection pooling for performance</li> <li>Error handling with graceful degradation</li> <li>Batch operations for efficiency</li> <li>Proper async/await patterns throughout</li> </ul>"},{"location":"cookbook/hr-automation/","title":"HR Automation System","text":"<p>Multi-agent workflow with feedback loops for comprehensive HR operations.</p>"},{"location":"cookbook/hr-automation/#basic-hr-workflow","title":"Basic HR Workflow","text":"<pre><code>from lionagi import Branch\nimport json\n\n# Define HR system agents\nrecruiter = Branch(\n    system=\"You are an HR recruiter. Screen candidates and assess fit for roles.\",\n    name=\"recruiter\"\n)\n\ninterviewer = Branch(\n    system=\"You are a technical interviewer. Conduct interviews and evaluate skills.\",\n    name=\"interviewer\"  \n)\n\nmanager = Branch(\n    system=\"You are a hiring manager. Make final hiring decisions based on all feedback.\",\n    name=\"manager\"\n)\n\n# Initial candidate screening\ncandidate_profile = {\n    \"name\": \"Alice Johnson\",\n    \"experience\": \"5 years Python development\",\n    \"skills\": [\"Python\", \"FastAPI\", \"PostgreSQL\", \"Docker\"],\n    \"role\": \"Senior Backend Developer\"\n}\n\nasync def hr_workflow(candidate_profile: dict):\n    \"\"\"Complete HR workflow with feedback loops\"\"\"\n\n    # Step 1: Initial screening\n    screening = await recruiter.communicate(\n        \"Screen this candidate for the role\",\n        context=candidate_profile\n    )\n\n    # Step 2: Technical interview\n    interview_result = await interviewer.communicate(\n        \"Conduct technical evaluation based on recruiter screening\",\n        context={\"screening\": screening, \"candidate\": candidate_profile}\n    )\n\n    # Step 3: Feedback loop - recruiter reviews interview\n    recruiter_feedback = await recruiter.communicate(\n        \"Review the technical interview results and provide additional insights\",\n        context=interview_result\n    )\n\n    # Step 4: Manager decision\n    final_decision = await manager.communicate(\n        \"Make hiring decision based on all feedback\",\n        context={\n            \"candidate\": candidate_profile,\n            \"screening\": screening,\n            \"interview\": interview_result,\n            \"recruiter_feedback\": recruiter_feedback\n        }\n    )\n\n    return {\n        \"screening\": screening,\n        \"interview\": interview_result,\n        \"recruiter_feedback\": recruiter_feedback,\n        \"final_decision\": final_decision\n    }\n\n# Execute workflow\nresult = await hr_workflow(candidate_profile)\n</code></pre>"},{"location":"cookbook/hr-automation/#iterative-improvement-workflow","title":"Iterative Improvement Workflow","text":"<pre><code>from pydantic import BaseModel\nfrom typing import Literal\n\nclass CandidateEvaluation(BaseModel):\n    score: int  # 1-10\n    strengths: list[str]\n    concerns: list[str]\n    recommendation: Literal[\"hire\", \"reject\", \"second_interview\"]\n\nasync def iterative_hiring_process(candidate_profile: dict):\n    \"\"\"Multi-round evaluation with feedback improvements\"\"\"\n\n    # Round 1: Initial assessments\n    recruiter_eval = await recruiter.operate(\n        instruction=\"Evaluate candidate for role fit and cultural alignment\",\n        context=candidate_profile,\n        response_format=CandidateEvaluation\n    )\n\n    interviewer_eval = await interviewer.operate(\n        instruction=\"Technical skills assessment and problem-solving evaluation\", \n        context=candidate_profile,\n        response_format=CandidateEvaluation\n    )\n\n    # Round 2: Cross-feedback and refinement\n    recruiter_refined = await recruiter.communicate(\n        \"Review technical assessment and refine your evaluation\",\n        context={\n            \"your_evaluation\": recruiter_eval.model_dump(),\n            \"technical_assessment\": interviewer_eval.model_dump()\n        }\n    )\n\n    interviewer_refined = await interviewer.communicate(\n        \"Consider cultural fit assessment and adjust technical evaluation\",\n        context={\n            \"your_evaluation\": interviewer_eval.model_dump(),\n            \"cultural_assessment\": recruiter_eval.model_dump()\n        }\n    )\n\n    # Round 3: Final synthesis\n    final_assessment = await manager.operate(\n        instruction=\"Synthesize all evaluations into final hiring decision\",\n        context={\n            \"candidate\": candidate_profile,\n            \"recruiter_initial\": recruiter_eval.model_dump(),\n            \"interviewer_initial\": interviewer_eval.model_dump(),\n            \"recruiter_refined\": recruiter_refined,\n            \"interviewer_refined\": interviewer_refined\n        },\n        response_format=CandidateEvaluation\n    )\n\n    return final_assessment\n\n# Usage\nassessment = await iterative_hiring_process(candidate_profile)\nprint(f\"Final recommendation: {assessment.recommendation}\")\nprint(f\"Score: {assessment.score}/10\")\n</code></pre>"},{"location":"cookbook/hr-automation/#performance-review-system","title":"Performance Review System","text":"<pre><code>class PerformanceReview(BaseModel):\n    employee_id: str\n    overall_rating: int  # 1-5\n    achievements: list[str]\n    areas_for_improvement: list[str]\n    goals: list[str]\n    manager_feedback: str\n\n# Performance review agents\ndirect_manager = Branch(\n    system=\"You are a direct manager conducting performance reviews\",\n    name=\"direct_manager\"\n)\n\npeer_reviewer = Branch(\n    system=\"You provide peer feedback for performance reviews\",\n    name=\"peer_reviewer\"\n)\n\nhr_specialist = Branch(\n    system=\"You synthesize performance data and ensure consistency\",\n    name=\"hr_specialist\"\n)\n\nasync def performance_review_cycle(employee_data: dict):\n    \"\"\"360-degree performance review with multiple perspectives\"\"\"\n\n    # Manager assessment\n    manager_review = await direct_manager.operate(\n        instruction=\"Conduct comprehensive performance review\",\n        context=employee_data,\n        response_format=PerformanceReview\n    )\n\n    # Peer feedback\n    peer_feedback = await peer_reviewer.communicate(\n        \"Provide peer perspective on performance and collaboration\",\n        context=employee_data\n    )\n\n    # HR synthesis with manager input\n    hr_synthesis = await hr_specialist.communicate(\n        \"Review manager assessment and peer feedback for consistency\",\n        context={\n            \"employee\": employee_data,\n            \"manager_review\": manager_review.model_dump(),\n            \"peer_feedback\": peer_feedback\n        }\n    )\n\n    # Manager refinement based on HR feedback\n    final_review = await direct_manager.communicate(\n        \"Refine review based on HR synthesis and peer feedback\",\n        context=hr_synthesis\n    )\n\n    return {\n        \"initial_review\": manager_review,\n        \"peer_feedback\": peer_feedback,\n        \"hr_synthesis\": hr_synthesis,\n        \"final_review\": final_review\n    }\n\n# Employee data\nemployee = {\n    \"id\": \"EMP001\",\n    \"name\": \"Bob Smith\",\n    \"role\": \"Software Engineer\",\n    \"tenure\": \"2 years\",\n    \"recent_projects\": [\"API refactoring\", \"Database optimization\"],\n    \"peer_ratings\": [4, 5, 4, 3]\n}\n\nreview_results = await performance_review_cycle(employee)\n</code></pre>"},{"location":"cookbook/hr-automation/#policy-consultation-system","title":"Policy Consultation System","text":"<pre><code># HR policy consultant with tools\ndef lookup_policy(policy_area: str) -&gt; str:\n    \"\"\"Look up HR policies by area\"\"\"\n    policies = {\n        \"vacation\": \"Employees accrue 2 weeks vacation per year...\",\n        \"remote_work\": \"Remote work approved for up to 3 days per week...\",\n        \"benefits\": \"Health insurance starts after 30 days...\",\n        \"performance\": \"Reviews conducted quarterly with annual ratings...\"\n    }\n    return policies.get(policy_area, \"Policy not found\")\n\ndef check_compliance(situation: str) -&gt; str:\n    \"\"\"Check compliance requirements\"\"\"\n    return f\"Compliance check for: {situation}\"\n\n# Policy-aware HR agent\npolicy_agent = Branch(\n    system=\"You are an HR policy expert. Use tools to look up policies and check compliance.\",\n    tools=[lookup_policy, check_compliance],\n    name=\"policy_agent\"\n)\n\n# Employee inquiry agent  \nemployee_support = Branch(\n    system=\"You help employees with HR questions and concerns.\",\n    name=\"employee_support\"\n)\n\nasync def hr_inquiry_workflow(employee_question: str):\n    \"\"\"Handle employee inquiries with policy consultation\"\"\"\n\n    # Initial response from support agent\n    initial_response = await employee_support.communicate(employee_question)\n\n    # Policy consultation using tools\n    policy_guidance = await policy_agent.ReAct(\n        instruct={\"instruction\": f\"Research policy guidance for: {employee_question}\"},\n        max_extensions=3\n    )\n\n    # Refined response with policy backing\n    final_response = await employee_support.communicate(\n        \"Provide comprehensive response using policy guidance\",\n        context={\n            \"original_question\": employee_question,\n            \"initial_response\": initial_response,\n            \"policy_guidance\": policy_guidance\n        }\n    )\n\n    return final_response\n\n# Usage\nquestion = \"Can I work remotely 4 days per week for family reasons?\"\nresponse = await hr_inquiry_workflow(question)\n</code></pre>"},{"location":"cookbook/hr-automation/#onboarding-workflow","title":"Onboarding Workflow","text":"<pre><code># Onboarding specialist agents\nonboarding_coordinator = Branch(\n    system=\"You coordinate new employee onboarding processes\",\n    name=\"coordinator\"\n)\n\nit_setup = Branch(\n    system=\"You handle IT setup and access provisioning for new employees\",\n    name=\"it_setup\"\n)\n\nbuddy_matcher = Branch(\n    system=\"You match new employees with suitable workplace buddies\",\n    name=\"buddy_matcher\"\n)\n\nasync def onboarding_process(new_employee: dict):\n    \"\"\"Complete onboarding with multi-agent coordination\"\"\"\n\n    # Phase 1: Initial planning\n    onboarding_plan = await onboarding_coordinator.operate(\n        instruction=\"Create onboarding plan for new employee\",\n        context=new_employee,\n        response_format=dict\n    )\n\n    # Phase 2: Parallel setup tasks\n    it_tasks = await it_setup.communicate(\n        \"Generate IT setup checklist and timeline\",\n        context={\"employee\": new_employee, \"plan\": onboarding_plan}\n    )\n\n    buddy_match = await buddy_matcher.communicate(\n        \"Find suitable workplace buddy based on role and interests\",\n        context={\"employee\": new_employee, \"plan\": onboarding_plan}\n    )\n\n    # Phase 3: Coordination feedback\n    coordination_update = await onboarding_coordinator.communicate(\n        \"Review setup progress and adjust plan as needed\",\n        context={\n            \"original_plan\": onboarding_plan,\n            \"it_progress\": it_tasks,\n            \"buddy_assignment\": buddy_match\n        }\n    )\n\n    # Phase 4: Final checklist\n    final_checklist = await onboarding_coordinator.communicate(\n        \"Generate final onboarding checklist with all requirements\"\n    )\n\n    return {\n        \"plan\": onboarding_plan,\n        \"it_setup\": it_tasks,\n        \"buddy_match\": buddy_match,\n        \"updates\": coordination_update,\n        \"checklist\": final_checklist\n    }\n\n# New employee data\nnew_hire = {\n    \"name\": \"Carol Davis\",\n    \"role\": \"Product Manager\",\n    \"start_date\": \"2024-02-01\",\n    \"department\": \"Product\",\n    \"manager\": \"Alice Johnson\",\n    \"location\": \"Remote\",\n    \"experience_level\": \"Senior\"\n}\n\nonboarding_result = await onboarding_process(new_hire)\n</code></pre>"},{"location":"cookbook/hr-automation/#state-persistence","title":"State Persistence","text":"<pre><code>async def save_hr_workflow_state():\n    \"\"\"Save conversation states for audit and continuity\"\"\"\n\n    # Save all agent conversations\n    agent_states = {}\n\n    for agent_name, agent in [\n        (\"recruiter\", recruiter),\n        (\"interviewer\", interviewer), \n        (\"manager\", manager)\n    ]:\n        # Convert to dict for persistence\n        state = agent.to_dict()\n\n        # Save to file or database\n        with open(f\"hr_data/{agent_name}_state.json\", \"w\") as f:\n            json.dump(state, f, indent=2)\n\n        agent_states[agent_name] = state\n\n    return agent_states\n\nasync def load_hr_workflow_state():\n    \"\"\"Restore agent states for continued processing\"\"\"\n\n    restored_agents = {}\n\n    for agent_name in [\"recruiter\", \"interviewer\", \"manager\"]:\n        try:\n            with open(f\"hr_data/{agent_name}_state.json\") as f:\n                state_data = json.load(f)\n\n            # Restore agent from saved state\n            agent = Branch.from_dict(state_data)\n            restored_agents[agent_name] = agent\n\n        except FileNotFoundError:\n            print(f\"No saved state for {agent_name}\")\n\n    return restored_agents\n\n# Usage\nawait save_hr_workflow_state()\nrestored_agents = await load_hr_workflow_state()\n</code></pre>"},{"location":"cookbook/hr-automation/#key-benefits","title":"Key Benefits","text":"<p>Multi-Agent Collaboration:</p> <ul> <li>Specialized roles with domain expertise</li> <li>Natural feedback loops between agents</li> <li>Iterative improvement of decisions</li> </ul> <p>Feedback Integration:</p> <ul> <li>Cross-agent review and refinement</li> <li>Multiple perspectives on each decision</li> <li>Continuous improvement through iteration</li> </ul> <p>State Management:</p> <ul> <li>Complete audit trail of decisions</li> <li>Ability to pause/resume workflows</li> <li>Historical analysis of HR patterns</li> </ul> <p>Production Features:</p> <ul> <li>Policy integration with tool usage</li> <li>Compliance checking and documentation</li> <li>Scalable workflow orchestration</li> </ul>"},{"location":"cookbook/research-synthesis/","title":"Research Synthesis","text":"<p>Parallel research with expert synthesis - the natural pattern for comprehensive analysis.</p>"},{"location":"cookbook/research-synthesis/#basic-research-synthesis","title":"Basic Research Synthesis","text":"<pre><code>from lionagi import Branch, Builder, Session, iModel\nfrom lionagi.fields import LIST_INSTRUCT_FIELD_MODEL, Instruct\nfrom lionagi.protocols.types import AssistantResponse\n\n# Setup orchestrator\norchestrator = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"),\n    system=\"Break research into parallel assignments and synthesize findings\"\n)\nsession = Session(default_branch=orchestrator)\nbuilder = Builder(\"ResearchSynthesis\")\n\ntopic = \"AI safety in production systems\"\n\n# Decomposition phase\nroot = builder.add_operation(\n    \"operate\",\n    instruct=Instruct(\n        instruction=f\"Create 3-4 research assignments for: {topic}\",\n        context=topic\n    ),\n    reason=True,\n    field_models=[LIST_INSTRUCT_FIELD_MODEL]\n)\n\n# Execute decomposition\nresult = await session.flow(builder.get_graph())\ninstruct_models = result[\"operation_results\"][root].instruct_models\n\n# Fan-out: Create researchers\nresearch_nodes = []\nfor i, instruction in enumerate(instruct_models):\n    researcher = Branch(\n        chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"),\n        system=f\"Research specialist #{i+1} - focused domain expert\"\n    )\n\n    node = builder.add_operation(\n        \"communicate\",\n        depends_on=[root],\n        branch=researcher,\n        **instruction.to_dict()\n    )\n    research_nodes.append(node)\n\n# Execute research\nawait session.flow(builder.get_graph())\n\n# Extract findings with cost tracking\ncosts = 0\ndef get_context(node_id):\n    global costs\n    graph = builder.get_graph()\n    node = graph.internal_nodes[node_id]\n    branch = session.get_branch(node.branch_id, None)\n    if (branch and len(branch.messages) &gt; 0 and \n        isinstance(msg := branch.messages[-1], AssistantResponse)):\n        costs += msg.model_response.get(\"total_cost_usd\") or 0\n        return f\"\"\"\nResponse: {msg.model_response.get(\"result\") or \"Not available\"}\nSummary: {msg.model_response.get(\"summary\") or \"Not available\"}\n        \"\"\".strip()\n\nctx = [get_context(i) for i in research_nodes]\n\n# Fan-in: Synthesize\nsynthesis = builder.add_operation(\n    \"communicate\",\n    depends_on=research_nodes,\n    branch=orchestrator,\n    instruction=\"Synthesize research findings into comprehensive analysis\",\n    context=[i for i in ctx if i is not None]\n)\n\nfinal_result = await session.flow(builder.get_graph())\nprint(f\"Research complete. Total cost: ${costs:.4f}\")\n</code></pre>"},{"location":"cookbook/research-synthesis/#literature-review","title":"Literature Review","text":"<pre><code># Literature review orchestrator\norchestrator = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"),\n    system=\"Literature review coordinator and synthesizer\"\n)\n\npapers = [\"Attention is All You Need\", \"BERT\", \"GPT-3\"]\nfocus = \"transformer architecture evolution\"\n\n# Generate review plan\nplanning = builder.add_operation(\n    \"operate\",\n    branch=orchestrator,\n    instruct=Instruct(\n        instruction=f\"Create analysis framework for: {focus}\",\n        context={\"papers\": papers, \"focus\": focus}\n    ),\n    field_models=[LIST_INSTRUCT_FIELD_MODEL]\n)\n\nresult = await session.flow(builder.get_graph())\nreview_tasks = result[\"operation_results\"][planning].instruct_models\n\n# Parallel paper analysis\nreview_nodes = []\nfor task in review_tasks:\n    reviewer = Branch(\n        chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"),\n        system=\"Academic paper analysis specialist\"\n    )\n\n    node = builder.add_operation(\n        \"communicate\",\n        depends_on=[planning],\n        branch=reviewer,\n        **task.to_dict()\n    )\n    review_nodes.append(node)\n\nawait session.flow(builder.get_graph())\n\n# Synthesis\nsynthesis = builder.add_operation(\n    \"communicate\",\n    depends_on=review_nodes,\n    branch=orchestrator,\n    instruction=\"Create comprehensive literature review synthesis\"\n)\n\nfinal_result = await session.flow(builder.get_graph())\n</code></pre>"},{"location":"cookbook/research-synthesis/#cost-efficient-research","title":"Cost-Efficient Research","text":"<pre><code># Budget-aware research\ntotal_cost = 0\nmax_cost = 1.0\n\ncoordinator = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"),\n    system=\"Efficient research coordinator\"\n)\n\n# Budget check before synthesis\ndef track_costs(node_id):\n    global total_cost\n    graph = builder.get_graph()\n    node = graph.internal_nodes[node_id]\n    branch = session.get_branch(node.branch_id, None)\n    if (branch and len(branch.messages) &gt; 0 and \n        isinstance(msg := branch.messages[-1], AssistantResponse)):\n        cost = msg.model_response.get(\"total_cost_usd\", 0)\n        total_cost += cost\n        return cost &gt; 0\n    return False\n\nawait session.flow(builder.get_graph())\n\nif total_cost &lt; max_cost:\n    synthesis = builder.add_operation(\n        \"communicate\",\n        depends_on=research_nodes,\n        branch=coordinator,\n        instruction=\"Synthesize research findings efficiently\"\n    )\n    final_result = await session.flow(builder.get_graph())\n    print(f\"Research completed. Cost: ${total_cost:.4f}\")\nelse:\n    print(f\"Budget exceeded: ${total_cost:.4f}\")\n</code></pre>"},{"location":"cookbook/research-synthesis/#production-error-handling","title":"Production Error Handling","text":"<pre><code>try:\n    # Research execution with error handling\n    final_result = await session.flow(builder.get_graph())\n    print(f\"Research complete. Total cost: ${costs:.4f}\")\n\nexcept Exception as e:\n    print(f\"Research failed: {e}\")\n    import traceback\n    traceback.print_exc()\n</code></pre>"},{"location":"cookbook/research-synthesis/#when-to-use","title":"When to Use","text":"<p>Perfect for: Complex topics, literature reviews, market research, technical analysis requiring domain expertise</p> <p>Execution Flow: Task Decomposition \u2192 Parallel Researchers \u2192 Context Extraction \u2192 Synthesis</p> <p>Key Benefits:</p> <ul> <li>3-4x faster than sequential research</li> <li>Higher quality through diverse perspectives</li> <li>Systematic evidence gathering</li> <li>Cost-efficient parallel execution</li> </ul> <p>Research synthesis leverages the fan-out/fan-in pattern for comprehensive analysis through specialized parallel research with intelligent synthesis.</p>"},{"location":"core-concepts/","title":"Core Concepts","text":"<p>You're in Step 3 of the Learning Path</p> <p>You've learned why LionAGI is different. Now let's understand the practical mechanics that make it work.</p> <p>Understanding LionAGI's core abstractions is essential for building effective multi-agent workflows. These concepts work together to provide the parallel execution, memory isolation, and flexible orchestration that make LionAGI powerful.</p>"},{"location":"core-concepts/#key-abstractions","title":"Key Abstractions","text":"<p>Learning Order</p> <p>Read these in order for best understanding: 1. Sessions &amp; Branches (the foundation) 2. Operations (how work gets done) 3. Messages &amp; Memory (how context works)</p>"},{"location":"core-concepts/#sessions-and-branches","title":"Sessions and Branches","text":"<ul> <li>Session: Workspace that coordinates multiple agents</li> <li>Branch: Individual agent with memory and tools</li> </ul>"},{"location":"core-concepts/#operations","title":"Operations","text":"<ul> <li>Building blocks of workflows</li> <li>Types: chat, communicate, operate, ReAct</li> </ul>"},{"location":"core-concepts/#messages-and-memory","title":"Messages and Memory","text":"<ul> <li>How conversation state is managed</li> <li>Memory isolation between branches</li> </ul>"},{"location":"core-concepts/#tools-and-functions","title":"Tools and Functions","text":"<ul> <li>Extending agents with capabilities</li> <li>Built-in and custom tools</li> </ul>"},{"location":"core-concepts/#models-and-providers","title":"Models and Providers","text":"<ul> <li>iModel abstraction for LLM providers</li> <li>Supporting OpenAI, Anthropic, Ollama, etc.</li> </ul>"},{"location":"core-concepts/#the-mental-model","title":"The Mental Model","text":"<pre><code># Traditional: Agents talk to each other\nagent1 \u2192 agent2 \u2192 agent3 \u2192 result\n\n# LionAGI: Agents work in parallel, results synthesized\nagent1 \u2198\nagent2 \u2192 synthesis \u2192 result\nagent3 \u2197\n</code></pre>"},{"location":"core-concepts/#architecture","title":"Architecture","text":"<pre><code>Session (Workspace)\n\u251c\u2500\u2500 Branch (Agent 1)\n\u2502   \u251c\u2500\u2500 Messages (Memory)\n\u2502   \u251c\u2500\u2500 Tools\n\u2502   \u2514\u2500\u2500 Model Config\n\u251c\u2500\u2500 Branch (Agent 2)\n\u2502   \u251c\u2500\u2500 Messages (Memory)\n\u2502   \u251c\u2500\u2500 Tools\n\u2502   \u2514\u2500\u2500 Model Config\n\u2514\u2500\u2500 Graph (Workflow)\n    \u251c\u2500\u2500 Operations\n    \u2514\u2500\u2500 Dependencies\n</code></pre> <p>Ready to Build Workflows?</p> <p>Now that you understand the core concepts, it's time to see them in action:</p> <p>Next: Patterns - Learn proven multi-agent workflow patterns Or: Cookbook - Jump to complete working examples</p>"},{"location":"core-concepts/lionagi-philosophy/","title":"LionAGI Philosophy: Orchestration, Not Operations","text":""},{"location":"core-concepts/lionagi-philosophy/#the-paradigm-shift","title":"The Paradigm Shift","text":"<p>LionAGI is evolving from a collection of operations to a central coordination engine for arbitrary orchestration patterns.</p>"},{"location":"core-concepts/lionagi-philosophy/#current-state-v0x-deprecating","title":"Current State (v0.x - Deprecating)","text":"<pre><code># Branch bloated with every possible operation\nbranch.chat()\nbranch.communicate()\nbranch.reason()\nbranch.plan()\nbranch.analyze()\n# ... dozens more built-in operations\n</code></pre>"},{"location":"core-concepts/lionagi-philosophy/#future-state-v10","title":"Future State (v1.0+)","text":"<pre><code># Branch as minimal interface - only core operations\nbranch.chat()        # Basic LLM interaction\nbranch.communicate() # Structured communication\nbranch.operate()     # Execute arbitrary operations\nbranch.react()       # ReAct pattern for tool use\n\n# Everything else is user-defined operations\nmy_custom_op = create_operation(...)\nbranch.operate(my_custom_op)\n</code></pre>"},{"location":"core-concepts/lionagi-philosophy/#core-philosophy","title":"Core Philosophy","text":""},{"location":"core-concepts/lionagi-philosophy/#branch-as-a-space","title":"Branch as a Space","text":"<p>In category theory terms, a <code>Branch</code> is a space where operations happen, not a collection of operations itself.</p> <pre><code># Branch = Space for computation\n# Operation = Instantiation of orchestration pattern\n# LionAGI = Coordinator that assembles operations\n</code></pre>"},{"location":"core-concepts/lionagi-philosophy/#operations-are-arbitrary","title":"Operations Are Arbitrary","text":"<p>Operations can be: - Agentic - LLM-based reasoning - Computational - Pure functions - External - API calls, database queries - Composite - Operations built from other operations - Cross-Branch - Operations spanning multiple branches</p>"},{"location":"core-concepts/lionagi-philosophy/#creating-custom-operations","title":"Creating Custom Operations","text":""},{"location":"core-concepts/lionagi-philosophy/#simple-operation","title":"Simple Operation","text":"<pre><code>from lionagi import Operation\n\ndef my_analysis_operation(data):\n    \"\"\"Any arbitrary computation\"\"\"\n    return {\"analyzed\": data}\n\n# Use it in a branch\nresult = await branch.operate(my_analysis_operation, data=my_data)\n</code></pre>"},{"location":"core-concepts/lionagi-philosophy/#composite-operation-across-branches","title":"Composite Operation Across Branches","text":"<pre><code>async def cross_branch_synthesis(branches: list[Branch]):\n    \"\"\"Operation spanning multiple branches\"\"\"\n    # Gather from all branches\n    results = await asyncio.gather(*[\n        b.communicate(\"Your perspective?\") \n        for b in branches\n    ])\n\n    # Synthesize in a new branch\n    synthesizer = Branch()\n    return await synthesizer.communicate(\n        f\"Synthesize: {results}\"\n    )\n</code></pre>"},{"location":"core-concepts/lionagi-philosophy/#orchestration-pattern-as-operation","title":"Orchestration Pattern as Operation","text":"<pre><code>class FanOutFanIn(Operation):\n    \"\"\"Reusable orchestration pattern\"\"\"\n\n    async def execute(self, question, expert_branches):\n        # Fan-out\n        analyses = await asyncio.gather(*[\n            branch.communicate(question) \n            for branch in expert_branches\n        ])\n\n        # Fan-in\n        return await self.synthesizer.communicate(\n            f\"Combine analyses: {analyses}\"\n        )\n\n# Use like any operation\npattern = FanOutFanIn(synthesizer=Branch())\nresult = await branch.operate(pattern, question=\"...\", expert_branches=[...])\n</code></pre>"},{"location":"core-concepts/lionagi-philosophy/#why-this-matters","title":"Why This Matters","text":""},{"location":"core-concepts/lionagi-philosophy/#1-unbounded-flexibility","title":"1. Unbounded Flexibility","text":"<p>You're not limited to what LionAGI provides. Create ANY operation you need.</p>"},{"location":"core-concepts/lionagi-philosophy/#2-clean-composition","title":"2. Clean Composition","text":"<p>Operations compose naturally - build complex from simple.</p>"},{"location":"core-concepts/lionagi-philosophy/#3-framework-agnostic","title":"3. Framework Agnostic","text":"<p>Your operations can wrap LangChain, CrewAI, or any other framework.</p>"},{"location":"core-concepts/lionagi-philosophy/#4-true-orchestration","title":"4. True Orchestration","text":"<p>LionAGI becomes the conductor, not the orchestra.</p>"},{"location":"core-concepts/lionagi-philosophy/#migration-guide","title":"Migration Guide","text":""},{"location":"core-concepts/lionagi-philosophy/#old-way-deprecated","title":"Old Way (Deprecated)","text":"<pre><code># Using built-in operations\nresult = await branch.analyze(data)\nsummary = await branch.summarize(result)\nplan = await branch.plan(summary)\n</code></pre>"},{"location":"core-concepts/lionagi-philosophy/#new-way-v10","title":"New Way (v1.0+)","text":"<pre><code># Define your operations\nanalyze = create_operation(my_analysis_logic)\nsummarize = create_operation(my_summary_logic)\nplan = create_operation(my_planning_logic)\n\n# Orchestrate them\nresult = await branch.operate(analyze, data)\nsummary = await branch.operate(summarize, result)\nplan = await branch.operate(plan, summary)\n\n# Or compose them\npipeline = compose(analyze, summarize, plan)\nresult = await branch.operate(pipeline, data)\n</code></pre>"},{"location":"core-concepts/lionagi-philosophy/#primary-orchestration-patterns","title":"Primary Orchestration Patterns","text":"<p>LionAGI will maintain these core patterns as building blocks:</p> <ol> <li>Sequential Pipeline - Operations in order</li> <li>Parallel Execution - Operations simultaneously  </li> <li>Fan-Out/Fan-In - Distribute then synthesize</li> <li>Conditional Branching - Dynamic paths</li> <li>Recursive Decomposition - Break down complex tasks</li> </ol> <p>These are provided not as fixed operations, but as composable patterns you can adapt.</p>"},{"location":"core-concepts/lionagi-philosophy/#the-future-complex-composite-operations","title":"The Future: Complex Composite Operations","text":"<pre><code># Complex operation spanning multiple branches and external systems\nclass EnterpriseWorkflow(Operation):\n    async def execute(self, request):\n        # Spawn specialized branches\n        branches = {\n            \"research\": Branch(system=\"Researcher\"),\n            \"analysis\": Branch(system=\"Analyst\"),\n            \"review\": Branch(system=\"Reviewer\")\n        }\n\n        # External operations\n        db_data = await self.database.query(request.context)\n        api_data = await self.external_api.fetch(request.params)\n\n        # Orchestrate across branches with external data\n        research = await branches[\"research\"].communicate(\n            instruction=request.question,\n            context={\"db\": db_data, \"api\": api_data}\n        )\n\n        analysis = await branches[\"analysis\"].communicate(\n            instruction=\"Analyze findings\",\n            context=research\n        )\n\n        review = await branches[\"review\"].communicate(\n            instruction=\"Review and validate\",\n            context={\"research\": research, \"analysis\": analysis}\n        )\n\n        return {\"workflow_result\": review}\n\n# Use it like any other operation\nworkflow = EnterpriseWorkflow(database=db, external_api=api)\nresult = await branch.operate(workflow, request=my_request)\n</code></pre>"},{"location":"core-concepts/lionagi-philosophy/#summary","title":"Summary","text":"<p>LionAGI is becoming a coordination engine, not an operation library.</p> <ul> <li>Minimal core: Just <code>chat</code>, <code>communicate</code>, <code>operate</code>, <code>ReAct</code></li> <li>User-defined everything: Create your own operations</li> <li>Arbitrary orchestration: Not limited to agentic patterns</li> <li>Composable patterns: Build complex from simple</li> <li>Cross-branch coordination: Operations can span multiple spaces</li> </ul> <p>This is the future of LionAGI: You bring the operations, we provide the orchestration.</p>"},{"location":"core-concepts/messages-and-memory/","title":"Messages and Memory","text":"<p>LionAGI automatically manages conversation state and memory through branches.</p>"},{"location":"core-concepts/messages-and-memory/#automatic-memory-management","title":"Automatic Memory Management","text":"<pre><code>from lionagi import Branch, iModel\n\nassistant = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4\"),\n    system=\"You are a helpful assistant\"\n)\n\n# Messages handled automatically\nresponse = await assistant.chat(\"Explain Python list comprehensions\")\n\n# Follow-up maintains context\nclarification = await assistant.chat(\"Show a complex example\")\n</code></pre>"},{"location":"core-concepts/messages-and-memory/#message-history","title":"Message History","text":"<pre><code>from lionagi import Branch, iModel\n\nanalyst = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4\"),\n    system=\"Data analyst\"\n)\n\nawait analyst.chat(\"Analyze this data: [1000, 1200, 800, 1500]\")\nawait analyst.chat(\"What's the trend?\")\n\n# Access message history\nprint(f\"Total messages: {len(analyst.messages)}\")\n\n# Iterate through messages\nfor msg in analyst.messages:\n    print(f\"{msg.role}: {msg.content[:50]}...\")\n</code></pre>"},{"location":"core-concepts/messages-and-memory/#multi-branch-memory","title":"Multi-Branch Memory","text":"<p>Each branch maintains independent memory within a session:</p> <pre><code>from lionagi import Session, Branch, iModel\n\nsession = Session()\n\nresearcher = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4\"),\n    system=\"Research specialist\",\n    name=\"researcher\"\n)\n\ncritic = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4\"),\n    system=\"Critical analyst\",\n    name=\"critic\"\n)\n\nsession.include_branches([researcher, critic])\n\n# Each branch maintains separate memory\nawait researcher.chat(\"Research AI safety measures\")\nawait critic.chat(\"What are potential risks of AI?\")\n\nprint(f\"Researcher: {len(researcher.messages)} messages\")\nprint(f\"Critic: {len(critic.messages)} messages\")\n</code></pre>"},{"location":"core-concepts/messages-and-memory/#context-continuity","title":"Context Continuity","text":"<p>Conversation context is automatically maintained:</p> <pre><code>mathematician = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4\"),\n    system=\"Math tutor\"\n)\n\nawait mathematician.chat(\"I'm learning calculus\")\nawait mathematician.chat(\"Explain derivatives\")\nawait mathematician.chat(\"What about the chain rule?\")\n\n# References full conversation context\nquiz = await mathematician.chat(\"Quiz me on what we've covered\")\n</code></pre>"},{"location":"core-concepts/messages-and-memory/#memory-serialization","title":"Memory Serialization","text":"<p>Export conversation data for persistence:</p> <pre><code>import json\nfrom lionagi import Branch, iModel\n\nassistant = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4\"),\n    system=\"Helpful assistant\"\n)\n\nawait assistant.chat(\"My favorite color is blue\")\nawait assistant.chat(\"I work as a software engineer\")\n\n# Export conversation\ndata = {\n    \"name\": assistant.name,\n    \"messages\": [\n        {\"role\": msg.role, \"content\": msg.content}\n        for msg in assistant.messages\n    ]\n}\n\nwith open(\"conversation.json\", \"w\") as f:\n    json.dump(data, f)\n</code></pre>"},{"location":"core-concepts/messages-and-memory/#workflow-memory","title":"Workflow Memory","text":"<p>Each branch in Builder workflows maintains independent memory:</p> <pre><code>from lionagi import Session, Builder, Branch, iModel\n\nsession = Session()\nbuilder = Builder(\"workflow\")\n\ncollector = Branch(system=\"Collect information\")\nanalyzer = Branch(system=\"Analyze information\")\n\nsession.include_branches([collector, analyzer])\n\ncollect_op = builder.add_operation(\n    \"communicate\",\n    branch=collector,\n    instruction=\"Collect renewable energy data\"\n)\n\nanalyze_op = builder.add_operation(\n    \"communicate\",\n    branch=analyzer,\n    instruction=\"Analyze the collected data\",\n    depends_on=[collect_op]\n)\n\nresult = await session.flow(builder.get_graph())\n</code></pre>"},{"location":"core-concepts/messages-and-memory/#best-practices","title":"Best Practices","text":"<p>Use descriptive system prompts for consistent memory context:</p> <pre><code>specialist = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4\"),\n    system=\"You are Dr. Smith, a cardiologist with 20 years experience\"\n)\n</code></pre> <p>Use <code>communicate()</code> for stateful conversations, <code>chat()</code> for independent queries:</p> <pre><code># Stateful conversation\nawait therapist.communicate(\"I had a difficult day\")\nawait therapist.communicate(\"Tell me about stress management\")\n\n# Independent query\nawait assistant.chat(\"What time is it in Tokyo?\")\n</code></pre> <p>Leverage specialized branches for domain-specific memory:</p> <pre><code>research_team = {\n    \"reviewer\": Branch(system=\"Literature reviewer\"),\n    \"analyst\": Branch(system=\"Data analyst\"),\n    \"writer\": Branch(system=\"Report writer\")\n}\n</code></pre> <p>LionAGI automatically handles conversation state, letting you focus on building intelligent workflows without managing message details.</p>"},{"location":"core-concepts/models-and-providers/","title":"Models and Providers","text":"<p>LionAGI's <code>iModel</code> provides a unified interface for working with different LLM providers.</p>"},{"location":"core-concepts/models-and-providers/#basic-usage","title":"Basic Usage","text":"<pre><code>from lionagi import Branch, iModel\n\nassistant = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4\")\n)\n\nresponse = await assistant.chat(\"Explain quantum computing\")\n</code></pre>"},{"location":"core-concepts/models-and-providers/#supported-providers","title":"Supported Providers","text":"<p>OpenAI: API key from <code>OPENAI_API_KEY</code> environment variable</p> <pre><code>openai_branch = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4\"),\n    system=\"Quantum computing expert\"\n)\n</code></pre> <p>Anthropic: API key from <code>ANTHROPIC_API_KEY</code> environment variable</p> <pre><code>claude_branch = Branch(\n    chat_model=iModel(provider=\"anthropic\", model=\"claude-3-5-sonnet-20240620\"),\n    system=\"Python programming expert\"\n)\n</code></pre> <p>Ollama: Local models</p> <pre><code>local_branch = Branch(\n    chat_model=iModel(\n        provider=\"ollama\", \n        model=\"llama2\",\n        base_url=\"http://localhost:11434\"\n    ),\n    system=\"Local assistant\"\n)\n</code></pre>"},{"location":"core-concepts/models-and-providers/#multiple-providers","title":"Multiple Providers","text":"<p>Mix different providers in a single session:</p> <pre><code>from lionagi import Session, Branch, iModel\n\nsession = Session()\n\nfast_branch = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4\"),\n    system=\"Quick answers\",\n    name=\"fast\"\n)\n\nanalytical_branch = Branch(\n    chat_model=iModel(provider=\"anthropic\", model=\"claude-3-5-sonnet-20240620\"),\n    system=\"Detailed analysis\",\n    name=\"analytical\"\n)\n\nsession.include_branches([fast_branch, analytical_branch])\n\n# Use different models for different tasks\nquick = await fast_branch.chat(\"What is 2+2?\")\nanalysis = await analytical_branch.chat(\"Analyze AI implications\")\n</code></pre>"},{"location":"core-concepts/models-and-providers/#model-configuration","title":"Model Configuration","text":"<p>Configure model parameters:</p> <pre><code>from lionagi import Branch, iModel\n\nconfigured_model = iModel(\n    provider=\"openai\",\n    model=\"gpt-4\",\n    temperature=0.7,        # Randomness (0.0-1.0) \n    max_tokens=2000,       # Response length limit\n    limit_requests=100,    # Rate limiting\n    limit_tokens=50000\n)\n\nwriter = Branch(\n    chat_model=configured_model,\n    system=\"Creative writer\"\n)\n</code></pre>"},{"location":"core-concepts/models-and-providers/#environment-configuration","title":"Environment Configuration","text":"<p>Load settings from environment variables:</p> <pre><code>import os\nfrom lionagi import Branch, iModel\n\nmodel = iModel(\n    provider=os.getenv(\"LLM_PROVIDER\", \"openai\"),\n    model=os.getenv(\"LLM_MODEL\", \"gpt-4\"),\n    temperature=float(os.getenv(\"LLM_TEMPERATURE\", \"0.7\"))\n)\n\nassistant = Branch(chat_model=model)\n</code></pre>"},{"location":"core-concepts/models-and-providers/#model-selection","title":"Model Selection","text":"<p>Choose models based on task requirements:</p> <pre><code>from lionagi import Branch, iModel\n\ndef select_model(task_type: str):\n    if task_type == \"simple_qa\":\n        return iModel(provider=\"openai\", model=\"gpt-4\")\n    elif task_type == \"complex_reasoning\": \n        return iModel(provider=\"anthropic\", model=\"claude-3-5-sonnet-20240620\")\n    elif task_type == \"private_data\":\n        return iModel(provider=\"ollama\", model=\"llama2\")\n    else:\n        return iModel(provider=\"openai\", model=\"gpt-4\")  # Default\n\n# Use appropriate model for task\nmodel = select_model(\"complex_reasoning\")\nassistant = Branch(chat_model=model, system=\"Analytical assistant\")\n</code></pre>"},{"location":"core-concepts/models-and-providers/#best-practices","title":"Best Practices","text":"<p>Use descriptive branch names and appropriate models:</p> <pre><code># Different models for different needs\nfast_qa = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4\", temperature=0.3),\n    system=\"Quick, consistent answers\",\n    name=\"qa_assistant\"\n)\n\nanalytical = Branch(\n    chat_model=iModel(provider=\"anthropic\", model=\"claude-3-5-sonnet-20240620\"),\n    system=\"Detailed analysis\", \n    name=\"analyst\"\n)\n</code></pre> <p>Handle errors gracefully:</p> <pre><code>models_to_try = [\n    (\"openai\", \"gpt-4\"),\n    (\"anthropic\", \"claude-3-5-sonnet-20240620\"),\n    (\"ollama\", \"llama2\")\n]\n\nfor provider, model in models_to_try:\n    try:\n        assistant = Branch(chat_model=iModel(provider=provider, model=model))\n        response = await assistant.chat(\"Hello!\")\n        break  # Use first successful model\n    except Exception as e:\n        continue\n</code></pre> <p>Pre-configure models for different scenarios:</p> <pre><code>CONFIGS = {\n    \"development\": iModel(provider=\"openai\", model=\"gpt-4\", temperature=0.3),\n    \"production\": iModel(provider=\"anthropic\", model=\"claude-3-5-sonnet-20240620\"),\n    \"creative\": iModel(provider=\"openai\", model=\"gpt-4\", temperature=0.9)\n}\n\ndev_assistant = Branch(chat_model=CONFIGS[\"development\"])\n</code></pre> <p>LionAGI's iModel provides a consistent interface across providers while handling API differences and configuration automatically.</p>"},{"location":"core-concepts/operations/","title":"Operations","text":"<p>Operations are the fundamental building blocks of LionAGI workflows. They define specific tasks that Branches execute, control execution order through dependencies, and enable complex orchestration patterns.</p> <p>Think of Operations as discrete work units: each Operation represents a single task (like analyzing data, writing content, or making decisions) that gets assigned to a specific Branch for execution.</p> <pre><code>from lionagi import Builder\n\nbuilder = Builder(\"workflow\")\n\nop = builder.add_operation(\n    \"chat\",\n    instruction=\"Analyze this data\",\n    branch=analyst_branch,\n    depends_on=[previous_op]\n)\n</code></pre> <p>This example shows the basic Operation structure: you specify the operation type, provide instructions, assign it to a Branch, and optionally define dependencies that control when the operation runs.</p>"},{"location":"core-concepts/operations/#core-concepts","title":"Core Concepts","text":"<ul> <li>Operations define tasks for branches to execute</li> <li>Dependencies control execution order</li> <li>Operations without dependencies run in parallel</li> </ul>"},{"location":"core-concepts/operations/#operation-types","title":"Operation Types","text":"<p>LionAGI provides three core operation types, each optimized for different use cases:</p>"},{"location":"core-concepts/operations/#chat-basic-conversation","title":"chat: Basic Conversation","text":"<p>Use <code>chat</code> for simple, stateless interactions where you need natural language responses:</p> <pre><code>builder.add_operation(\"chat\", branch=agent, instruction=\"Explain quantum computing\")\n</code></pre> <p>The <code>chat</code> operation is ideal for one-off questions, explanations, and tasks that don't require maintaining conversation context.</p>"},{"location":"core-concepts/operations/#communicate-stateful-conversation","title":"communicate: Stateful Conversation","text":"<p>Use <code>communicate</code> when you need the Branch to maintain conversation history and context across operations:</p> <pre><code>builder.add_operation(\"communicate\", branch=agent, instruction=\"Continue discussion\", context=data)\n</code></pre> <p>This operation type enables follow-up questions, iterative refinement, and complex dialogues where context matters.</p>"},{"location":"core-concepts/operations/#operate-structured-output","title":"operate: Structured Output","text":"<p>Use <code>operate</code> when you need predictable, structured responses that conform to specific data models:</p> <pre><code>from pydantic import BaseModel\n\nclass Analysis(BaseModel):\n    sentiment: str\n    confidence: float\n\nbuilder.add_operation(\"operate\", branch=agent, instruction=\"Analyze feedback\", response_format=Analysis)\n</code></pre> <p>The <code>operate</code> operation ensures consistent output format, making it perfect for data processing, analysis, and integration with downstream systems.</p>"},{"location":"core-concepts/operations/#react-reasoning-and-tool-use","title":"ReAct: Reasoning and Tool Use","text":"<p>Use <code>ReAct</code> when you need the Branch to reason through problems and use tools to gather information or perform actions:</p> <pre><code>builder.add_operation(\n    \"ReAct\", \n    branch=agent_with_tools,\n    instruct={\"instruction\": \"Research AI advances\"},\n    max_extensions=3\n)\n</code></pre> <p>The ReAct operation enables sophisticated reasoning patterns where the agent can think, act (use tools), and observe results in iterative cycles. The <code>max_extensions</code> parameter controls how many reasoning-action cycles are allowed.</p>"},{"location":"core-concepts/operations/#common-orchestration-patterns","title":"Common Orchestration Patterns","text":"<p>Understanding these fundamental patterns will help you design effective multi-agent workflows:</p>"},{"location":"core-concepts/operations/#sequential-processing","title":"Sequential Processing","text":"<p>Use sequential patterns when each step builds on the results of the previous step:</p> <pre><code>step1 = builder.add_operation(\"chat\", instruction=\"Extract data\")\nstep2 = builder.add_operation(\"chat\", instruction=\"Clean data\", depends_on=[step1])\nstep3 = builder.add_operation(\"chat\", instruction=\"Analyze data\", depends_on=[step2])\n</code></pre> <p>Sequential processing ensures proper data flow and maintains logical dependencies, but executes operations one at a time.</p> <p>Learn More</p> <p>See Sequential Analysis Pattern for complete examples and best practices.</p>"},{"location":"core-concepts/operations/#fan-outfan-in-pattern","title":"Fan-Out/Fan-In Pattern","text":"<p>Use this pattern to analyze data from multiple perspectives simultaneously, then synthesize the results:</p> <pre><code>analysis1 = builder.add_operation(\"chat\", branch=agent1, instruction=\"Analyze A\")\nanalysis2 = builder.add_operation(\"chat\", branch=agent2, instruction=\"Analyze B\")\n\n# Synthesis depends on both analyses completing\nsynthesis = builder.add_operation(\n    \"chat\", \n    depends_on=[analysis1, analysis2],\n    instruction=\"Combine analyses\"\n)\n</code></pre> <p>This pattern maximizes parallelism while ensuring all perspectives are considered before synthesis.</p> <p>Learn More</p> <p>See Fan-Out/In Pattern for production examples and performance characteristics.</p>"},{"location":"core-concepts/operations/#execution","title":"Execution","text":"<pre><code># Build and execute workflow\ngraph = builder.get_graph()\nresults = await session.flow(graph)\n\n# Access operation results\nresult = results[\"operation_results\"][op.id]\n</code></pre>"},{"location":"core-concepts/operations/#key-parameters","title":"Key Parameters","text":"<pre><code>builder.add_operation(\n    \"chat\",\n    branch=agent,              # Which branch executes\n    instruction=\"Task\",        # What to do\n    depends_on=[],            # Dependencies\n    timeout_seconds=300,      # Optional timeout\n    retry_times=3             # Retry on failure\n)\n</code></pre>"},{"location":"core-concepts/operations/#best-practices","title":"Best Practices","text":"<ul> <li>Use <code>chat</code> for simple tasks, <code>operate</code> for structured output, <code>ReAct</code> for tool use \u2192 See Tools and Functions</li> <li>Minimize dependencies to maximize parallelism \u2192 See Performance Guide </li> <li>Set appropriate timeouts and retry parameters \u2192 See Error Handling</li> </ul>"},{"location":"core-concepts/operations/#next-steps","title":"Next Steps","text":"<p>Ready to Apply Operations?</p> <p>Now that you understand operations, see them in action:</p> <ul> <li>Patterns - Common workflow patterns using operations</li> <li>Cookbook - Complete examples you can copy and modify</li> <li>Advanced Operations - Build your own operation types</li> </ul>"},{"location":"core-concepts/sessions-and-branches/","title":"Sessions and Branches","text":"<p>Understanding the core abstractions that power LionAGI's orchestration engine.</p>"},{"location":"core-concepts/sessions-and-branches/#core-concepts-overview","title":"Core Concepts Overview","text":"<p>At the heart of LionAGI are two fundamental abstractions that enable powerful multi-agent coordination:</p> <ul> <li>Session: A workspace that coordinates multiple agents and manages their interactions</li> <li>Branch: An individual agent with its own memory, tools, and specialized capabilities</li> </ul> <p>Think of a Session as a project workspace where multiple expert agents collaborate, while each Branch represents a distinct expert with isolated memory and context.</p>"},{"location":"core-concepts/sessions-and-branches/#the-evolution-branch-as-computational-space","title":"The Evolution: Branch as Computational Space","text":"<p>A <code>Branch</code> has evolved from a simple collection of methods to a computational space where AI operations happen. Each Branch maintains independent state, enabling true parallel processing without interference between agents.</p>"},{"location":"core-concepts/sessions-and-branches/#quick-example","title":"Quick Example","text":"<p>Let's start with a simple example to see how Sessions and Branches work together:</p> <pre><code>from lionagi import Session, Branch, iModel\n\nsession = Session()\n\n# Create specialized agent\nresearcher = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4\"),\n    system=\"Research specialist\"\n)\n\nsession.include_branches(researcher)\nresult = await researcher.communicate(\"Analyze quantum computing trends\")\n</code></pre> <p>This example demonstrates the basic pattern: create a Session as your workspace, define a specialized Branch with specific capabilities, and coordinate their interactions through the Session.</p>"},{"location":"core-concepts/sessions-and-branches/#key-benefits","title":"Key Benefits","text":"<p>Understanding these core concepts unlocks several powerful capabilities:</p> <ul> <li>Memory Isolation: Each Branch maintains separate conversation history, preventing context bleeding \u2192 See Messages and Memory</li> <li>Parallel Processing: Multiple Branches can work simultaneously without interference \u2192 See Fan-Out/In Pattern</li> <li>Specialized Roles: Each Branch can have distinct system prompts, tools, and configurations \u2192 See Tools and Functions </li> <li>Coordinated Workflows: Sessions orchestrate complex multi-agent interactions \u2192 See Operations</li> </ul>"},{"location":"core-concepts/sessions-and-branches/#multi-agent-coordination","title":"Multi-Agent Coordination","text":"<p>Now let's see how multiple Branches can work together in parallel. This pattern is particularly powerful when you need different perspectives on the same problem:</p> <pre><code>from lionagi import Session, Branch, iModel\nimport lionagi as ln\n\nsession = Session()\n\nresearcher = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4\"),\n    system=\"Research specialist\"\n)\n\ncritic = Branch(\n    chat_model=iModel(provider=\"anthropic\", model=\"claude-3-sonnet-20240229\"),\n    system=\"Critical reviewer\"\n)\n\nsession.include_branches([researcher, critic])\n\n# Parallel execution with TaskGroup\nresults = {}\n\nasync def research_task():\n    results[\"research\"] = await researcher.communicate(\"Research quantum computing\")\n\nasync def critique_task():\n    results[\"critique\"] = await critic.communicate(\"Critique quantum computing risks\")\n\nasync with ln.create_task_group() as tg:\n    tg.start_soon(research_task)\n    tg.start_soon(critique_task)\n\nprint(results)  # Both tasks complete when TaskGroup exits\n</code></pre> <p>In this example, we create two specialized agents that work simultaneously. The researcher gathers information while the critic evaluates risks, both running in parallel for faster execution. The TaskGroup ensures both operations complete before proceeding.</p>"},{"location":"core-concepts/sessions-and-branches/#session-as-workspace","title":"Session as Workspace","text":"<p>Sessions act as project workspaces where you can create and manage multiple specialized Branches. This pattern is ideal for building teams of AI agents with distinct roles:</p> <pre><code>from lionagi import Session\n\nsession = Session(name=\"project_alpha\")\n\n# Create specialized branches\ndata_analyst = session.new_branch(\n    name=\"data_analyst\",\n    system=\"Data analysis specialist\",\n    tools=[analyze_function]\n)\n\nreport_writer = session.new_branch(\n    name=\"report_writer\",\n    system=\"Report writing specialist\"\n)\n\n# Coordinate workflow\nasync def generate_report(data):\n    analysis = await data_analyst.chat(f\"Analyze: {data}\")\n    report = await report_writer.communicate(f\"Report on: {analysis}\")\n    return {\"analysis\": analysis, \"report\": report}\n</code></pre> <p>Here we use <code>session.new_branch()</code> to create Branches directly within the Session context. This approach provides better organization and makes it easier to coordinate workflows between agents.</p>"},{"location":"core-concepts/sessions-and-branches/#branch-as-agent","title":"Branch as Agent","text":"<p>Each Branch functions as an independent agent with its own memory, tools, and conversation history. This design enables sophisticated interactions and parallel processing:</p> <pre><code>from lionagi import Branch, iModel\n\n# Branch with tools and memory\nresearch_agent = Branch(\n    name=\"researcher\",\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4\"),\n    system=\"Research specialist\",\n    tools=[web_search, calculator]\n)\n\n# Maintains conversation history\nawait research_agent.communicate(\"What's the latest in renewable energy?\")\nawait research_agent.communicate(\"Compare that to the previous answer\")  # References history\n\n# Clone for parallel work\nclone = research_agent.clone()\nclone.name = \"research_clone\"\n\n# Independent memory, shared configuration\noriginal = await research_agent.chat(\"Research wind energy\")\nparallel = await clone.chat(\"Research solar energy\")\n</code></pre> <p>Notice how the Branch maintains conversation history across multiple interactions, allowing for contextual follow-up questions. The <code>clone()</code> method creates an independent copy with the same configuration but separate memory, enabling parallel work streams.</p>"},{"location":"core-concepts/sessions-and-branches/#multi-branch-coordination","title":"Multi-Branch Coordination","text":"<p>For complex decision-making, you can coordinate multiple expert Branches to gather diverse perspectives. This expert panel pattern is particularly effective for comprehensive analysis:</p> <pre><code>from lionagi import Session, iModel\nimport lionagi as ln\n\nasync def expert_panel():\n    session = Session()\n\n    # Create expert branches\n    experts = [\n        session.new_branch(\n            name=f\"{role}_expert\",\n            system=f\"{role.title()} expert\"\n        )\n        for role in [\"technical\", \"business\", \"regulatory\"]\n    ]\n\n    topic = \"AI regulation impact\"\n\n    # Parallel expert opinions\n    results = {}\n\n    async def get_opinion(expert):\n        results[expert.name] = await expert.chat(f\"Your view on: {topic}\")\n\n    async with ln.create_task_group() as tg:\n        for expert in experts:\n            tg.start_soon(get_opinion, expert)\n\n    # Synthesis\n    moderator = session.new_branch(name=\"moderator\", system=\"Consensus builder\")\n    consensus = await moderator.chat(f\"Synthesize: {results}\")\n\n    return consensus\n</code></pre> <p>This pattern demonstrates the power of coordinated multi-agent systems: we dynamically create expert Branches with different specializations, gather their opinions in parallel, then use a moderator Branch to synthesize the results into a coherent consensus.</p>"},{"location":"core-concepts/sessions-and-branches/#builder-pattern","title":"Builder Pattern","text":"<p>For complex workflows with dependencies, use the Builder pattern to define operation graphs. This approach provides clear control over execution order and data flow:</p> <pre><code>from lionagi import Session, Builder\n\nsession = Session()\n\n# Create processing branches\nparser = session.new_branch(name=\"parser\", system=\"Extract key information\")\nclassifier = session.new_branch(name=\"classifier\", system=\"Classify content\")\n\n# Build workflow graph\nbuilder = Builder(\"document_flow\")\n\nparse_op = builder.add_operation(\n    \"chat\",\n    instruction=\"Extract info from: {document}\",\n    branch=parser\n)\n\nclassify_op = builder.add_operation(\n    \"chat\",\n    instruction=\"Classify: {parse_op}\",\n    depends_on=[parse_op],\n    branch=classifier\n)\n\n# Execute workflow\nresult = await session.flow(\n    graph=builder.get_graph(),\n    context={\"document\": \"Sample document...\"}\n)\n</code></pre> <p>The Builder pattern excels at orchestrating sequential operations where later steps depend on earlier results. The <code>depends_on</code> parameter ensures the parser completes before the classifier begins, while the <code>{parse_op}</code> template automatically injects the parser's output into the classifier's instruction.</p>"},{"location":"core-concepts/sessions-and-branches/#best-practices","title":"Best Practices","text":"<p>Here are proven patterns for organizing Sessions and Branches in production applications.</p>"},{"location":"core-concepts/sessions-and-branches/#session-management","title":"Session Management","text":"<p>Encapsulate Sessions within classes to create reusable, stateful processors:</p> <pre><code>from lionagi import Session\n\nclass DocumentProcessor:\n    def __init__(self):\n        self.session = Session(name=\"processor\")\n        self.parser = self.session.new_branch(name=\"parser\", system=\"Parse documents\")\n        self.validator = self.session.new_branch(name=\"validator\", system=\"Validate data\")\n\n    async def process(self, document):\n        parsed = await self.parser.chat(f\"Parse: {document}\")\n        validated = await self.validator.chat(f\"Validate: {parsed}\")\n        return validated\n</code></pre> <p>This pattern provides clean encapsulation and makes it easy to manage long-lived agent systems with consistent behavior.</p>"},{"location":"core-concepts/sessions-and-branches/#branch-factory","title":"Branch Factory","text":"<p>Use factory functions to create consistently configured Branches with specialized roles:</p> <pre><code>from lionagi import Branch, iModel\n\ndef create_specialist(role: str, domain: str) -&gt; Branch:\n    return Branch(\n        name=f\"{role}_{domain}\",\n        chat_model=iModel(provider=\"openai\", model=\"gpt-4\"),\n        system=f\"{role.title()} expert in {domain}\"\n    )\n\n# Usage\nresearcher = create_specialist(\"researcher\", \"finance\")\nanalyst = create_specialist(\"analyst\", \"data\")\n</code></pre> <p>Factory functions ensure consistent configuration across similar agent types while making it easy to customize roles and domains.</p>"},{"location":"core-concepts/sessions-and-branches/#memory-management","title":"Memory Management","text":"<p>For applications serving multiple users, implement per-user Branch isolation to maintain separate conversation contexts:</p> <pre><code>from lionagi import Session\n\nsession = Session(name=\"conversation_mgr\")\nuser_branches = {}\n\nasync def get_user_branch(user_id: str):\n    if user_id not in user_branches:\n        user_branches[user_id] = session.new_branch(\n            name=f\"user_{user_id}\",\n            system=\"Helpful assistant\"\n        )\n    return user_branches[user_id]\n\n# Maintains separate conversation history per user\nbranch = await get_user_branch(\"123\")\nresponse = await branch.chat(\"Hello\")\n</code></pre> <p>This pattern ensures each user has their own isolated agent with persistent memory, preventing cross-contamination of conversation contexts.</p>"},{"location":"core-concepts/sessions-and-branches/#summary","title":"Summary","text":"<p>Sessions and Branches form the foundation of LionAGI's orchestration capabilities. Sessions provide workspace coordination and workflow management, while Branches encapsulate individual agent capabilities with isolated memory and specialized tools. Together, they enable sophisticated multi-agent systems with clear boundaries, predictable behavior, and powerful parallel processing capabilities.</p> <p>Understanding these core abstractions unlocks the full potential of LionAGI for building production-ready AI systems that scale from simple single-agent interactions to complex multi-agent orchestration patterns.</p>"},{"location":"core-concepts/tools-and-functions/","title":"Tools and Functions","text":"<p>Give agents custom capabilities with functions and tools.</p>"},{"location":"core-concepts/tools-and-functions/#function-tools","title":"Function Tools","text":"<pre><code>from lionagi import Branch\n\ndef calculate_sum(a: float, b: float) -&gt; float:\n    return a + b\n\nagent = Branch(tools=[calculate_sum])\nresult = await agent.ReAct(instruct={\"instruction\": \"What is 15 + 27?\"})\n</code></pre>"},{"location":"core-concepts/tools-and-functions/#built-in-tools","title":"Built-in Tools","text":""},{"location":"core-concepts/tools-and-functions/#readertool","title":"ReaderTool","text":"<pre><code>from lionagi.tools.file.reader import ReaderTool\n\nagent = Branch(tools=[ReaderTool])\nawait agent.communicate(\"Read docs/README.md and summarize it\")\n</code></pre>"},{"location":"core-concepts/tools-and-functions/#tool-validation","title":"Tool Validation","text":"<p>For complex tools with input validation:</p> <pre><code>from pydantic import BaseModel\nfrom lionagi.protocols.action.tool import Tool\n\nclass WeatherRequest(BaseModel):\n    city: str\n    units: str = \"celsius\"\n\ndef get_weather(city: str, units: str = \"celsius\") -&gt; dict:\n    return {\"city\": city, \"temperature\": 22, \"units\": units}\n\nweather_tool = Tool(func_callable=get_weather, request_options=WeatherRequest)\nagent = Branch(tools=[weather_tool])\n</code></pre>"},{"location":"core-concepts/tools-and-functions/#multiple-tools","title":"Multiple Tools","text":"<pre><code>def search(query: str) -&gt; str:\n    return f\"Results for: {query}\"\n\ndef calculate(expression: str) -&gt; float:\n    return eval(expression)  # In production, use safe evaluation\n\nagent = Branch(tools=[search, calculate])\n</code></pre> <p>That's it. LionAGI makes tools simple - just pass your functions to the <code>tools</code> parameter.</p>"},{"location":"for-ai-agents/","title":"For AI Agents","text":"<p>Guidance specifically for AI agents working with LionAGI.</p>"},{"location":"for-ai-agents/#agent-resources","title":"Agent Resources","text":"<ul> <li>Orchestration Guide - How to orchestrate   multi-agent workflows</li> <li>Pattern Selection - Choose the right patterns for   your task</li> <li>Claude Code Usage - LionAGI in Claude Code   environment</li> <li>Self-Improvement - Learn and adapt your approaches</li> </ul>"},{"location":"for-ai-agents/#key-concepts-for-ai-agents","title":"Key Concepts for AI Agents","text":"<ul> <li>Think in Graphs: LionAGI uses dependency graphs, not sequential chains</li> <li>Leverage Parallelism: Multiple operations can run simultaneously  </li> <li>Use Builder Pattern: Construct workflows declaratively</li> <li>Handle Context: Each Branch maintains persistent memory</li> </ul>"},{"location":"for-ai-agents/#common-agent-workflows","title":"Common Agent Workflows","text":"<ol> <li>Research &amp; Analysis: Fan-out for parallel research, fan-in for synthesis</li> <li>Multi-Perspective: Different agents provide specialized viewpoints</li> <li>Iterative Refinement: Sequential operations that build on each other</li> <li>Validation Patterns: Multiple agents validate and critique results</li> </ol>"},{"location":"for-ai-agents/#best-practices","title":"Best Practices","text":"<ul> <li>Start with simple patterns before complex orchestration</li> <li>Use appropriate concurrency levels for your resources</li> <li>Monitor costs and performance as you scale</li> <li>Handle errors gracefully with fallback strategies</li> </ul>"},{"location":"for-ai-agents/claude-code-usage/","title":"Using LionAGI from Claude Code","text":"<p>Native integration guide for Claude Code users.</p>"},{"location":"for-ai-agents/claude-code-usage/#basic-setup","title":"Basic Setup","text":"<pre><code>from lionagi import Branch, iModel\n\nBASE_CONFIG = {\n    \"provider\": \"claude_code\",\n    \"endpoint\": \"query_cli\",\n    \"model\": \"sonnet\",\n    \"api_key\": \"dummy_api_key\",\n    \"allowed_tools\": [\"Read\"],\n    \"permission_mode\": \"bypassPermissions\",\n    \"verbose_output\": True,\n    \"cli_display_theme\": \"dark\",\n}\n\n# Create Claude Code branch\nbranch = Branch(\n    chat_model=iModel(cwd=\"lionagi\", **BASE_CONFIG),\n    name=\"cc_agent\"\n)\n\nresponse = await branch.communicate(\"Analyze the codebase structure\")\n</code></pre>"},{"location":"for-ai-agents/claude-code-usage/#workspace-management","title":"Workspace Management","text":"<pre><code>CC_WORKSPACE = \".khive/workspace\"\n\ndef create_cc(\n    subdir: str,\n    model: str = \"sonnet\",\n    verbose_output: bool = True,\n    permission_mode=\"default\",\n    auto_finish: bool = False,\n):\n    return iModel(\n        provider=\"claude_code\",\n        endpoint=\"query_cli\",\n        model=model,\n        ws=f\"{CC_WORKSPACE}/{subdir}\",\n        verbose_output=verbose_output,\n        add_dir=\"../../../\",\n        permission_mode=permission_mode,\n        cli_display_theme=\"light\",\n        auto_finish=auto_finish,\n    )\n</code></pre>"},{"location":"for-ai-agents/claude-code-usage/#multi-agent-with-claude-code","title":"Multi-Agent with Claude Code","text":"<pre><code>from lionagi import Session, Builder\nfrom lionagi.fields import LIST_INSTRUCT_FIELD_MODEL, Instruct\n\nasync def claude_code_orchestration():\n    try:\n        orc_cc = create_cc(\"orchestrator\")\n        orc_branch = Branch(\n            chat_model=orc_cc,\n            parse_model=orc_cc,\n            use_lion_system_message=True,\n            system_datetime=True,\n            name=\"orchestrator\",\n        )\n        session = Session(default_branch=orc_branch)\n\n        builder = Builder(\"CodeAnalysis\")\n        root = builder.add_operation(\n            \"operate\",\n            instruct=Instruct(\n                instruction=\"Analyze codebase and create research tasks\",\n                context=\"project_root\",\n            ),\n            reason=True,\n            field_models=[LIST_INSTRUCT_FIELD_MODEL],\n        )\n\n        result = await session.flow(builder.get_graph())\n        instruct_models = result[\"operation_results\"][root].instruct_models\n\n        # Fan-out to researchers\n        research_nodes = []\n        for i in instruct_models:\n            node = builder.add_operation(\n                \"communicate\",\n                depends_on=[root],\n                chat_model=create_cc(\"researcher\"),\n                **i.to_dict(),\n            )\n            research_nodes.append(node)\n\n        # Execute research\n        await session.flow(builder.get_graph())\n\n        # Synthesis\n        synthesis = builder.add_operation(\n            \"communicate\",\n            depends_on=research_nodes,\n            branch=orc_branch,\n            instruction=\"Synthesize researcher findings\",\n        )\n\n        final_result = await session.flow(builder.get_graph())\n        return final_result[\"operation_results\"][synthesis]\n\n    except Exception as e:\n        print(f\"Error: {e}\")\n        import traceback\n        traceback.print_exc()\n\n# Usage\nresult = await claude_code_orchestration()\n</code></pre>"},{"location":"for-ai-agents/claude-code-usage/#cost-tracking","title":"Cost Tracking","text":"<pre><code>def get_context_with_costs(node_id, builder, session):\n    nonlocal costs\n    graph = builder.get_graph()\n    node = graph.internal_nodes[node_id]\n    branch = session.get_branch(node.branch_id, None)\n\n    if (branch and len(branch.messages) &gt; 0 and \n        isinstance(msg := branch.messages[-1], AssistantResponse)):\n        costs += msg.model_response.get(\"total_cost_usd\") or 0\n        return f\"\"\"\nResponse: {msg.model_response.get(\"result\") or \"Not available\"}\nSummary: {msg.model_response.get(\"summary\") or \"Not available\"}\n        \"\"\".strip()\n\ncosts = 0\n# After execution\nprint(f\"Total cost: ${costs:.4f}\")\n</code></pre>"},{"location":"for-ai-agents/claude-code-usage/#integration-patterns","title":"Integration Patterns","text":"<p>Single Agent Analysis:</p> <pre><code>cc_model = iModel(**BASE_CONFIG)\ninvestigator = Branch(\n    name=\"investigator\", \n    chat_model=cc_model,\n    parse_model=cc_model,\n)\n\nresponse = await investigator.communicate(\n    \"Read into the architecture and explain key components\"\n)\n</code></pre> <p>Multi-Workspace Setup:</p> <pre><code># Orchestrator workspace\norc_model = create_cc(\"orchestrator\")\n\n# Researcher workspaces\nresearchers = [\n    create_cc(f\"researcher_{i}\") \n    for i in range(3)\n]\n</code></pre>"},{"location":"for-ai-agents/claude-code-usage/#common-patterns","title":"Common Patterns","text":"<p>File Analysis:</p> <pre><code>prompt = \"\"\"\nRead the specified directory structure.\nFocus on architecture and design patterns.\nProvide structured analysis of key components.\n\"\"\"\n\nresponse = await branch.communicate(prompt)\n</code></pre> <p>Error Handling:</p> <pre><code>try:\n    result = await session.flow(builder.get_graph())\n    return result[\"operation_results\"][synthesis]\nexcept Exception as e:\n    print(f\"Error: {e}\")\n    import traceback\n    traceback.print_exc()\n    return None\n</code></pre>"},{"location":"for-ai-agents/claude-code-usage/#best-practices","title":"Best Practices","text":"<ul> <li>Use workspace isolation for multi-agent scenarios</li> <li>Track costs with <code>total_cost_usd</code> extraction</li> <li>Handle errors with traceback for debugging</li> <li>Keep configurations clean and reusable</li> <li>Leverage Claude Code's file access capabilities</li> </ul>"},{"location":"for-ai-agents/claude-code-usage/#troubleshooting","title":"Troubleshooting","text":"<p>Permission Issues:</p> <pre><code># Use bypass for development\n\"permission_mode\": \"bypassPermissions\"\n</code></pre> <p>Workspace Conflicts:</p> <pre><code># Separate workspaces per agent\nws=f\"{CC_WORKSPACE}/{unique_subdir}\"\n</code></pre> <p>Cost Monitoring:</p> <pre><code># Extract from model response\ntotal_cost = msg.model_response.get(\"total_cost_usd\", 0)\n</code></pre>"},{"location":"for-ai-agents/orchestration-guide/","title":"Orchestration Guide for AI Agents","text":"<p>Quick Decision: When and how to orchestrate multi-agent workflows</p>"},{"location":"for-ai-agents/orchestration-guide/#pattern-selection","title":"\ud83c\udfaf Pattern Selection","text":"<p>Choose your orchestration pattern based on task characteristics:</p> <pre><code>def select_orchestration_pattern(task):\n    \"\"\"Quick pattern selector for AI agents\"\"\"\n\n    # Multiple viewpoints needed?\n    if task.needs_multiple_perspectives:\n        return \"fan_out_fan_in\"\n\n    # Steps must happen in order?\n    if task.has_sequential_dependencies:\n        return \"sequential_pipeline\"\n\n    # Tasks can run simultaneously?\n    if task.has_independent_subtasks:\n        return \"parallel_execution\"\n\n    # Conditional branching required?\n    if task.requires_conditional_logic:\n        return \"conditional_flows\"\n\n    # Simple single task\n    return \"single_branch\"\n</code></pre>"},{"location":"for-ai-agents/orchestration-guide/#orchestration-rules","title":"\ud83d\udcd0 Orchestration Rules","text":"<p>Simple rules for deciding when to orchestrate:</p>"},{"location":"for-ai-agents/orchestration-guide/#rule-1-single-task-direct-execution","title":"\u2705 Rule 1: Single Task \u2192 Direct Execution","text":"<p>When: Task is self-contained and straightforward Examples: Simple questions, single analysis, direct requests</p> <pre><code>from lionagi import Branch, iModel\nimport asyncio\n\nasync def single_task():\n    \"\"\"One task = one branch, no orchestration needed\"\"\"\n    branch = Branch(\n        chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\")\n    )\n    return await branch.communicate(\"Analyze this code\")\n</code></pre>"},{"location":"for-ai-agents/orchestration-guide/#rule-2-independent-tasks-parallel-execution","title":"\u26a1 Rule 2: Independent Tasks \u2192 Parallel Execution","text":"<p>When: Multiple tasks that don't depend on each other Examples: Code review (security + performance + style), multi-aspect analysis</p> <pre><code>async def parallel_tasks():\n    \"\"\"Independent tasks = run simultaneously\"\"\"\n    branch = Branch(\n        chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\")\n    )\n\n    tasks = [\n        \"Review security\",\n        \"Check performance\", \n        \"Validate style\"\n    ]\n\n    return await asyncio.gather(*[\n        branch.communicate(task) for task in tasks\n    ])\n</code></pre>"},{"location":"for-ai-agents/orchestration-guide/#rule-3-dependencies-builder-graph","title":"\ud83d\udd17 Rule 3: Dependencies \u2192 Builder Graph","text":"<p>When: Tasks depend on results from previous tasks Examples: Analysis \u2192 Recommendations \u2192 Implementation</p> <pre><code>async def dependent_tasks():\n    \"\"\"Sequential dependencies = Builder with graph\"\"\"\n    session = Session()\n    builder = Builder(\"workflow\")\n    branch = Branch(chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"))\n\n    # Chain of dependencies\n    step1 = builder.add_operation(\"communicate\", branch=branch,\n        instruction=\"Analyze architecture\")\n\n    step2 = builder.add_operation(\"communicate\", branch=branch,\n        instruction=\"Provide recommendations\",\n        depends_on=[step1])\n\n    step3 = builder.add_operation(\"communicate\", branch=branch,\n        instruction=\"Create implementation plan\",\n        depends_on=[step2])\n\n    return await session.flow(builder.get_graph())\n</code></pre>"},{"location":"for-ai-agents/orchestration-guide/#rule-4-multiple-perspectives-fan-outin","title":"\ud83c\udf1f Rule 4: Multiple Perspectives \u2192 Fan-Out/In","text":"<p>When: Need different expert viewpoints synthesized Examples: Security + Performance + Maintainability review</p> <pre><code>async def multiple_perspectives():\n    \"\"\"Different viewpoints = specialized branches + synthesis\"\"\"\n    session = Session()\n    builder = Builder(\"perspectives\")\n\n    # Create specialized experts\n    experts = {\n        \"security\": Branch(chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"),\n                          system=\"Security expert\"),\n        \"performance\": Branch(chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"),\n                             system=\"Performance expert\"),\n        \"quality\": Branch(chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"),\n                         system=\"Code quality expert\")\n    }\n\n    # Fan-out: Parallel expert analysis\n    analyses = []\n    for name, expert in experts.items():\n        analyses.append(\n            builder.add_operation(\"communicate\", branch=expert,\n                                 instruction=f\"{name} analysis\")\n        )\n\n    # Fan-in: Synthesize all perspectives\n    synthesis = builder.add_aggregation(\n        \"communicate\", branch=experts[\"security\"],\n        source_node_ids=analyses,\n        instruction=\"Synthesize all analyses\"\n    )\n\n    return await session.flow(builder.get_graph(), max_concurrent=3)\n</code></pre>"},{"location":"for-ai-agents/orchestration-guide/#ready-to-use-templates","title":"\ud83d\udccb Ready-to-Use Templates","text":"<p>Copy and adapt these patterns:</p>"},{"location":"for-ai-agents/orchestration-guide/#parallel-tasks","title":"Parallel Tasks","text":"<pre><code># Run multiple tasks simultaneously\ntasks = [\"Review security\", \"Check performance\", \"Validate style\"]\nresults = await asyncio.gather(*[\n    branch.communicate(task) for task in tasks\n])\n</code></pre>"},{"location":"for-ai-agents/orchestration-guide/#sequential-pipeline","title":"Sequential Pipeline","text":"<pre><code># Each step depends on the previous\nsession = Session()\nbuilder = Builder(\"pipeline\")\nbranch = Branch(chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"))\n    session.include_branches([branch])\n\n    previous_step = None\n    for i, step in enumerate(pipeline_steps):\n        step_op = builder.add_operation(\n            \"communicate\", branch=branch,\n            instruction=step,\n            depends_on=[previous_step] if previous_step else None\n        )\n        previous_step = step_op\n\n    return await session.flow(builder.get_graph())\n</code></pre>"},{"location":"for-ai-agents/orchestration-guide/#template-3-multi-expert-analysis","title":"Template 3: Multi-Expert Analysis","text":"<pre><code>async def multi_expert_template(task: str, expert_roles: list[str]):\n    \"\"\"Template for multiple expert perspectives\"\"\"\n    session = Session()\n    builder = Builder(\"multi_expert\")\n\n    # Create expert branches\n    experts = []\n    analyses = []\n\n    for role in expert_roles:\n        expert = Branch(\n            chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"),\n            system=f\"You are a {role}\"\n        )\n        experts.append(expert)\n\n        analysis = builder.add_operation(\n            \"communicate\", branch=expert,\n            instruction=f\"{role} analysis: {task}\"\n        )\n        analyses.append(analysis)\n\n    session.include_branches(experts)\n\n    # Synthesize all expert analyses\n    synthesis = builder.add_aggregation(\n        \"communicate\", branch=experts[0],\n        source_node_ids=analyses,\n        instruction=\"Combine all expert analyses\"\n    )\n\n    return await session.flow(builder.get_graph(), max_concurrent=len(experts))\n</code></pre>"},{"location":"for-ai-agents/orchestration-guide/#template-4-research-analysis-report","title":"Template 4: Research \u2192 Analysis \u2192 Report","text":"<pre><code>async def research_analysis_report_template(topic: str):\n    \"\"\"Template for research-analysis-report workflow\"\"\"\n    session = Session()\n    builder = Builder(\"research_workflow\")\n\n    branch = Branch(\n        chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\")\n    )\n    session.include_branches([branch])\n\n    # Phase 1: Research\n    research = builder.add_operation(\n        \"communicate\", branch=branch,\n        instruction=f\"Research comprehensive information about {topic}\"\n    )\n\n    # Phase 2: Analysis\n    analysis = builder.add_operation(\n        \"communicate\", branch=branch,\n        instruction=\"Analyze the research findings for key insights\",\n        depends_on=[research]\n    )\n\n    # Phase 3: Report\n    report = builder.add_operation(\n        \"communicate\", branch=branch,\n        instruction=\"Create executive summary report\",\n        depends_on=[analysis]\n    )\n\n    return await session.flow(builder.get_graph())\n</code></pre>"},{"location":"for-ai-agents/orchestration-guide/#success-indicators","title":"\u2705 Success Indicators","text":"<p>Know when you've made the right choice:</p> Pattern Used Good Sign Bad Sign Single Task Fast, direct answer Incomplete or needs multiple tries Parallel All tasks complete quickly Tasks waiting on each other Sequential Each step builds properly Later steps lack context Fan-Out/In Rich synthesis from experts Conflicting or redundant views"},{"location":"for-ai-agents/orchestration-guide/#quick-decision-checklist","title":"Quick Decision Checklist","text":"<ul> <li>\u2705 Correct Pattern: Task completes efficiently with quality results</li> <li>\u26a0\ufe0f Maybe Wrong: Taking longer than expected but still produces results</li> <li>\u274c Wrong Pattern: Failed operations, timeout, or poor quality output</li> </ul>"},{"location":"for-ai-agents/orchestration-guide/#learn-and-adapt","title":"\ud83d\udcca Learn and Adapt","text":"<p>Track what works:</p> <ol> <li>Record Pattern Performance</li> <li>Which patterns work best for which tasks</li> <li>Average completion time</li> <li> <p>Success rate</p> </li> <li> <p>Adapt Based on Results</p> </li> <li>If parallel is slow \u2192 try sequential</li> <li>If single task incomplete \u2192 try multi-perspective</li> <li>If synthesis poor \u2192 add more experts</li> </ol>"},{"location":"for-ai-agents/orchestration-guide/#quick-reference","title":"\ud83d\ude80 Quick Reference","text":""},{"location":"for-ai-agents/orchestration-guide/#do-orchestrate-when","title":"\u2705 DO Orchestrate When","text":"<ul> <li>Multiple viewpoints needed \u2192 Fan-out/in pattern</li> <li>Tasks can run in parallel \u2192 Asyncio.gather</li> <li>Steps depend on each other \u2192 Builder with dependencies</li> <li>Complex workflow \u2192 Full orchestration graph</li> </ul>"},{"location":"for-ai-agents/orchestration-guide/#dont-orchestrate-when","title":"\u274c DON'T Orchestrate When","text":"<ul> <li>Single simple question \u2192 Direct execution</li> <li>No parallelism benefit \u2192 Keep it simple</li> <li>Quick conversation \u2192 Direct branch chat</li> </ul>"},{"location":"for-ai-agents/orchestration-guide/#remember","title":"\ud83d\udca1 Remember","text":"<p>Orchestration is a tool, not a requirement. Start simple, add complexity only when needed.</p>"},{"location":"for-ai-agents/pattern-selection/","title":"Pattern Selection Guide","text":"<p>Choosing the right orchestration pattern for the task.</p>"},{"location":"for-ai-agents/pattern-selection/#quick-decision-framework","title":"Quick Decision Framework","text":"<pre><code># Single simple task \u2192 Direct execution\nif single_task and simple:\n    result = await branch.communicate(instruction)\n\n# Multiple independent tasks \u2192 asyncio.gather\nif multiple_tasks and independent:\n    results = await asyncio.gather(*[branch.communicate(task) for task in tasks])\n\n# Complex workflow with dependencies \u2192 Builder\nif dependencies or aggregation_needed:\n    builder = Builder(\"workflow\")\n    # ... build graph\n    result = await session.flow(builder.get_graph())\n</code></pre>"},{"location":"for-ai-agents/pattern-selection/#pattern-characteristics","title":"Pattern Characteristics","text":""},{"location":"for-ai-agents/pattern-selection/#direct-execution","title":"Direct Execution","text":"<p>Best for single, straightforward tasks.</p> <pre><code>from lionagi import Branch, iModel\nimport asyncio\n\nasync def direct_pattern():\n    \"\"\"Use for: Single analysis, simple questions, quick tasks\"\"\"\n    branch = Branch(\n        chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\")\n    )\n\n    # Perfect for single operations\n    result = await branch.communicate(\"Analyze this market trend\")\n    return result\n\nasyncio.run(direct_pattern())\n</code></pre> <p>When to use:</p> <ul> <li>Single analysis or question</li> <li>No dependencies on other operations</li> <li>Quick, standalone tasks</li> <li>Conversational interactions</li> </ul>"},{"location":"for-ai-agents/pattern-selection/#parallel-execution-asynciogather","title":"Parallel Execution (asyncio.gather)","text":"<p>Best for multiple independent tasks.</p> <pre><code>async def parallel_pattern():\n    \"\"\"Use for: Independent parallel tasks, bulk processing\"\"\"\n    branch = Branch(\n        chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\")\n    )\n\n    tasks = [\n        \"Analyze market trends\",\n        \"Research competitors\", \n        \"Evaluate risks\",\n        \"Assess opportunities\"\n    ]\n\n    # Perfect for independent parallel work\n    results = await asyncio.gather(*[\n        branch.communicate(task) for task in tasks\n    ])\n\n    return results\n\nasyncio.run(parallel_pattern())\n</code></pre> <p>When to use:</p> <ul> <li>Multiple independent tasks</li> <li>No dependencies between operations</li> <li>Bulk processing</li> <li>Maximum parallelism needed</li> </ul>"},{"location":"for-ai-agents/pattern-selection/#builder-graphs","title":"Builder Graphs","text":"<p>Best for complex workflows with dependencies.</p> <pre><code>from lionagi import Session, Builder\n\nasync def builder_pattern():\n    \"\"\"Use for: Dependencies, aggregation, complex workflows\"\"\"\n    session = Session()\n    builder = Builder(\"analysis\")\n\n    branch = Branch(\n        chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\")\n    )\n    session.include_branches([branch])\n\n    # Step 1: Research\n    research = builder.add_operation(\n        \"communicate\", branch=branch,\n        instruction=\"Research market conditions\"\n    )\n\n    # Step 2: Analysis (depends on research)\n    analysis = builder.add_operation(\n        \"communicate\", branch=branch,\n        instruction=\"Analyze research findings\",\n        depends_on=[research]\n    )\n\n    # Step 3: Synthesis\n    synthesis = builder.add_aggregation(\n        \"communicate\", branch=branch,\n        source_node_ids=[research, analysis],\n        instruction=\"Create final report\"\n    )\n\n    result = await session.flow(builder.get_graph())\n    return result\n\nasyncio.run(builder_pattern())\n</code></pre> <p>When to use:</p> <ul> <li>Operations depend on each other</li> <li>Need to aggregate/synthesize results</li> <li>Multi-phase workflows</li> <li>Complex coordination required</li> </ul>"},{"location":"for-ai-agents/pattern-selection/#pattern-comparison","title":"Pattern Comparison","text":"Pattern Best For Avoid When Complexity Performance Direct Single tasks, conversations Multiple operations Low Fast Gather Independent parallel tasks Dependencies exist Medium Very Fast Builder Complex workflows, dependencies Simple single tasks High Optimized"},{"location":"for-ai-agents/pattern-selection/#selection-examples","title":"Selection Examples","text":""},{"location":"for-ai-agents/pattern-selection/#example-1-simple-analysis","title":"Example 1: Simple Analysis","text":"<pre><code># Task: \"Analyze this code for security issues\"\n# Pattern: Direct execution\n\nasync def security_analysis():\n    branch = Branch(\n        chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"),\n        system=\"Security expert\"\n    )\n    return await branch.communicate(\"Analyze this code for security issues\")\n</code></pre>"},{"location":"for-ai-agents/pattern-selection/#example-2-multiple-independent-reviews","title":"Example 2: Multiple Independent Reviews","text":"<pre><code># Task: Review code for security, performance, style\n# Pattern: Parallel execution\n\nasync def multi_review():\n    security = Branch(system=\"Security reviewer\")\n    performance = Branch(system=\"Performance reviewer\") \n    style = Branch(system=\"Style reviewer\")\n\n    results = await asyncio.gather(\n        security.communicate(\"Review security\"),\n        performance.communicate(\"Review performance\"),\n        style.communicate(\"Review style\")\n    )\n    return results\n</code></pre>"},{"location":"for-ai-agents/pattern-selection/#example-3-research-analysis-report","title":"Example 3: Research \u2192 Analysis \u2192 Report","text":"<pre><code># Task: Multi-step workflow with dependencies\n# Pattern: Builder graph\n\nasync def research_workflow():\n    session = Session()\n    builder = Builder(\"research\")\n\n    branch = Branch(chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"))\n    session.include_branches([branch])\n\n    research = builder.add_operation(\n        \"communicate\", branch=branch,\n        instruction=\"Research topic\"\n    )\n\n    analysis = builder.add_operation(\n        \"communicate\", branch=branch,\n        instruction=\"Analyze findings\",\n        depends_on=[research]\n    )\n\n    report = builder.add_operation(\n        \"communicate\", branch=branch,\n        instruction=\"Write final report\", \n        depends_on=[analysis]\n    )\n\n    return await session.flow(builder.get_graph())\n</code></pre>"},{"location":"for-ai-agents/pattern-selection/#decision-tree","title":"Decision Tree","text":"<pre><code>Is it a single operation?\n\u251c\u2500\u2500 Yes \u2192 Use Direct Execution\n\u2514\u2500\u2500 No \u2192 Are operations independent?\n    \u251c\u2500\u2500 Yes \u2192 Use asyncio.gather\n    \u2514\u2500\u2500 No \u2192 Do you need aggregation/synthesis?\n        \u251c\u2500\u2500 Yes \u2192 Use Builder with aggregation\n        \u2514\u2500\u2500 No \u2192 Use Builder with dependencies\n</code></pre>"},{"location":"for-ai-agents/pattern-selection/#common-mistakes","title":"Common Mistakes","text":""},{"location":"for-ai-agents/pattern-selection/#over-engineering-simple-tasks","title":"Over-engineering Simple Tasks","text":"<pre><code># Bad: Builder for single task\nbuilder = Builder(\"simple\")\nop = builder.add_operation(\"communicate\", branch=branch, instruction=\"Hello\")\nresult = await session.flow(builder.get_graph())\n\n# Good: Direct execution\nresult = await branch.communicate(\"Hello\")\n</code></pre>"},{"location":"for-ai-agents/pattern-selection/#missing-parallelism-opportunities","title":"Missing Parallelism Opportunities","text":"<pre><code># Bad: Sequential when could be parallel\nresult1 = await branch.communicate(\"Task 1\")\nresult2 = await branch.communicate(\"Task 2\")  # Waits for Task 1\n\n# Good: Parallel independent tasks\nresults = await asyncio.gather(\n    branch.communicate(\"Task 1\"),\n    branch.communicate(\"Task 2\")  # Runs in parallel\n)\n</code></pre>"},{"location":"for-ai-agents/pattern-selection/#forcing-dependencies","title":"Forcing Dependencies","text":"<pre><code># Bad: Unnecessary dependencies\nstep2 = builder.add_operation(..., depends_on=[step1])  # Not needed\n\n# Good: Parallel when possible\nstep1 = builder.add_operation(...)  # No depends_on\nstep2 = builder.add_operation(...)  # No depends_on\nsynthesis = builder.add_aggregation(..., source_node_ids=[step1, step2])\n</code></pre>"},{"location":"for-ai-agents/pattern-selection/#fallback-strategy","title":"Fallback Strategy","text":"<p>When unsure, start simple and evolve:</p> <pre><code># 1. Start with direct execution\nresult = await branch.communicate(instruction)\n\n# 2. If you need multiple independent tasks, upgrade to gather\nresults = await asyncio.gather(*tasks)\n\n# 3. If you need dependencies or aggregation, upgrade to Builder\nbuilder = Builder(\"workflow\")\n# ... add operations with dependencies\nresult = await session.flow(builder.get_graph())\n</code></pre>"},{"location":"for-ai-agents/pattern-selection/#performance-guidelines","title":"Performance Guidelines","text":"<ul> <li>Direct: Fastest for single operations</li> <li>Gather: Fastest for independent parallel operations</li> <li>Builder: Optimized for complex workflows, handles concurrency limits</li> </ul> <p>Choose based on your specific coordination needs, not just performance.</p>"},{"location":"for-ai-agents/self-improvement/","title":"Self-Improvement Guide","text":"<p>Learning from execution results to improve orchestration.</p>"},{"location":"for-ai-agents/self-improvement/#execution-metrics-to-track","title":"Execution Metrics to Track","text":"<p>Monitor key metrics from LionAGI workflow executions.</p> <pre><code>from lionagi import Session, Builder, Branch, iModel\nimport time\nimport asyncio\n\nasync def track_execution_metrics():\n    \"\"\"Track key metrics for improvement\"\"\"\n    session = Session()\n    builder = Builder(\"tracked_workflow\")\n\n    branch = Branch(\n        chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\")\n    )\n    session.include_branches([branch])\n\n    # Add operations\n    op1 = builder.add_operation(\"communicate\", branch=branch, instruction=\"Task 1\")\n    op2 = builder.add_operation(\"communicate\", branch=branch, instruction=\"Task 2\")\n    synthesis = builder.add_aggregation(\n        \"communicate\", branch=branch,\n        source_node_ids=[op1, op2],\n        instruction=\"Synthesize results\"\n    )\n\n    # Execute and track metrics\n    start_time = time.time()\n    result = await session.flow(builder.get_graph(), max_concurrent=2, verbose=True)\n    execution_time = time.time() - start_time\n\n    # Extract metrics\n    metrics = {\n        \"execution_time\": execution_time,\n        \"completed_operations\": len(result[\"completed_operations\"]),\n        \"failed_operations\": len(result[\"skipped_operations\"]),\n        \"success_rate\": len(result[\"completed_operations\"]) / (len(result[\"completed_operations\"]) + len(result[\"skipped_operations\"])),\n        \"parallel_efficiency\": len(result[\"completed_operations\"]) / execution_time,\n        \"pattern_used\": \"builder_graph\"\n    }\n\n    print(f\"Metrics: {metrics}\")\n    return metrics\n\nasyncio.run(track_execution_metrics())\n</code></pre>"},{"location":"for-ai-agents/self-improvement/#pattern-performance-analysis","title":"Pattern Performance Analysis","text":"<p>Compare different orchestration patterns for similar tasks.</p> <pre><code>async def pattern_comparison():\n    \"\"\"Compare patterns for the same task set\"\"\"\n\n    tasks = [\"Analyze market\", \"Research competitors\", \"Assess risks\"]\n    results = {}\n\n    # Pattern 1: Sequential execution\n    start_time = time.time()\n    branch = Branch(chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"))\n    sequential_results = []\n    for task in tasks:\n        result = await branch.communicate(task)\n        sequential_results.append(result)\n\n    results[\"sequential\"] = {\n        \"time\": time.time() - start_time,\n        \"pattern\": \"sequential\",\n        \"results\": len(sequential_results)\n    }\n\n    # Pattern 2: Parallel execution\n    start_time = time.time()\n    parallel_results = await asyncio.gather(*[\n        branch.communicate(task) for task in tasks\n    ])\n\n    results[\"parallel\"] = {\n        \"time\": time.time() - start_time,\n        \"pattern\": \"asyncio_gather\", \n        \"results\": len(parallel_results)\n    }\n\n    # Pattern 3: Builder graph\n    start_time = time.time()\n    session = Session()\n    builder = Builder(\"comparison\")\n    session.include_branches([branch])\n\n    ops = [builder.add_operation(\"communicate\", branch=branch, instruction=task) for task in tasks]\n    synthesis = builder.add_aggregation(\"communicate\", branch=branch, source_node_ids=ops, instruction=\"Combine\")\n\n    graph_result = await session.flow(builder.get_graph(), max_concurrent=3)\n\n    results[\"builder\"] = {\n        \"time\": time.time() - start_time,\n        \"pattern\": \"builder_graph\",\n        \"results\": len(graph_result[\"completed_operations\"])\n    }\n\n    # Analysis\n    print(\"Pattern Performance Comparison:\")\n    for pattern, metrics in results.items():\n        print(f\"{pattern}: {metrics['time']:.2f}s, {metrics['results']} results\")\n\n    # Best pattern for this task type\n    fastest = min(results.items(), key=lambda x: x[1][\"time\"])\n    print(f\"Best pattern: {fastest[0]} ({fastest[1]['time']:.2f}s)\")\n\n    return results\n\nasyncio.run(pattern_comparison())\n</code></pre>"},{"location":"for-ai-agents/self-improvement/#failure-analysis-and-learning","title":"Failure Analysis and Learning","text":"<p>Learn from execution failures to improve future orchestration.</p> <pre><code>async def failure_analysis():\n    \"\"\"Analyze failures to improve future executions\"\"\"\n\n    failure_log = []\n\n    # Simulate different failure scenarios\n    test_scenarios = [\n        {\"pattern\": \"direct\", \"task\": \"Complex multi-step analysis\"},\n        {\"pattern\": \"gather\", \"task\": \"Sequential dependent tasks\"},\n        {\"pattern\": \"builder\", \"task\": \"Simple single question\"}\n    ]\n\n    for scenario in test_scenarios:\n        try:\n            if scenario[\"pattern\"] == \"direct\":\n                branch = Branch(chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"))\n                # This might fail for complex tasks\n                result = await branch.communicate(scenario[\"task\"])\n\n            elif scenario[\"pattern\"] == \"gather\":\n                # This might fail for dependent tasks\n                branch = Branch(chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"))\n                results = await asyncio.gather(*[\n                    branch.communicate(\"Step 1 of analysis\"),\n                    branch.communicate(\"Step 2 that depends on Step 1\")  # Problem: no dependency\n                ])\n\n            elif scenario[\"pattern\"] == \"builder\":\n                # This might be overkill for simple tasks\n                session = Session()\n                builder = Builder(\"simple\")\n                branch = Branch(chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"))\n                session.include_branches([branch])\n\n                op = builder.add_operation(\"communicate\", branch=branch, instruction=\"What is 2+2?\")\n                result = await session.flow(builder.get_graph())\n\n            print(f\"\u2713 {scenario['pattern']} pattern worked for: {scenario['task']}\")\n\n        except Exception as e:\n            failure_info = {\n                \"pattern\": scenario[\"pattern\"],\n                \"task\": scenario[\"task\"], \n                \"error\": str(e),\n                \"lesson\": self.analyze_failure(scenario[\"pattern\"], scenario[\"task\"], str(e))\n            }\n            failure_log.append(failure_info)\n            print(f\"\u2717 {scenario['pattern']} failed: {e}\")\n\n    return failure_log\n\ndef analyze_failure(pattern, task, error):\n    \"\"\"Extract lessons from failures\"\"\"\n    lessons = {\n        (\"direct\", \"multi-step\"): \"Use Builder for complex workflows with multiple steps\",\n        (\"gather\", \"dependent\"): \"Use Builder with dependencies for sequential tasks\",\n        (\"builder\", \"simple\"): \"Use direct execution for simple single tasks\"\n    }\n\n    # Simple pattern matching for lessons\n    for key, lesson in lessons.items():\n        if key[0] in pattern.lower() and any(word in task.lower() for word in key[1].split(\"-\")):\n            return lesson\n\n    return \"Review pattern selection guide\"\n\nasyncio.run(failure_analysis())\n</code></pre>"},{"location":"for-ai-agents/self-improvement/#optimization-loop","title":"Optimization Loop","text":"<p>Implement continuous improvement based on execution history.</p> <pre><code>class ExecutionOptimizer:\n    \"\"\"Learn from execution patterns and optimize future choices\"\"\"\n\n    def __init__(self):\n        self.execution_history = []\n        self.pattern_performance = {}\n\n    def record_execution(self, task_type: str, pattern: str, metrics: dict):\n        \"\"\"Record execution results for learning\"\"\"\n        execution = {\n            \"task_type\": task_type,\n            \"pattern\": pattern,\n            \"execution_time\": metrics.get(\"execution_time\", 0),\n            \"success_rate\": metrics.get(\"success_rate\", 0),\n            \"efficiency\": metrics.get(\"parallel_efficiency\", 0)\n        }\n\n        self.execution_history.append(execution)\n\n        # Update pattern performance\n        key = (task_type, pattern)\n        if key not in self.pattern_performance:\n            self.pattern_performance[key] = []\n        self.pattern_performance[key].append(execution)\n\n    def recommend_pattern(self, task_type: str) -&gt; str:\n        \"\"\"Recommend best pattern based on historical performance\"\"\"\n\n        # Find all patterns used for this task type\n        relevant_patterns = {}\n        for (stored_task, pattern), executions in self.pattern_performance.items():\n            if stored_task == task_type:\n                avg_time = sum(e[\"execution_time\"] for e in executions) / len(executions)\n                avg_success = sum(e[\"success_rate\"] for e in executions) / len(executions)\n\n                # Score: balance speed and success rate\n                score = avg_success * 0.7 + (1/avg_time) * 0.3\n                relevant_patterns[pattern] = score\n\n        if relevant_patterns:\n            best_pattern = max(relevant_patterns.items(), key=lambda x: x[1])\n            return best_pattern[0]\n\n        # Default fallback\n        return \"direct\"\n\n    def get_insights(self) -&gt; dict:\n        \"\"\"Generate insights from execution history\"\"\"\n        if not self.execution_history:\n            return {\"insight\": \"No execution history available\"}\n\n        # Pattern usage frequency\n        pattern_usage = {}\n        for execution in self.execution_history:\n            pattern = execution[\"pattern\"]\n            pattern_usage[pattern] = pattern_usage.get(pattern, 0) + 1\n\n        # Average performance by pattern\n        pattern_performance = {}\n        for execution in self.execution_history:\n            pattern = execution[\"pattern\"]\n            if pattern not in pattern_performance:\n                pattern_performance[pattern] = []\n            pattern_performance[pattern].append(execution[\"execution_time\"])\n\n        avg_performance = {\n            pattern: sum(times) / len(times) \n            for pattern, times in pattern_performance.items()\n        }\n\n        return {\n            \"most_used_pattern\": max(pattern_usage.items(), key=lambda x: x[1])[0],\n            \"fastest_pattern\": min(avg_performance.items(), key=lambda x: x[1])[0],\n            \"total_executions\": len(self.execution_history),\n            \"pattern_usage\": pattern_usage,\n            \"avg_performance\": avg_performance\n        }\n\nasync def optimization_example():\n    \"\"\"Example of using the optimization loop\"\"\"\n    optimizer = ExecutionOptimizer()\n\n    # Simulate some executions\n    optimizer.record_execution(\"analysis\", \"direct\", {\"execution_time\": 2.0, \"success_rate\": 1.0})\n    optimizer.record_execution(\"analysis\", \"gather\", {\"execution_time\": 1.5, \"success_rate\": 0.9})\n    optimizer.record_execution(\"multi_step\", \"builder\", {\"execution_time\": 3.0, \"success_rate\": 1.0})\n    optimizer.record_execution(\"multi_step\", \"gather\", {\"execution_time\": 2.0, \"success_rate\": 0.6})\n\n    # Get recommendations\n    print(f\"Recommended pattern for 'analysis': {optimizer.recommend_pattern('analysis')}\")\n    print(f\"Recommended pattern for 'multi_step': {optimizer.recommend_pattern('multi_step')}\")\n\n    # Get insights\n    insights = optimizer.get_insights()\n    print(f\"Insights: {insights}\")\n\n    return optimizer\n\nasyncio.run(optimization_example())\n</code></pre>"},{"location":"for-ai-agents/self-improvement/#knowledge-persistence","title":"Knowledge Persistence","text":"<p>Save and retrieve learned orchestration patterns.</p> <pre><code>import json\nimport os\n\nclass PatternKnowledge:\n    \"\"\"Persist learned orchestration knowledge\"\"\"\n\n    def __init__(self, knowledge_file: str = \"orchestration_knowledge.json\"):\n        self.knowledge_file = knowledge_file\n        self.knowledge = self.load_knowledge()\n\n    def save_pattern_success(self, task_pattern: str, orchestration_pattern: str, metrics: dict):\n        \"\"\"Save successful pattern combination\"\"\"\n        if \"successful_patterns\" not in self.knowledge:\n            self.knowledge[\"successful_patterns\"] = {}\n\n        key = f\"{task_pattern}:{orchestration_pattern}\"\n        if key not in self.knowledge[\"successful_patterns\"]:\n            self.knowledge[\"successful_patterns\"][key] = []\n\n        self.knowledge[\"successful_patterns\"][key].append(metrics)\n        self.save_knowledge()\n\n    def save_pattern_failure(self, task_pattern: str, orchestration_pattern: str, error: str):\n        \"\"\"Save failed pattern combination to avoid repeating\"\"\"\n        if \"failed_patterns\" not in self.knowledge:\n            self.knowledge[\"failed_patterns\"] = {}\n\n        key = f\"{task_pattern}:{orchestration_pattern}\"\n        if key not in self.knowledge[\"failed_patterns\"]:\n            self.knowledge[\"failed_patterns\"][key] = []\n\n        self.knowledge[\"failed_patterns\"][key].append(error)\n        self.save_knowledge()\n\n    def get_best_pattern(self, task_pattern: str) -&gt; str:\n        \"\"\"Get best orchestration pattern for task type\"\"\"\n        successful = self.knowledge.get(\"successful_patterns\", {})\n\n        # Find all successful patterns for this task type\n        candidates = {}\n        for key, results in successful.items():\n            stored_task, pattern = key.split(\":\", 1)\n            if stored_task == task_pattern:\n                # Average success metrics\n                avg_time = sum(r.get(\"execution_time\", 10) for r in results) / len(results)\n                avg_success = sum(r.get(\"success_rate\", 0) for r in results) / len(results)\n\n                score = avg_success * 0.8 + (1/avg_time) * 0.2\n                candidates[pattern] = score\n\n        if candidates:\n            return max(candidates.items(), key=lambda x: x[1])[0]\n\n        return \"direct\"  # Default\n\n    def should_avoid_pattern(self, task_pattern: str, orchestration_pattern: str) -&gt; bool:\n        \"\"\"Check if pattern combination should be avoided\"\"\"\n        failed = self.knowledge.get(\"failed_patterns\", {})\n        key = f\"{task_pattern}:{orchestration_pattern}\"\n\n        # Avoid if it has failed multiple times\n        return len(failed.get(key, [])) &gt;= 3\n\n    def load_knowledge(self) -&gt; dict:\n        \"\"\"Load knowledge from file\"\"\"\n        if os.path.exists(self.knowledge_file):\n            with open(self.knowledge_file, \"r\") as f:\n                return json.load(f)\n        return {}\n\n    def save_knowledge(self):\n        \"\"\"Save knowledge to file\"\"\"\n        with open(self.knowledge_file, \"w\") as f:\n            json.dump(self.knowledge, f, indent=2)\n\nasync def knowledge_example():\n    \"\"\"Example of using persistent knowledge\"\"\"\n    knowledge = PatternKnowledge()\n\n    # Save some successful patterns\n    knowledge.save_pattern_success(\n        \"code_review\", \"gather\", \n        {\"execution_time\": 1.5, \"success_rate\": 0.95}\n    )\n\n    knowledge.save_pattern_success(\n        \"research_workflow\", \"builder\",\n        {\"execution_time\": 3.0, \"success_rate\": 1.0}\n    )\n\n    # Save a failure\n    knowledge.save_pattern_failure(\n        \"simple_question\", \"builder\",\n        \"Overkill for simple task\"\n    )\n\n    # Get recommendations\n    print(f\"Best for code_review: {knowledge.get_best_pattern('code_review')}\")\n    print(f\"Best for research_workflow: {knowledge.get_best_pattern('research_workflow')}\")\n    print(f\"Should avoid simple_question+builder: {knowledge.should_avoid_pattern('simple_question', 'builder')}\")\n\n    return knowledge\n\nasyncio.run(knowledge_example())\n</code></pre>"},{"location":"for-ai-agents/self-improvement/#best-practices-for-self-improvement","title":"Best Practices for Self-Improvement","text":""},{"location":"for-ai-agents/self-improvement/#1-track-key-metrics","title":"1. Track Key Metrics","text":"<ul> <li>Execution time</li> <li>Success/failure rates</li> <li>Pattern effectiveness</li> <li>Resource usage</li> </ul>"},{"location":"for-ai-agents/self-improvement/#2-analyze-patterns","title":"2. Analyze Patterns","text":"<ul> <li>Compare different approaches for similar tasks</li> <li>Identify what makes patterns successful</li> <li>Learn from failures and mistakes</li> </ul>"},{"location":"for-ai-agents/self-improvement/#3-build-feedback-loops","title":"3. Build Feedback Loops","text":"<ul> <li>Record execution results</li> <li>Update recommendations based on performance</li> <li>Continuously refine pattern selection</li> </ul>"},{"location":"for-ai-agents/self-improvement/#4-persist-learning","title":"4. Persist Learning","text":"<ul> <li>Save successful pattern combinations</li> <li>Remember failed approaches to avoid repeating</li> <li>Build knowledge base over time</li> </ul> <p>Self-improvement in LionAGI orchestration comes from systematically tracking execution results, analyzing pattern effectiveness, and building persistent knowledge to make better orchestration decisions over time.</p>"},{"location":"includes/abbreviations/","title":"Abbreviations","text":"<p>systems *[RAG]: Retrieval-Augmented Generation *[LLM]: Large Language Model</p> <p>Learning *[NLP]: Natural Language Processing *[DSPy]: Declarative Self-improving Python *[MCP]: Model Context Protocol *[GPU]: Graphics Processing Unit *[CPU]: Central Processing Unit *[RAM]: Random Access Memory *[SDK]: Software Development Kit *[UUID]: Universally Unique Identifier *[HTTP]: HyperText Transfer Protocol *[HTTPS]: HyperText Transfer Protocol Secure *[URL]: Uniform Resource Locator *[URI]: Uniform Resource Identifier *[SQL]: Structured Query Language *[NoSQL]: Not Only SQL *[CRUD]: Create, Read, Update, Delete *[REST]: Representational State Transfer *[GraphQL]: Graph Query Language *[JWT]: JSON Web Token *[OAuth]: Open Authorization *[SSL]: Secure Sockets Layer *[TLS]: Transport Layer Security *[CORS]: Cross-Origin Resource Sharing *[YAML]: YAML Ain't Markup Language *[XML]: eXtensible Markup Language *[CSV]: Comma-Separated Values *[PDF]: Portable Document Format *[HTML]: HyperText Markup Language</p> <p>Package Manager *[YARN]: Yet Another Resource Negotiator *[Git]: Global Information Tracker *[GitHub]: Git-based hosting service *[GitLab]: Git-based DevOps platform *[CI]: Continuous Integration *[CD]: Continuous Deployment</p> <p>Cloud Platform *[Azure]: Microsoft Azure *[Docker]: Container platform *[K8s]: Kubernetes *[VM]: Virtual Machine *[OS]: Operating System *[Linux]: Linux Operating System *[macOS]: Mac Operating System *[Windows]: Microsoft Windows</p> <p>General Public License *[MIT]: Massachusetts Institute of Technology *[Apache]: Apache Software Foundation *[BSD]: Berkeley Software Distribution *[ISC]: Internet Software Consortium *[CC]: Creative Commons *[FOSS]: Free and Open Source Software *[OSS]: Open Source Software *[SaaS]: Software as a Service</p> <p>Content Delivery Network *[DNS]: Domain Name System *[IP]: Internet Protocol</p> <p>Simple Mail Transfer Protocol *[FTP]: File Transfer Protocol *[SSH]: Secure Shell *[VPN]: Virtual Private Network *[VPS]: Virtual Private Server *[IDE]: Integrated Development Environment *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[UI]: User Interface *[UX]: User Experience *[QA]: Quality Assurance *[QC]: Quality Control *[TDD]: Test-Driven Development *[BDD]: Behavior-Driven Development *[DDD]: Domain-Driven Design *[SOLID]: Single Responsibility, Open-Closed, Liskov Substitution, Interface Segregation, Dependency Inversion *[DRY]: Don't Repeat Yourself *[KISS]: Keep It Simple, Stupid *[YAGNI]: You Aren't Gonna Need It *[MVC]: Model-View-Controller *[MVP]: Model-View-Presenter *[MVVM]: Model-View-ViewModel *[SPA]: Single Page Application *[PWA]: Progressive Web Application *[RPC]: Remote Procedure Call</p> <p>Peer-to-Peer *[B2B]: Business-to-Business *[B2C]: Business-to-Consumer *[C2C]: Consumer-to-Consumer *[B2G]: Business-to-Government *[G2B]: Government-to-Business *[G2C]: Government-to-Consumer *[IoT]: Internet of Things</p> <p>Mixed Reality *[5G]: Fifth Generation *[4G]: Fourth Generation *[3G]: Third Generation *[2G]: Second Generation *[WiFi]: Wireless Fidelity *[Bluetooth]: Short-range wireless technology *[NFC]: Near Field Communication *[RFID]: Radio Frequency Identification *[GPS]: Global Positioning System *[GNSS]: Global Navigation Satellite System *[ISP]: Internet Service Provider *[ISV]: Independent Software Vendor *[OEM]: Original Equipment Manufacturer *[ODM]: Original Design Manufacturer *[VAR]: Value-Added Reseller *[MSP]: Managed Service Provider *[SLA]: Service Level Agreement *[SLO]: Service Level Objective</p> <p>on Investment *[TCO]: Total Cost of Ownership *[CAPEX]: Capital Expenditure</p> <p>Depreciation, and Amortization *[IPO]: Initial Public Offering *[VC]: Venture Capital *[PE]: Private Equity *[M&amp;A]: Mergers and Acquisitions *[CEO]: Chief Executive Officer *[CTO]: Chief Technology Officer *[CIO]: Chief Information Officer *[CISO]: Chief Information Security Officer *[CFO]: Chief Financial Officer *[COO]: Chief Operating Officer *[CMO]: Chief Marketing Officer *[CPO]: Chief Product Officer *[CDO]: Chief Data Officer *[VP]: Vice President *[SVP]: Senior Vice President *[EVP]: Executive Vice President *[GM]: General Manager</p> <p>Management Professional *[Agile]: Agile methodology *[Scrum]: Scrum framework</p> <p>methodology *[ITIL]: Information Technology Infrastructure Library *[COBIT]: Control Objectives for Information and Related Technologies *[SOX]: Sarbanes-Oxley Act *[GDPR]: General Data Protection Regulation *[HIPAA]: Health Insurance Portability and Accountability Act *[PCI DSS]: Payment Card Industry Data Security Standard *[SOC]: Service Organization Control *[ISO]: International Organization for Standardization *[NIST]: National Institute of Standards and Technology *[IEEE]: Institute of Electrical and Electronics Engineers *[ANSI]: American National Standards Institute *[W3C]: World Wide Web Consortium *[WHATWG]: Web Hypertext Application Technology Working Group</p> <p>Multipurpose Internet Mail Extensions *[ASCII]: American Standard Code for Information Interchange *[UTF-8]: 8-bit Unicode Transformation Format *[UTF-16]: 16-bit Unicode Transformation Format *[UTF-32]: 32-bit Unicode Transformation Format *[Base64]: Base64 encoding *[MD5]: Message Digest 5 *[SHA]: Secure Hash Algorithm *[AES]: Advanced Encryption Standard *[RSA]: Rivest-Shamir-Adleman</p> <p>Algorithm *[PKI]: Public Key Infrastructure *[CA]: Certificate Authority *[CRL]: Certificate Revocation List *[OCSP]: Online Certificate Status Protocol *[SAML]: Security Assertion Markup Language *[LDAP]: Lightweight Directory Access Protocol *[AD]: Active Directory *[RBAC]: Role-Based Access Control *[ABAC]: Attribute-Based Access Control *[MFA]: Multi-Factor Authentication *[2FA]: Two-Factor Authentication *[SSO]: Single Sign-On *[CAPTCHA]: Completely Automated Public Turing test to tell Computers and Humans Apart *[DDoS]: Distributed Denial of Service *[DoS]: Denial of Service *[WAF]: Web Application Firewall *[IDS]: Intrusion Detection System *[IPS]: Intrusion Prevention System</p> <p>Center *[CSIRT]: Computer Security Incident Response Team *[CERT]: Computer Emergency Response Team *[CVE]: Common Vulnerabilities and Exposures *[CVSS]: Common Vulnerability Scoring System *[CWE]: Common Weakness Enumeration</p> <p>Network, and Security *[NIST CSF]: NIST Cybersecurity Framework *[STRIDE]: Spoofing, Tampering, Repudiation, Information Disclosure, Denial of Service, Elevation of Privilege *[DREAD]: Damage, Reproducibility, Exploitability, Affected users, Discoverability</p>"},{"location":"integrations/","title":"Integrations","text":"<p>For Production Systems</p> <p>This section helps you connect LionAGI to real-world systems - databases, APIs, tools, and other AI frameworks.</p> <p>Connect LionAGI with external services and frameworks to build comprehensive AI systems.</p>"},{"location":"integrations/#available-integrations","title":"Available Integrations","text":""},{"location":"integrations/#core-infrastructure","title":"Core Infrastructure","text":"<ul> <li>LLM Providers - OpenAI, Anthropic, local models</li> <li>Vector Stores - Qdrant, Pinecone, Chroma</li> <li>Databases - PostgreSQL, MongoDB, Redis</li> <li>Tools - External APIs and services</li> <li>MCP Servers - Model Context Protocol integration</li> </ul>"},{"location":"integrations/#ai-framework-orchestration","title":"AI Framework Orchestration","text":"<ul> <li>LlamaIndex RAG - Wrap RAG capabilities as operations</li> <li>DSPy Optimization - Embed prompt optimization</li> </ul>"},{"location":"integrations/#meta-orchestration","title":"Meta-Orchestration","text":"<p>LionAGI's key advantage: orchestrate any existing AI framework without code changes.</p> <pre><code># Your existing framework code runs unchanged\nbuilder.add_operation(operation=your_existing_workflow)\n</code></pre> <p>This works with:</p> <ul> <li>LangChain workflows</li> <li>CrewAI crews</li> <li>AutoGen conversations</li> <li>Custom Python functions</li> <li>External APIs</li> </ul>"},{"location":"integrations/#integration-patterns","title":"Integration Patterns","text":"<ul> <li>Wrapper Operations: Embed external tools as LionAGI operations</li> <li>Multi-Framework: Coordinate multiple frameworks in single workflow</li> <li>Gradual Migration: Keep existing code while gaining orchestration benefits</li> </ul>"},{"location":"integrations/#when-you-need-integrations","title":"When You Need Integrations","text":"<p>Use Integrations When:</p> <ul> <li>Persistent data: Need to store workflow results in databases</li> <li>External knowledge: RAG systems with vector stores and knowledge bases</li> <li>Tool augmentation: Agents need access to APIs, calculators, or specialized services</li> <li>Framework combination: Want to orchestrate existing LangChain/CrewAI workflows</li> <li>Production deployment: Need monitoring, logging, and enterprise infrastructure</li> </ul>"},{"location":"integrations/#getting-started","title":"Getting Started","text":"<p>Integration Strategy</p> <p>Start simple: Begin with LLM Providers and Tools Add persistence: Connect Databases for workflow state Scale up: Add Vector Stores for knowledge-intensive workflows Orchestrate: Integrate existing frameworks with meta-orchestration patterns</p>"},{"location":"integrations/databases/","title":"Database Integration","text":"<p>Persisting agent state and data across sessions.</p>"},{"location":"integrations/databases/#postgresql","title":"PostgreSQL","text":""},{"location":"integrations/databases/#setup","title":"Setup","text":"<p>Install PostgreSQL dependencies:</p> <pre><code>uv add lionagi[postgres]\n</code></pre>"},{"location":"integrations/databases/#basic-usage","title":"Basic Usage","text":"<pre><code># Note: Database integration requires custom implementation\n# This is a conceptual example - actual implementation varies\nfrom pydapter.extras.async_postgres_ import AsyncPostgresAdapter\nfrom lionagi import Branch, iModel\nimport asyncio\n\nasync def main():\n    # Database connection using pydapter\n    adapter = AsyncPostgresAdapter(\n        host=\"localhost\",\n        port=5432,\n        database=\"lionagi_db\",\n        username=\"your_user\",\n        password=\"your_password\"\n    )\n\n    # Create branch with model\n    branch = Branch(\n        chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\")\n    )\n\n    # Chat and manually persist if needed\n    result = await branch.communicate(\"Hello, I'm testing database integration\")\n    print(result)\n\n    # Custom persistence logic would go here\n    # await adapter.insert(\"conversations\", {\"message\": result, \"timestamp\": datetime.now()})\n\nasyncio.run(main())\n</code></pre>"},{"location":"integrations/databases/#advanced-configuration","title":"Advanced Configuration","text":"<pre><code>from pydapter import PostgresAdapter\nfrom lionagi.models import Note\nfrom datetime import datetime\n\n# Custom schema for agent conversations\nclass ConversationLog(Note):\n    session_id: str\n    timestamp: datetime\n    user_input: str\n    agent_response: str\n    model_used: str\n    token_count: int\n\n# Database operations\nasync def store_conversation(adapter, log: ConversationLog):\n    await adapter.insert(log)\n\nasync def retrieve_session_history(adapter, session_id: str):\n    query = \"SELECT * FROM conversation_log WHERE session_id = $1 ORDER BY timestamp\"\n    return await adapter.select(query, session_id)\n</code></pre>"},{"location":"integrations/databases/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Connection Issues: Verify PostgreSQL is running: <code>pg_isready -h localhost</code></li> <li>Permission Denied: Check user permissions:   <code>GRANT ALL ON DATABASE lionagi_db TO your_user;</code></li> <li>SSL Errors: Add <code>sslmode=require</code> to connection string for cloud databases</li> </ul>"},{"location":"integrations/databases/#sqlite","title":"SQLite","text":""},{"location":"integrations/databases/#setup_1","title":"Setup","text":"<p>Install SQLite dependencies:</p> <pre><code>uv add lionagi[sqlite]\n</code></pre>"},{"location":"integrations/databases/#basic-usage_1","title":"Basic Usage","text":"<pre><code># Note: Automatic persistence requires custom implementation\nfrom lionagi import Branch, iModel\nimport aiosqlite\nimport asyncio\nfrom datetime import datetime\n\nasync def main():\n    # Create branch with SQLite for manual persistence\n    branch = Branch(\n        chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\")\n    )\n\n    # Chat without automatic persistence\n    response = await branch.communicate(\n        \"Remember this: I prefer technical explanations.\"\n    )\n\n    # Manual persistence example\n    async with aiosqlite.connect(\"lionagi_sessions.db\") as db:\n        await db.execute(\n            \"INSERT INTO conversations (message, timestamp) VALUES (?, ?)\",\n            (str(response), datetime.now())\n        )\n        await db.commit()\n\n    print(response)\n\nasyncio.run(main())\n</code></pre>"},{"location":"integrations/databases/#custom-sqlite-operations","title":"Custom SQLite Operations","text":"<pre><code>import aiosqlite\nfrom lionagi.models import FieldModel\nfrom datetime import datetime\n\nclass AgentMemory(FieldModel):\n    memory_id: str\n    content: str\n    importance: float\n    created_at: datetime\n    accessed_count: int = 0\n\nasync def setup_memory_database():\n    async with aiosqlite.connect(\"agent_memory.db\") as db:\n        await db.execute(\"\"\"\n            CREATE TABLE IF NOT EXISTS memories (\n                memory_id TEXT PRIMARY KEY,\n                content TEXT NOT NULL,\n                importance REAL,\n                created_at TIMESTAMP,\n                accessed_count INTEGER DEFAULT 0\n            )\n        \"\"\")\n        await db.commit()\n\nasync def store_memory(memory: AgentMemory):\n    async with aiosqlite.connect(\"agent_memory.db\") as db:\n        await db.execute(\n            \"INSERT INTO memories VALUES (?, ?, ?, ?, ?)\",\n            (memory.memory_id, memory.content, memory.importance, \n             memory.created_at, memory.accessed_count)\n        )\n        await db.commit()\n\nasync def retrieve_important_memories(threshold: float = 0.8):\n    async with aiosqlite.connect(\"agent_memory.db\") as db:\n        async with db.execute(\n            \"SELECT * FROM memories WHERE importance &gt;= ? ORDER BY importance DESC\",\n            (threshold,)\n        ) as cursor:\n            return await cursor.fetchall()\n</code></pre>"},{"location":"integrations/databases/#troubleshooting_1","title":"Troubleshooting","text":"<ul> <li>Database Locked: Ensure proper connection closing with <code>async with</code></li> <li>No such table: Run schema creation before operations</li> <li>Disk Full: Check available disk space: <code>df -h</code></li> </ul>"},{"location":"integrations/databases/#neo4j","title":"Neo4j","text":""},{"location":"integrations/databases/#setup_2","title":"Setup","text":"<pre><code># Install Neo4j driver\nuv add neo4j\n</code></pre>"},{"location":"integrations/databases/#basic-usage_2","title":"Basic Usage","text":"<pre><code>from neo4j import AsyncGraphDatabase\nfrom lionagi.session import Session\nfrom lionagi.models import FieldModel\nfrom typing import List\n\nclass AgentRelationship(FieldModel):\n    from_agent: str\n    to_agent: str\n    relationship_type: str\n    strength: float\n    created_at: str\n\nclass Neo4jAgentGraph:\n    def __init__(self, uri: str, user: str, password: str):\n        self.driver = AsyncGraphDatabase.driver(uri, auth=(user, password))\n\n    async def close(self):\n        await self.driver.close()\n\n    async def create_agent_node(self, agent_id: str, properties: dict):\n        async with self.driver.session() as session:\n            await session.run(\n                \"CREATE (a:Agent {id: $agent_id, name: $name, type: $type})\",\n                agent_id=agent_id, **properties\n            )\n\n    async def create_relationship(self, rel: AgentRelationship):\n        async with self.driver.session() as session:\n            await session.run(\"\"\"\n                MATCH (a:Agent {id: $from_agent}), (b:Agent {id: $to_agent})\n                CREATE (a)-[:RELATES {type: $rel_type, strength: $strength, created_at: $created_at}]-&gt;(b)\n            \"\"\", \n                from_agent=rel.from_agent, \n                to_agent=rel.to_agent,\n                rel_type=rel.relationship_type,\n                strength=rel.strength,\n                created_at=rel.created_at\n            )\n\n    async def find_connected_agents(self, agent_id: str) -&gt; List[dict]:\n        async with self.driver.session() as session:\n            result = await session.run(\"\"\"\n                MATCH (a:Agent {id: $agent_id})-[r:RELATES]-(b:Agent)\n                RETURN b.id as connected_agent, r.type as relationship, r.strength as strength\n                ORDER BY r.strength DESC\n            \"\"\", agent_id=agent_id)\n            return [record.data() for record in await result.consume()]\n\n# Usage example\nasync def main():\n    graph = Neo4jAgentGraph(\"bolt://localhost:7687\", \"neo4j\", \"password\")\n\n    # Create agent nodes\n    await graph.create_agent_node(\"researcher_001\", {\n        \"name\": \"Research Agent\",\n        \"type\": \"researcher\"\n    })\n\n    await graph.create_agent_node(\"writer_001\", {\n        \"name\": \"Writer Agent\", \n        \"type\": \"writer\"\n    })\n\n    # Create relationship\n    rel = AgentRelationship(\n        from_agent=\"researcher_001\",\n        to_agent=\"writer_001\",\n        relationship_type=\"PROVIDES_DATA\",\n        strength=0.9,\n        created_at=\"2025-01-01T12:00:00Z\"\n    )\n    await graph.create_relationship(rel)\n\n    # Find connections\n    connections = await graph.find_connected_agents(\"researcher_001\")\n    print(connections)\n\n    await graph.close()\n</code></pre>"},{"location":"integrations/databases/#troubleshooting_2","title":"Troubleshooting","text":"<ul> <li>Connection Refused: Check Neo4j service: <code>systemctl status neo4j</code></li> <li>Authentication Failed: Verify username/password in Neo4j browser</li> <li>Cypher Query Errors: Test queries in Neo4j browser first</li> </ul>"},{"location":"integrations/databases/#mongodb","title":"MongoDB","text":""},{"location":"integrations/databases/#setup_3","title":"Setup","text":"<pre><code>uv add motor  # Async MongoDB driver\n</code></pre>"},{"location":"integrations/databases/#basic-usage_3","title":"Basic Usage","text":"<pre><code>from motor.motor_asyncio import AsyncIOMotorClient\nfrom lionagi.models import FieldModel\nfrom lionagi.session import Session\nfrom datetime import datetime\nfrom typing import Optional, List\n\nclass AgentDocument(FieldModel):\n    agent_id: str\n    session_id: str\n    messages: List[dict]\n    metadata: dict\n    created_at: datetime\n    updated_at: datetime\n\nclass MongoAgentStore:\n    def __init__(self, connection_string: str, database_name: str):\n        self.client = AsyncIOMotorClient(connection_string)\n        self.db = self.client[database_name]\n        self.agents = self.db.agents\n        self.sessions = self.db.sessions\n\n    async def close(self):\n        self.client.close()\n\n    async def store_agent_document(self, doc: AgentDocument):\n        result = await self.agents.replace_one(\n            {\"agent_id\": doc.agent_id, \"session_id\": doc.session_id},\n            doc.model_dump(),\n            upsert=True\n        )\n        return str(result.upserted_id) if result.upserted_id else None\n\n    async def get_agent_sessions(self, agent_id: str) -&gt; List[AgentDocument]:\n        cursor = self.agents.find({\"agent_id\": agent_id})\n        docs = await cursor.to_list(length=100)\n        return [AgentDocument(**doc) for doc in docs]\n\n    async def search_by_content(self, query: str) -&gt; List[AgentDocument]:\n        # Text search across message content\n        cursor = self.agents.find({\n            \"$text\": {\"$search\": query}\n        })\n        docs = await cursor.to_list(length=50)\n        return [AgentDocument(**doc) for doc in docs]\n\n# Usage with Branch instead of fabricated Session API\nasync def main():\n    store = MongoAgentStore(\n        \"mongodb://localhost:27017\",\n        \"lionagi_agents\"\n    )\n\n    # Create text index for search\n    await store.agents.create_index([\n        (\"messages.content\", \"text\"),\n        (\"metadata.tags\", \"text\")\n    ])\n\n    # Use with branch\n    branch = Branch(\n        chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\")\n    )\n\n    response = await branch.communicate(\"Explain quantum computing briefly\")\n\n    # Extract messages manually for storage\n    messages = []\n    for msg in getattr(branch, 'messages', []):\n        messages.append({\n            'content': str(msg),\n            'timestamp': datetime.now().isoformat()\n        })\n\n    # Store branch data\n    doc = AgentDocument(\n        agent_id=\"quantum_expert\",\n        session_id=f\"session_{datetime.now().timestamp()}\",\n        messages=messages,\n        metadata={\"topic\": \"quantum_computing\", \"complexity\": \"beginner\"},\n        created_at=datetime.now(),\n        updated_at=datetime.now()\n    )\n\n    await store.store_agent_document(doc)\n\n    # Search previous sessions\n    quantum_sessions = await store.search_by_content(\"quantum\")\n    print(f\"Found {len(quantum_sessions)} sessions about quantum topics\")\n\n    await store.close()\n</code></pre>"},{"location":"integrations/databases/#troubleshooting_3","title":"Troubleshooting","text":"<ul> <li>Connection Timeout: Check MongoDB service: <code>systemctl status mongod</code></li> <li>Authentication Failed: Verify connection string format</li> <li>Index Errors: Ensure unique index constraints are met</li> </ul>"},{"location":"integrations/databases/#database-patterns","title":"Database Patterns","text":""},{"location":"integrations/databases/#session-state-management","title":"Session State Management","text":"<pre><code># Custom session state management example\nfrom lionagi import Branch\nfrom lionagi.models import FieldModel\nfrom typing import Dict, Any, List, Optional\nfrom datetime import datetime, timedelta\nimport json\n\nclass BranchState(FieldModel):\n    branch_id: str\n    system_prompt: str\n    message_history: List[dict]\n    metadata: Dict[str, Any]\n    last_updated: datetime\n\nclass DatabaseBranchManager:\n    def __init__(self, adapter):\n        self.adapter = adapter\n\n    async def save_branch(self, branch: Branch, branch_id: str):\n        # Extract messages manually (example structure)\n        messages = []\n        for msg in getattr(branch, 'messages', []):\n            messages.append({\n                'role': getattr(msg, 'role', 'unknown'),\n                'content': getattr(msg, 'content', str(msg))\n            })\n\n        state = BranchState(\n            branch_id=branch_id,\n            system_prompt=getattr(branch, 'system', ''),\n            message_history=messages,\n            metadata={'model': str(getattr(branch, 'chat_model', None))},\n            last_updated=datetime.now()\n        )\n\n        await self.adapter.upsert(\"branch_states\", state.model_dump())\n\n    async def load_branch_data(self, branch_id: str) -&gt; Optional[BranchState]:\n        state_data = await self.adapter.get(\"branch_states\", branch_id)\n        if not state_data:\n            return None\n\n        return BranchState(**state_data)\n\n    async def cleanup_old_branches(self, days_old: int = 30):\n        cutoff_date = datetime.now() - timedelta(days=days_old)\n        await self.adapter.delete_where(\n            \"branch_states\", \n            \"last_updated &lt; %s\", \n            cutoff_date\n        )\n</code></pre>"},{"location":"integrations/databases/#agent-knowledge-base-schema","title":"Agent Knowledge Base Schema","text":"<pre><code>class Knowledge(FieldModel):\n    knowledge_id: str\n    domain: str  # e.g., \"programming\", \"science\", \"history\"\n    content: str\n    confidence: float  # 0.0 to 1.0\n    source: str\n    validated: bool = False\n    created_by: str  # agent_id\n    created_at: datetime\n    access_count: int = 0\n    last_accessed: Optional[datetime] = None\n\nclass AgentKnowledgeBase:\n    async def add_knowledge(self, knowledge: Knowledge):\n        # Validate and store new knowledge\n        if knowledge.confidence &lt; 0.7:\n            knowledge.validated = False\n\n        await self.adapter.insert(\"knowledge\", knowledge.model_dump())\n\n    async def query_knowledge(self, domain: str, query: str, limit: int = 10):\n        # Semantic search within domain\n        results = await self.adapter.semantic_search(\n            table=\"knowledge\",\n            query_text=query,\n            filters={\"domain\": domain, \"validated\": True},\n            limit=limit\n        )\n\n        # Update access statistics\n        for result in results:\n            await self.adapter.increment(\n                \"knowledge\", \n                result[\"knowledge_id\"], \n                \"access_count\"\n            )\n\n        return results\n</code></pre>"},{"location":"integrations/databases/#performance-optimization","title":"Performance Optimization","text":"<pre><code># Connection pooling for high-throughput scenarios  \n# Note: Use pydapter for actual PostgreSQL connections\nfrom pydapter.extras.async_postgres_ import AsyncPostgresAdapter\nimport asyncpg\n\nclass OptimizedDatabaseManager:\n    def __init__(self, connection_string: str, pool_size: int = 20):\n        self.connection_string = connection_string\n        self.pool_size = pool_size\n        self.pool = None\n\n    async def initialize(self):\n        self.pool = await asyncpg.create_pool(\n            self.connection_string,\n            min_size=5,\n            max_size=self.pool_size,\n            command_timeout=60\n        )\n\n    async def batch_insert(self, table: str, records: List[dict]):\n        async with self.pool.acquire() as conn:\n            async with conn.transaction():\n                # Efficient batch insert\n                await conn.executemany(\n                    f\"INSERT INTO {table} VALUES ($1, $2, $3, $4)\",\n                    [(r['id'], r['data'], r['timestamp'], r['agent_id']) \n                     for r in records]\n                )\n\n    async def close(self):\n        if self.pool:\n            await self.pool.close()\n</code></pre>"},{"location":"integrations/databases/#best-practices","title":"Best Practices","text":"<ol> <li>Use Connection Pooling: For production environments with multiple agents</li> <li>Implement Proper Indexing: Create indexes on frequently queried fields</li> <li>Regular Cleanup: Implement automated cleanup of old sessions and logs</li> <li>Backup Strategy: Regular backups for persistent agent knowledge</li> <li>Monitor Performance: Track query performance and optimize slow queries</li> <li>Handle Failures: Implement retry logic for database operations</li> <li>Data Validation: Validate data before storing in the database</li> <li>Concurrency Control: Use transactions for operations that modify multiple    records</li> </ol>"},{"location":"integrations/dspy-optimization/","title":"DSPy Integration","text":"<p>Use DSPy for prompt optimization within LionAGI workflows.</p>"},{"location":"integrations/dspy-optimization/#why-dspy-lionagi","title":"Why DSPy + LionAGI?","text":"<p>DSPy: Best-in-class prompt optimization, automatic compilation, few-shot learning LionAGI: Superior orchestration, parallel execution, multi-agent coordination</p> <p>Together: Optimized prompts with intelligent orchestration.</p>"},{"location":"integrations/dspy-optimization/#basic-integration","title":"Basic Integration","text":"<p>Use your existing DSPy signatures as custom operations:</p> <pre><code>from lionagi import Branch, Builder, Session\nimport dspy\n\n# Your existing DSPy setup - no changes needed!\nclass AnalyzeMarket(dspy.Signature):\n    \"\"\"Analyze market conditions and provide insights.\"\"\"\n    market_data = dspy.InputField(desc=\"Raw market data\")\n    analysis = dspy.OutputField(desc=\"Market analysis with key insights\")\n\nclass AssessRisk(dspy.Signature):\n    \"\"\"Assess risks based on market analysis.\"\"\"\n    analysis = dspy.InputField(desc=\"Market analysis\")\n    risk_assessment = dspy.OutputField(desc=\"Risk assessment with recommendations\")\n\n# Your DSPy modules - unchanged!\nmarket_analyzer = dspy.ChainOfThought(AnalyzeMarket)\nrisk_assessor = dspy.ChainOfThought(AssessRisk)\n\n# Wrap DSPy modules as LionAGI custom operations\nasync def dspy_market_analysis(branch: Branch, market_data: str, **kwargs):\n    \"\"\"Custom operation using optimized DSPy prompt\"\"\"\n    # Your existing DSPy logic - no changes\n    result = market_analyzer(market_data=market_data)\n\n    # Return to LionAGI workflow\n    return result.analysis\n\nasync def dspy_risk_assessment(branch: Branch, analysis: str, **kwargs):\n    \"\"\"Custom operation using optimized DSPy prompt\"\"\"  \n    # Your existing DSPy logic - no changes\n    result = risk_assessor(analysis=analysis)\n\n    # Return to LionAGI workflow\n    return result.risk_assessment\n\n# Orchestrate DSPy modules with LionAGI\nsession = Session()\nbuilder = Builder(\"dspy_workflow\")\n\n# Phase 1: DSPy market analysis\nanalysis_op = builder.add_operation(\n    operation=dspy_market_analysis,\n    market_data=\"Q3 earnings data, market trends, competitor analysis...\"\n)\n\n# Phase 2: DSPy risk assessment (depends on analysis)\nrisk_op = builder.add_operation(\n    operation=dspy_risk_assessment,\n    analysis=\"{analysis_op}\",  # Use result from previous operation\n    depends_on=[analysis_op]\n)\n\nresult = await session.flow(builder.get_graph())\n</code></pre>"},{"location":"integrations/dspy-optimization/#multi-signature-workflow","title":"Multi-Signature Workflow","text":"<p>Orchestrate multiple DSPy signatures in parallel:</p> <pre><code>import dspy\nfrom lionagi import Session, Builder\n\n# Your existing DSPy signatures - unchanged!\nclass TechnicalAnalysis(dspy.Signature):\n    data = dspy.InputField()\n    technical_insights = dspy.OutputField()\n\nclass FundamentalAnalysis(dspy.Signature): \n    data = dspy.InputField()\n    fundamental_insights = dspy.OutputField()\n\nclass SentimentAnalysis(dspy.Signature):\n    data = dspy.InputField() \n    sentiment_insights = dspy.OutputField()\n\n# Your optimized DSPy modules\ntech_analyzer = dspy.ChainOfThought(TechnicalAnalysis)\nfundamental_analyzer = dspy.ChainOfThought(FundamentalAnalysis) \nsentiment_analyzer = dspy.ChainOfThought(SentimentAnalysis)\n\n# Wrap each as custom operations\nasync def tech_analysis_op(branch: Branch, data: str, **kwargs):\n    result = tech_analyzer(data=data)\n    return result.technical_insights\n\nasync def fundamental_analysis_op(branch: Branch, data: str, **kwargs):\n    result = fundamental_analyzer(data=data)\n    return result.fundamental_insights\n\nasync def sentiment_analysis_op(branch: Branch, data: str, **kwargs):\n    result = sentiment_analyzer(data=data)\n    return result.sentiment_insights\n\n# Orchestrate all analyses in parallel\nsession = Session()\nbuilder = Builder(\"multi_analysis\")\n\nmarket_data = \"AAPL Q3 earnings, market conditions, news sentiment...\"\n\n# Parallel execution of optimized DSPy modules\ntech_op = builder.add_operation(operation=tech_analysis_op, data=market_data)\nfundamental_op = builder.add_operation(operation=fundamental_analysis_op, data=market_data)\nsentiment_op = builder.add_operation(operation=sentiment_analysis_op, data=market_data)\n\n# Synthesize all optimized insights\nsynthesis_op = builder.add_aggregation(\n    \"communicate\",\n    source_node_ids=[tech_op, fundamental_op, sentiment_op],\n    instruction=\"Combine technical, fundamental, and sentiment analysis into investment recommendation\"\n)\n\nresult = await session.flow(builder.get_graph())\n</code></pre>"},{"location":"integrations/dspy-optimization/#dspy-optimization-in-lionagi","title":"DSPy Optimization in LionAGI","text":"<p>Show how to optimize DSPy modules within LionAGI workflows:</p> <pre><code>import dspy\nfrom dspy.datasets import HotPotQA\nfrom lionagi import Session, Builder\n\n# Your DSPy module to optimize\nclass GenerateAnswer(dspy.Signature):\n    \"\"\"Answer questions based on context.\"\"\"\n    context = dspy.InputField(desc=\"Background context\")  \n    question = dspy.InputField(desc=\"Question to answer\")\n    answer = dspy.OutputField(desc=\"Comprehensive answer\")\n\nclass RAGPipeline(dspy.Module):\n    def __init__(self, num_passages=3):\n        super().__init__()\n        self.retrieve = dspy.Retrieve(k=num_passages)\n        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)\n\n    def forward(self, question):\n        context = self.retrieve(question).passages\n        prediction = self.generate_answer(context=context, question=question)\n        return dspy.Prediction(context=context, answer=prediction.answer)\n\n# Optimization within LionAGI workflow\nasync def optimize_dspy_pipeline(branch: Branch, **kwargs):\n    \"\"\"Custom operation that optimizes DSPy pipeline\"\"\"\n\n    # Your existing DSPy optimization - unchanged!\n    trainset = [{'question': q, 'answer': a} for q, a in training_data]\n\n    # Compile/optimize the pipeline\n    teleprompter = dspy.BootstrapFewShot(metric=answer_correctness)\n    compiled_rag = teleprompter.compile(RAGPipeline(), trainset=trainset)\n\n    # Save optimized pipeline\n    compiled_rag.save(\"optimized_rag.json\")\n\n    return \"Pipeline optimization complete\"\n\nasync def use_optimized_pipeline(branch: Branch, question: str, **kwargs):\n    \"\"\"Custom operation using optimized DSPy pipeline\"\"\"\n\n    # Load your optimized pipeline\n    optimized_rag = RAGPipeline()\n    optimized_rag.load(\"optimized_rag.json\")\n\n    # Use optimized pipeline\n    result = optimized_rag(question)\n    return result.answer\n\n# Orchestrate optimization and usage\nsession = Session()\nbuilder = Builder(\"dspy_optimization\")\n\n# Phase 1: Optimize DSPy pipeline\noptimize_op = builder.add_operation(\n    operation=optimize_dspy_pipeline\n)\n\n# Phase 2: Use optimized pipeline (depends on optimization)\nquestions = [\"What is quantum computing?\", \"How does AI work?\", \"Explain blockchain\"]\nfor i, question in enumerate(questions):\n    builder.add_operation(\n        operation=use_optimized_pipeline,\n        question=question,\n        depends_on=[optimize_op]\n    )\n\nresult = await session.flow(builder.get_graph())\n</code></pre>"},{"location":"integrations/dspy-optimization/#hybrid-dspy-lionagi-agents","title":"Hybrid DSPy + LionAGI Agents","text":"<p>Combine DSPy optimization with LionAGI's multi-agent capabilities:</p> <pre><code># DSPy signatures for different agent types\nclass ResearchSignature(dspy.Signature):\n    topic = dspy.InputField()\n    research_findings = dspy.OutputField()\n\nclass AnalysisSignature(dspy.Signature):\n    findings = dspy.InputField()\n    analysis = dspy.OutputField()\n\nclass CritiqueSignature(dspy.Signature):\n    analysis = dspy.InputField()\n    critique = dspy.OutputField()\n\n# Optimized DSPy modules\nresearch_module = dspy.ChainOfThought(ResearchSignature)\nanalysis_module = dspy.ChainOfThought(AnalysisSignature)\ncritique_module = dspy.ChainOfThought(CritiqueSignature)\n\n# LionAGI branches using optimized DSPy prompts\nclass DSPyBranch(Branch):\n    def __init__(self, dspy_module, **kwargs):\n        super().__init__(**kwargs)\n        self.dspy_module = dspy_module\n\n    async def dspy_execute(self, **inputs):\n        \"\"\"Execute DSPy module and return to LionAGI context\"\"\"\n        result = self.dspy_module(**inputs)\n        # Return as standard LionAGI communication\n        return str(result)\n\n# Create specialized branches with optimized prompts\nsession = Session()\nbuilder = Builder(\"hybrid_workflow\")\n\nresearcher = DSPyBranch(research_module, system=\"Research specialist with optimized prompts\")\nanalyst = DSPyBranch(analysis_module, system=\"Analysis specialist with optimized prompts\") \ncritic = DSPyBranch(critique_module, system=\"Critique specialist with optimized prompts\")\n\nsession.include_branches([researcher, analyst, critic])\n\n# Workflow using optimized DSPy prompts in LionAGI orchestration\ntopic = \"Impact of AI on healthcare\"\n\nresearch_op = builder.add_operation(\n    \"dspy_execute\", \n    branch=researcher,\n    topic=topic\n)\n\nanalysis_op = builder.add_operation(\n    \"dspy_execute\",\n    branch=analyst, \n    findings=\"{research_op}\",\n    depends_on=[research_op]\n)\n\ncritique_op = builder.add_operation(\n    \"dspy_execute\",\n    branch=critic,\n    analysis=\"{analysis_op}\",\n    depends_on=[analysis_op]\n)\n\nresult = await session.flow(builder.get_graph())\n</code></pre>"},{"location":"integrations/dspy-optimization/#performance-with-dspy","title":"Performance with DSPy","text":"<p>Optimize both prompts and orchestration:</p> <pre><code>import asyncio\nfrom lionagi import Session, Builder\n\n# Batch processing with optimized DSPy prompts\nasync def batch_dspy_analysis(branch: Branch, questions: list, **kwargs):\n    \"\"\"Process multiple questions with optimized DSPy in parallel\"\"\"\n\n    async def single_analysis(question):\n        # Your optimized DSPy module - unchanged\n        result = optimized_analyzer(question=question)\n        return result.analysis\n\n    # Parallel execution within the operation\n    results = await asyncio.gather(*[\n        single_analysis(q) for q in questions\n    ])\n\n    return results\n\n# High-throughput DSPy processing\nquestions = [f\"Question {i}\" for i in range(100)]\nbatch_size = 10\n\nbuilder = Builder(\"high_throughput_dspy\")\n\nfor i in range(0, len(questions), batch_size):\n    batch = questions[i:i + batch_size]\n    builder.add_operation(\n        operation=batch_dspy_analysis,\n        questions=batch\n    )\n\n# Execute with controlled concurrency\nresult = await session.flow(\n    builder.get_graph(),\n    max_concurrent=5  # Control resource usage\n)\n</code></pre>"},{"location":"integrations/dspy-optimization/#ab-testing-dspy-models","title":"A/B Testing DSPy Models","text":"<p>Compare different DSPy optimizations:</p> <pre><code># Different DSPy optimizations to compare\nasync def dspy_model_a(branch: Branch, input_data: str, **kwargs):\n    # Your Model A optimization\n    return model_a_result\n\nasync def dspy_model_b(branch: Branch, input_data: str, **kwargs): \n    # Your Model B optimization\n    return model_b_result\n\n# A/B test different optimizations in parallel\nbuilder = Builder(\"dspy_ab_test\")\n\ntest_data = \"Sample input for comparison\"\n\nmodel_a_op = builder.add_operation(operation=dspy_model_a, input_data=test_data)\nmodel_b_op = builder.add_operation(operation=dspy_model_b, input_data=test_data)\n\n# Compare results\ncomparison_op = builder.add_aggregation(\n    \"communicate\",\n    source_node_ids=[model_a_op, model_b_op],\n    instruction=\"Compare Model A and Model B results. Which performs better and why?\"\n)\n\nresult = await session.flow(builder.get_graph())\n</code></pre>"},{"location":"integrations/dspy-optimization/#key-benefits","title":"Key Benefits","text":"<ol> <li>Zero Migration: Keep your existing DSPy code unchanged</li> <li>Superior Orchestration: LionAGI handles parallel execution, dependencies</li> <li>Optimization + Orchestration: Best of both worlds</li> <li>A/B Testing: Easy comparison of different optimizations</li> <li>Scalable: Built-in performance controls</li> <li>Hybrid Workflows: Mix DSPy with other AI operations</li> </ol> <p>DSPy provides the prompt optimization, LionAGI provides the orchestration intelligence.</p>"},{"location":"integrations/llamaindex-rag/","title":"LlamaIndex Integration","text":"<p>Use LlamaIndex for RAG (Retrieval-Augmented Generation) within LionAGI workflows.</p>"},{"location":"integrations/llamaindex-rag/#why-llamaindex-lionagi","title":"Why LlamaIndex + LionAGI?","text":"<p>LlamaIndex: Best-in-class RAG capabilities, document indexing, retrieval LionAGI: Superior orchestration, parallel execution, multi-agent coordination</p> <p>Together: Powerful RAG workflows with intelligent orchestration.</p>"},{"location":"integrations/llamaindex-rag/#basic-integration","title":"Basic Integration","text":"<p>Wrap LlamaIndex queries as custom operations:</p> <pre><code>from lionagi import Branch, Builder, Session\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n\n# Your existing LlamaIndex setup - no changes needed!\ndef setup_llamaindex():\n    documents = SimpleDirectoryReader(\"./data\").load_data()\n    index = VectorStoreIndex.from_documents(documents)\n    return index.as_query_engine()\n\nquery_engine = setup_llamaindex()\n\n# Custom operation wrapping LlamaIndex\nasync def rag_query(branch: Branch, question: str, **kwargs):\n    \"\"\"RAG query using LlamaIndex - keeps your existing code\"\"\"\n    # Your LlamaIndex query logic unchanged\n    response = query_engine.query(question)\n\n    # Return in LionAGI format\n    return await branch.chat(f\"Based on this retrieved information: {response}\\n\\nQuestion: {question}\")\n\n# Use in LionAGI workflow\nsession = Session()\nbuilder = Builder(\"rag_workflow\")\n\nrag_op = builder.add_operation(\n    operation=rag_query,\n    question=\"What are the key findings in the research papers?\"\n)\n\nresult = await session.flow(builder.get_graph())\n</code></pre>"},{"location":"integrations/llamaindex-rag/#multi-source-rag","title":"Multi-Source RAG","text":"<p>Orchestrate multiple knowledge sources in parallel:</p> <pre><code>from lionagi import Session, Builder, Branch\n\n# Different RAG sources\ndef setup_financial_rag():\n    # Your financial documents RAG\n    return financial_query_engine\n\ndef setup_technical_rag():\n    # Your technical documents RAG  \n    return technical_query_engine\n\n# Custom operations for each source\nasync def financial_rag(branch: Branch, question: str, **kwargs):\n    query_engine = setup_financial_rag()\n    response = query_engine.query(question)\n    return await branch.chat(f\"Financial data: {response}\\nAnalyze: {question}\")\n\nasync def technical_rag(branch: Branch, question: str, **kwargs):\n    query_engine = setup_technical_rag()\n    response = query_engine.query(question) \n    return await branch.chat(f\"Technical data: {response}\\nAnalyze: {question}\")\n\n# Orchestrate multiple RAG sources in parallel\nsession = Session()\nbuilder = Builder(\"multi_rag\")\n\nquestion = \"What are the technical and financial risks of AI adoption?\"\n\n# Parallel RAG queries\nfinancial_op = builder.add_operation(\n    operation=financial_rag,\n    question=question\n)\n\ntechnical_op = builder.add_operation(\n    operation=technical_rag,\n    question=question\n)\n\n# Synthesize results\nsynthesis = builder.add_aggregation(\n    \"communicate\",\n    source_node_ids=[financial_op, technical_op],\n    instruction=\"Combine financial and technical analysis into comprehensive risk assessment\"\n)\n\nresult = await session.flow(builder.get_graph())\n</code></pre>"},{"location":"integrations/llamaindex-rag/#rag-analysis-pipeline","title":"RAG + Analysis Pipeline","text":"<p>Chain RAG retrieval with specialized analysis:</p> <pre><code>from lionagi import Session, Builder, Branch, iModel\n\nsession = Session()\nbuilder = Builder(\"rag_analysis_pipeline\")\n\n# Specialized analysis branches\nretriever = Branch(system=\"Document retrieval specialist\")\nanalyzer = Branch(\n    chat_model=iModel(provider=\"anthropic\", model=\"claude-3-5-sonnet-20240620\"),\n    system=\"Deep analysis specialist\"\n)\nsynthesizer = Branch(system=\"Report synthesis specialist\")\n\nsession.include_branches([retriever, analyzer, synthesizer])\n\n# Phase 1: RAG retrieval\nretrieval_op = builder.add_operation(\n    operation=rag_query,\n    branch=retriever,\n    question=\"What are current market trends in AI?\"\n)\n\n# Phase 2: Deep analysis (depends on retrieval)\nanalysis_op = builder.add_operation(\n    \"communicate\",\n    branch=analyzer,\n    instruction=\"Provide detailed analysis of the retrieved information\",\n    depends_on=[retrieval_op]\n)\n\n# Phase 3: Final synthesis\nsynthesis_op = builder.add_operation(\n    \"communicate\",\n    branch=synthesizer, \n    instruction=\"Create executive summary of analysis\",\n    depends_on=[analysis_op]\n)\n\nresult = await session.flow(builder.get_graph())\n</code></pre>"},{"location":"integrations/llamaindex-rag/#advanced-multi-modal-rag","title":"Advanced: Multi-Modal RAG","text":"<p>Handle different document types with specialized processing:</p> <pre><code># Different document processors\nasync def pdf_rag(branch: Branch, query: str, **kwargs):\n    # Your LlamaIndex PDF processing\n    pdf_engine = setup_pdf_rag()\n    response = pdf_engine.query(query)\n    return await branch.chat(f\"PDF data: {response}\\nQuery: {query}\")\n\nasync def web_rag(branch: Branch, query: str, **kwargs):\n    # Your LlamaIndex web scraping  \n    web_engine = setup_web_rag()\n    response = web_engine.query(query)\n    return await branch.chat(f\"Web data: {response}\\nQuery: {query}\")\n\nasync def database_rag(branch: Branch, query: str, **kwargs):\n    # Your LlamaIndex database querying\n    db_engine = setup_database_rag()\n    response = db_engine.query(query)\n    return await branch.chat(f\"Database data: {response}\\nQuery: {query}\")\n\n# Orchestrate all sources\nbuilder = Builder(\"multimodal_rag\")\nquery = \"What are the latest developments in AI regulations?\"\n\n# Parallel multi-modal retrieval\npdf_op = builder.add_operation(operation=pdf_rag, query=query)\nweb_op = builder.add_operation(operation=web_rag, query=query)  \ndb_op = builder.add_operation(operation=database_rag, query=query)\n\n# Cross-reference and validate\nvalidator = Branch(system=\"Information validation specialist\")\nvalidation_op = builder.add_operation(\n    \"communicate\",\n    branch=validator,\n    instruction=\"Cross-reference all sources and validate information consistency\",\n    depends_on=[pdf_op, web_op, db_op]\n)\n\nresult = await session.flow(builder.get_graph())\n</code></pre>"},{"location":"integrations/llamaindex-rag/#error-handling-with-rag","title":"Error Handling with RAG","text":"<p>Handle RAG failures gracefully:</p> <pre><code>async def resilient_rag(branch: Branch, question: str, **kwargs):\n    \"\"\"RAG with fallback strategies\"\"\"\n\n    # Try primary RAG source\n    try:\n        primary_engine = setup_primary_rag()\n        response = primary_engine.query(question)\n        return await branch.chat(f\"Retrieved: {response}\\nAnalyze: {question}\")\n\n    except Exception as e:\n        print(f\"Primary RAG failed: {e}\")\n\n        # Fallback to secondary source\n        try:\n            fallback_engine = setup_fallback_rag()\n            response = fallback_engine.query(question)\n            return await branch.chat(f\"Fallback data: {response}\\nAnalyze: {question}\")\n\n        except Exception as e2:\n            print(f\"Fallback RAG failed: {e2}\")\n\n            # Ultimate fallback: use LLM knowledge\n            return await branch.chat(f\"Using model knowledge to answer: {question}\")\n\n# Use resilient RAG in workflow\nbuilder.add_operation(operation=resilient_rag, question=\"Complex query here\")\n</code></pre>"},{"location":"integrations/llamaindex-rag/#performance-optimization","title":"Performance Optimization","text":"<p>Optimize RAG workflows for production:</p> <pre><code>import asyncio\n\nasync def batch_rag_queries(branch: Branch, questions: list, **kwargs):\n    \"\"\"Process multiple RAG queries in parallel\"\"\"\n\n    async def single_rag_query(question):\n        response = query_engine.query(question)\n        return {\"question\": question, \"response\": str(response)}\n\n    # Parallel RAG queries within the operation\n    results = await asyncio.gather(*[\n        single_rag_query(q) for q in questions\n    ])\n\n    # Synthesize all results\n    combined = \"\\n\\n\".join([f\"Q: {r['question']}\\nA: {r['response']}\" for r in results])\n    return await branch.chat(f\"Synthesize these RAG results:\\n{combined}\")\n\n# Use in high-throughput scenarios\nquestions = [\"Question 1\", \"Question 2\", \"Question 3\"]\nbatch_op = builder.add_operation(\n    operation=batch_rag_queries,\n    questions=questions\n)\n</code></pre>"},{"location":"integrations/llamaindex-rag/#key-benefits","title":"Key Benefits","text":"<ol> <li>Zero Migration: Keep your existing LlamaIndex code unchanged</li> <li>Superior Orchestration: LionAGI handles parallel execution, dependencies,    error handling</li> <li>Multi-Source: Easily orchestrate multiple RAG sources</li> <li>Scalable: Built-in performance controls and monitoring</li> <li>Flexible: Mix RAG with other AI operations seamlessly</li> </ol> <p>LlamaIndex provides the RAG capabilities, LionAGI provides the orchestration intelligence.</p>"},{"location":"integrations/llm-providers/","title":"LLM Provider Integration","text":"<p>Comprehensive guide to all supported LLM providers in LionAGI.</p>"},{"location":"integrations/llm-providers/#openai","title":"OpenAI","text":""},{"location":"integrations/llm-providers/#setup","title":"Setup","text":"<p>Set your OpenAI API key:</p> <pre><code>export OPENAI_API_KEY=\"your-api-key-here\"\n</code></pre> <p>Or use a <code>.env</code> file:</p> <pre><code>OPENAI_API_KEY=your-api-key-here\n</code></pre>"},{"location":"integrations/llm-providers/#basic-usage","title":"Basic Usage","text":"<pre><code>from lionagi import Branch, iModel\nimport asyncio\n\nasync def main():\n    # Using GPT-4o-mini (recommended for development)\n    branch = Branch(chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"))\n\n    response = await branch.communicate(\n        \"Explain the difference between async and sync programming in Python\"\n    )\n\n    print(response)\n\nasyncio.run(main())\n</code></pre>"},{"location":"integrations/llm-providers/#advanced-configuration","title":"Advanced Configuration","text":"<pre><code>from lionagi import Branch, iModel\n\n# Custom OpenAI configuration\nconfig = {\n    \"model\": \"gpt-4o-mini\",\n    \"temperature\": 0.7,\n    \"max_tokens\": 2000,\n    \"top_p\": 0.9,\n    \"frequency_penalty\": 0.1,\n    \"presence_penalty\": 0.1\n}\n\nasync def main():\n    session = Session(\n        imodel=\"gpt-4o-mini\",\n        **config\n    )\n\n    # Multi-turn conversation with memory\n    await session.chat(\"I'm building a web scraper in Python.\")\n    response = await session.chat(\"What libraries should I use for handling JavaScript?\")\n\n    print(response)\n</code></pre>"},{"location":"integrations/llm-providers/#streaming-responses","title":"Streaming Responses","text":"<pre><code>from lionagi.session import Session\n\nasync def stream_chat():\n    session = Branch(chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"))\n\n    async for chunk in session.chat_stream(\n        \"Write a short story about AI agents working together\"\n    ):\n        print(chunk.content, end=\"\", flush=True)\n\n    print()  # New line at the end\n\nasyncio.run(stream_chat())\n</code></pre>"},{"location":"integrations/llm-providers/#function-calling","title":"Function Calling","text":"<pre><code>from lionagi.session import Session\nfrom lionagi.tools.base import Tool\nfrom pydantic import Field\nfrom typing import Dict, Any\n\nclass WeatherTool(Tool):\n    \"\"\"Get current weather for a location.\"\"\"\n\n    location: str = Field(description=\"City name\")\n\n    async def call(self) -&gt; Dict[str, Any]:\n        # Simulate API call\n        return {\n            \"location\": self.location,\n            \"temperature\": \"22\u00b0C\",\n            \"condition\": \"Sunny\",\n            \"humidity\": \"45%\"\n        }\n\nasync def function_calling_example():\n    session = Branch(chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"))\n    session.register_tool(WeatherTool)\n\n    response = await session.chat(\n        \"What's the weather like in Tokyo? Format it nicely.\"\n    )\n\n    print(response)\n\nasyncio.run(function_calling_example())\n</code></pre>"},{"location":"integrations/llm-providers/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Rate Limits: Use exponential backoff (built into LionAGI)</li> <li>Token Limits: Monitor token usage with <code>session.get_token_count()</code></li> <li>API Errors: Check API key validity and billing status</li> <li>Timeout Issues: Increase timeout in session configuration</li> </ul>"},{"location":"integrations/llm-providers/#anthropic","title":"Anthropic","text":""},{"location":"integrations/llm-providers/#setup_1","title":"Setup","text":"<pre><code>export ANTHROPIC_API_KEY=\"your-anthropic-key\"\n</code></pre>"},{"location":"integrations/llm-providers/#basic-usage_1","title":"Basic Usage","text":"<pre><code>from lionagi.session import Session\n\nasync def claude_example():\n    # Using Claude 3.5 Sonnet\n    session = Branch(chat_model=iModel(provider=\"anthropic\", model=\"claude-3-5-sonnet-20241022\"))\n\n    response = await session.chat(\n        \"Analyze this code and suggest improvements:\",\n        context=\"\"\"\n        def process_data(data):\n            result = []\n            for item in data:\n                if item &gt; 0:\n                    result.append(item * 2)\n            return result\n        \"\"\"\n    )\n\n    print(response)\n\nasyncio.run(claude_example())\n</code></pre>"},{"location":"integrations/llm-providers/#long-context-processing","title":"Long Context Processing","text":"<pre><code>from lionagi.session import Session\nfrom lionagi.tools.file.reader import ReaderTool\n\nasync def long_document_analysis():\n    session = Branch(chat_model=iModel(provider=\"anthropic\", model=\"claude-3-5-sonnet-20241022\"))\n\n    # Claude excels at long context\n    reader = ReaderTool()\n    document = await reader.read(\"long_document.pdf\")\n\n    response = await session.chat(\n        \"Summarize the key points and identify any inconsistencies in this document:\",\n        context=document.content\n    )\n\n    print(response)\n</code></pre>"},{"location":"integrations/llm-providers/#structured-output-with-claude","title":"Structured Output with Claude","text":"<pre><code>from lionagi.session import Session\nfrom pydantic import BaseModel, Field\nfrom typing import List\n\nclass CodeReview(BaseModel):\n    overall_score: int = Field(description=\"Score from 1-10\")\n    strengths: List[str] = Field(description=\"Code strengths\")\n    weaknesses: List[str] = Field(description=\"Areas for improvement\")\n    recommendations: List[str] = Field(description=\"Specific recommendations\")\n\nasync def structured_code_review():\n    session = Branch(chat_model=iModel(provider=\"anthropic\", model=\"claude-3-5-sonnet-20241022\"))\n\n    response = await session.instruct(\n        instruction=\"Review this Python code and provide structured feedback\",\n        context=\"\"\"\n        class DataProcessor:\n            def __init__(self, data):\n                self.data = data\n                self.processed = False\n\n            def process(self):\n                if not self.processed:\n                    self.data = [x * 2 for x in self.data if x &gt; 0]\n                    self.processed = True\n                return self.data\n        \"\"\",\n        response_format=CodeReview\n    )\n\n    print(f\"Score: {response.overall_score}/10\")\n    print(f\"Strengths: {response.strengths}\")\n    print(f\"Recommendations: {response.recommendations}\")\n\nasyncio.run(structured_code_review())\n</code></pre>"},{"location":"integrations/llm-providers/#troubleshooting_1","title":"Troubleshooting","text":"<ul> <li>Content Policy: Claude has strict content policies</li> <li>API Limits: Monitor usage through Anthropic console</li> <li>Context Windows: Claude 3.5 Sonnet supports up to 200K tokens</li> </ul>"},{"location":"integrations/llm-providers/#ollama-local","title":"Ollama (Local)","text":""},{"location":"integrations/llm-providers/#setup_2","title":"Setup","text":"<p>Install Ollama and dependencies:</p> <pre><code># Install Ollama\ncurl -fsSL https://ollama.ai/install.sh | sh\n\n# Install Python package\nuv add lionagi[ollama]\n\n# Pull a model\nollama pull llama3.2:3b\n</code></pre>"},{"location":"integrations/llm-providers/#basic-usage_2","title":"Basic Usage","text":"<pre><code>from lionagi.session import Session\n\nasync def local_llama_example():\n    # Local Llama model\n    session = Session(\n        imodel=\"llama3.2:3b\",\n        provider=\"ollama\",\n        base_url=\"http://localhost:11434\"\n    )\n\n    response = await session.chat(\n        \"Explain how transformers work in machine learning\"\n    )\n\n    print(response)\n\nasyncio.run(local_llama_example())\n</code></pre>"},{"location":"integrations/llm-providers/#model-management","title":"Model Management","text":"<pre><code>from lionagi.service.connections.providers.ollama_ import OllamaEndpoint\nimport ollama\n\nasync def manage_models():\n    client = ollama.AsyncClient()\n\n    # List available models\n    models = await client.list()\n    print(\"Available models:\")\n    for model in models['models']:\n        print(f\"- {model['name']} ({model['size']} bytes)\")\n\n    # Pull a new model\n    await client.pull(\"codellama:7b\")\n\n    # Use the new model\n    session = Session(\n        imodel=\"codellama:7b\",\n        provider=\"ollama\"\n    )\n\n    response = await session.chat(\n        \"Write a Python function to calculate Fibonacci numbers\"\n    )\n    print(response)\n\nasyncio.run(manage_models())\n</code></pre>"},{"location":"integrations/llm-providers/#custom-model-configuration","title":"Custom Model Configuration","text":"<pre><code>from lionagi.session import Session\n\n# Fine-tuned local model settings\nconfig = {\n    \"temperature\": 0.1,  # Lower for code generation\n    \"top_p\": 0.9,\n    \"top_k\": 40,\n    \"repeat_penalty\": 1.1,\n    \"num_ctx\": 4096,  # Context window\n    \"num_predict\": 1000,  # Max tokens to generate\n}\n\nasync def custom_ollama_config():\n    session = Session(\n        imodel=\"codellama:7b\",\n        provider=\"ollama\",\n        **config\n    )\n\n    response = await session.chat(\n        \"Optimize this SQL query:\",\n        context=\"SELECT * FROM users WHERE age &gt; 18 AND city = 'NYC' ORDER BY name\"\n    )\n\n    print(response)\n</code></pre>"},{"location":"integrations/llm-providers/#troubleshooting_2","title":"Troubleshooting","text":"<ul> <li>Ollama Not Running: Check service: <code>ollama serve</code></li> <li>Model Not Found: Pull model first: <code>ollama pull model_name</code></li> <li>Memory Issues: Use smaller models (3B vs 7B vs 13B parameters)</li> <li>Slow Performance: Ensure GPU drivers are installed for acceleration</li> </ul>"},{"location":"integrations/llm-providers/#google-gemini","title":"Google (Gemini)","text":""},{"location":"integrations/llm-providers/#setup_3","title":"Setup","text":"<pre><code>export GOOGLE_API_KEY=\"your-google-api-key\"\n# or\nexport GEMINI_API_KEY=\"your-gemini-api-key\"\n</code></pre>"},{"location":"integrations/llm-providers/#basic-usage_3","title":"Basic Usage","text":"<pre><code>from lionagi.session import Session\n\nasync def gemini_example():\n    session = Session(\n        imodel=\"gemini-1.5-flash\",\n        provider=\"google\"\n    )\n\n    response = await session.chat(\n        \"Compare Python and Rust for system programming\"\n    )\n\n    print(response)\n\nasyncio.run(gemini_example())\n</code></pre>"},{"location":"integrations/llm-providers/#multimodal-capabilities","title":"Multimodal Capabilities","text":"<pre><code>from lionagi.session import Session\nfrom lionagi.tools.file.reader import ReaderTool\n\nasync def gemini_vision_example():\n    session = Session(\n        imodel=\"gemini-1.5-pro\",\n        provider=\"google\"\n    )\n\n    # Image analysis\n    reader = ReaderTool()\n    image_data = await reader.read(\"diagram.png\")\n\n    response = await session.chat(\n        \"Describe what you see in this image and explain any technical concepts shown\",\n        attachments=[image_data]\n    )\n\n    print(response)\n</code></pre>"},{"location":"integrations/llm-providers/#large-context-processing","title":"Large Context Processing","text":"<pre><code>from lionagi.session import Session\n\nasync def large_context_analysis():\n    # Gemini 1.5 Pro supports up to 2M tokens\n    session = Session(\n        imodel=\"gemini-1.5-pro\",\n        provider=\"google\"\n    )\n\n    # Process entire codebase\n    codebase = await load_entire_codebase()  # Your function\n\n    response = await session.chat(\n        \"Analyze this entire codebase for security vulnerabilities and architectural issues\",\n        context=codebase\n    )\n\n    print(response)\n</code></pre>"},{"location":"integrations/llm-providers/#troubleshooting_3","title":"Troubleshooting","text":"<ul> <li>API Quota: Check Google Cloud Console for quota limits</li> <li>Safety Settings: Adjust safety settings for content filtering</li> <li>Regional Availability: Gemini availability varies by region</li> </ul>"},{"location":"integrations/llm-providers/#custom-providers","title":"Custom Providers","text":""},{"location":"integrations/llm-providers/#adding-new-providers","title":"Adding New Providers","text":"<pre><code>from lionagi.service.connections.endpoint import Endpoint\nfrom lionagi.service.connections.endpoint_config import EndpointConfig\nfrom lionagi.service.connections.providers.types import ProviderConfig\nfrom typing import Dict, Any\n\nclass CustomLLMEndpoint(Endpoint):\n    \"\"\"Custom LLM provider endpoint.\"\"\"\n\n    def __init__(self, api_key: str, base_url: str, **kwargs):\n        config = EndpointConfig(\n            name=\"custom_llm\",\n            provider=\"custom\",\n            base_url=base_url,\n            endpoint=\"v1/chat/completions\",\n            headers={\n                \"Authorization\": f\"Bearer {api_key}\",\n                \"Content-Type\": \"application/json\"\n            },\n            **kwargs\n        )\n        super().__init__(config)\n\n    async def call(self, messages: list, **kwargs) -&gt; Dict[str, Any]:\n        payload = {\n            \"messages\": messages,\n            \"model\": kwargs.get(\"model\", \"default\"),\n            \"temperature\": kwargs.get(\"temperature\", 0.7),\n            \"max_tokens\": kwargs.get(\"max_tokens\", 1000)\n        }\n\n        response = await self.client.post(\n            self.config.endpoint,\n            json=payload\n        )\n\n        return response.json()\n\n# Register the custom provider\nfrom lionagi.service.manager import ServiceManager\n\nclass CustomLLMProvider:\n    def __init__(self, api_key: str, base_url: str):\n        self.endpoint = CustomLLMEndpoint(api_key, base_url)\n\n    async def create_completion(self, messages: list, **kwargs):\n        return await self.endpoint.call(messages, **kwargs)\n\n# Usage\nasync def custom_provider_example():\n    provider = CustomLLMProvider(\n        api_key=\"your-custom-api-key\",\n        base_url=\"https://api.customllm.com\"\n    )\n\n    # Register with LionAGI\n    ServiceManager.register_provider(\"custom_llm\", provider)\n\n    # Use in session\n    session = Session(\n        imodel=\"custom-model-name\",\n        provider=\"custom_llm\"\n    )\n\n    response = await session.chat(\"Hello from custom provider!\")\n    print(response)\n</code></pre>"},{"location":"integrations/llm-providers/#openai-compatible-providers","title":"OpenAI-Compatible Providers","text":"<pre><code>from lionagi.session import Session\n\n# Many providers are OpenAI-compatible\nasync def openai_compatible_providers():\n    # Together AI\n    session_together = Session(\n        imodel=\"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\",\n        provider=\"openai\",  # OpenAI-compatible\n        base_url=\"https://api.together.xyz/v1\",\n        api_key=\"your-together-key\"\n    )\n\n    # Groq\n    session_groq = Session(\n        imodel=\"llama-3.1-8b-instant\",\n        provider=\"openai\",  # OpenAI-compatible\n        base_url=\"https://api.groq.com/openai/v1\",\n        api_key=\"your-groq-key\"\n    )\n\n    # Perplexity\n    session_pplx = Session(\n        imodel=\"llama-3.1-sonar-small-128k-online\",\n        provider=\"perplexity\",\n        api_key=\"your-perplexity-key\"\n    )\n\n    # Use any of them\n    response = await session_groq.chat(\"Fast inference test\")\n    print(response)\n\nasyncio.run(openai_compatible_providers())\n</code></pre>"},{"location":"integrations/llm-providers/#provider-configuration-templates","title":"Provider Configuration Templates","text":"<pre><code># config/custom_providers.py\nfrom typing import Dict, Any\n\nCUSTOM_PROVIDER_CONFIGS = {\n    \"huggingface\": {\n        \"base_url\": \"https://api-inference.huggingface.co/models\",\n        \"headers\": {\n            \"Authorization\": \"Bearer {api_key}\",\n            \"Content-Type\": \"application/json\"\n        },\n        \"endpoint_template\": \"{model_name}\",\n        \"request_format\": \"huggingface\",\n        \"response_format\": \"huggingface\"\n    },\n\n    \"replicate\": {\n        \"base_url\": \"https://api.replicate.com/v1\",\n        \"headers\": {\n            \"Authorization\": \"Token {api_key}\",\n            \"Content-Type\": \"application/json\"\n        },\n        \"endpoint_template\": \"predictions\",\n        \"request_format\": \"replicate\",\n        \"response_format\": \"replicate\"\n    }\n}\n\nclass ProviderFactory:\n    @staticmethod\n    def create_provider(provider_name: str, config: Dict[str, Any]):\n        if provider_name in CUSTOM_PROVIDER_CONFIGS:\n            template = CUSTOM_PROVIDER_CONFIGS[provider_name]\n            # Merge template with user config\n            final_config = {**template, **config}\n            return CustomLLMProvider(**final_config)\n\n        raise ValueError(f\"Unknown provider: {provider_name}\")\n</code></pre>"},{"location":"integrations/llm-providers/#provider-comparison","title":"Provider Comparison","text":""},{"location":"integrations/llm-providers/#performance-characteristics","title":"Performance Characteristics","text":"Provider Latency Context Strengths Best For GPT-4o-mini Fast 128K Balanced, cost-effective General development GPT-4 Medium 128K Reasoning, complex tasks Critical applications Claude 3.5 Sonnet Medium 200K Code, analysis, safety Code review, analysis Gemini 1.5 Pro Medium 2M Multimodal, large context Document processing Ollama/Local Varies Varies Privacy, no API costs Sensitive data, offline"},{"location":"integrations/llm-providers/#cost-optimization","title":"Cost Optimization","text":"<pre><code>from lionagi.session import Session\nfrom lionagi.service.token_calculator import TokenCalculator\n\nasync def cost_aware_routing():\n    # Simple tasks \u2192 GPT-4o-mini\n    simple_session = Branch(chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"))\n\n    # Complex reasoning \u2192 GPT-4\n    complex_session = Branch(chat_model=iModel(provider=\"openai\", model=\"gpt-4\"))\n\n    # Code tasks \u2192 Claude\n    code_session = Branch(chat_model=iModel(provider=\"anthropic\", model=\"claude-3-5-sonnet-20241022\"))\n\n    def route_by_complexity(task: str):\n        # Simple heuristics\n        if any(word in task.lower() for word in [\"code\", \"debug\", \"review\"]):\n            return code_session\n        elif any(word in task.lower() for word in [\"analyze\", \"complex\", \"reasoning\"]):\n            return complex_session\n        else:\n            return simple_session\n\n    # Route task to appropriate model\n    task = \"Fix this Python bug in my authentication system\"\n    session = route_by_complexity(task)\n\n    response = await session.chat(task)\n    return response\n</code></pre>"},{"location":"integrations/llm-providers/#best-practices","title":"Best Practices","text":"<ol> <li>Model Selection: Choose based on task complexity and cost requirements</li> <li>API Key Management: Use environment variables, never hardcode keys</li> <li>Error Handling: Implement retry logic with exponential backoff</li> <li>Rate Limiting: Respect provider rate limits to avoid blocking</li> <li>Token Management: Monitor token usage to control costs</li> <li>Caching: Cache responses for identical queries</li> <li>Streaming: Use streaming for long responses to improve UX</li> <li>Fallback Providers: Have backup providers for high availability</li> </ol>"},{"location":"integrations/mcp-servers/","title":"MCP Server Integration","text":"<p>Connecting LionAGI to Model Context Protocol servers for external capabilities.</p>"},{"location":"integrations/mcp-servers/#memory-mcp","title":"Memory MCP","text":"<pre><code>from lionagi import Branch\n# Memory MCP is automatically available\n\n# Save information to memory\nbranch = Branch(name=\"researcher\")\nawait branch.communicate(\"Remember that user prefers concise responses\")\n\n# Memory is automatically saved via MCP\n# Access with mcp__memory__search(), mcp__memory__save(), etc.\n</code></pre>"},{"location":"integrations/mcp-servers/#external-api-integration-pattern","title":"External API Integration Pattern","text":"<pre><code>from lionagi import iModel\n\n# Exa search integration\nexa_model = iModel(\n    provider=\"exa\",\n    endpoint=\"search\",\n    queue_capacity=5,\n    capacity_refresh_time=1,\n    invoke_with_endpoint=False,\n)\n\nasync def research_with_external_api(query: str):\n    \"\"\"Use external MCP servers for enhanced research\"\"\"\n\n    researcher = Branch(\n        chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"),\n        system=\"Research specialist with external search capabilities\"\n    )\n\n    # Use external search API\n    search_request = {\n        \"query\": query,\n        \"num_results\": 5,\n        \"include_domains\": [\"arxiv.org\", \"openai.com\"],\n        \"is_cached\": True\n    }\n\n    # Make API call\n    api_call = exa_model.create_api_calling(**search_request)\n    search_result = await exa_model.invoke(api_call)\n\n    # Process with LionAGI\n    analysis = await researcher.communicate(\n        \"Analyze search results and provide insights\",\n        context=search_result.response\n    )\n\n    return analysis\n\n# Usage\ninsights = await research_with_external_api(\"AI safety research trends 2024\")\n</code></pre>"},{"location":"integrations/mcp-servers/#custom-mcp-server-pattern","title":"Custom MCP Server Pattern","text":"<pre><code># Example: Custom knowledge base MCP\nclass KnowledgeBaseMCP:\n    \"\"\"Custom MCP server for domain knowledge\"\"\"\n\n    def __init__(self, knowledge_db_path: str):\n        self.db_path = knowledge_db_path\n\n    async def query_knowledge(self, topic: str) -&gt; str:\n        \"\"\"Query domain-specific knowledge base\"\"\"\n        # Implementation would connect to actual knowledge base\n        return f\"Knowledge about {topic}: ...\"\n\n    async def add_knowledge(self, topic: str, content: str) -&gt; bool:\n        \"\"\"Add new knowledge to base\"\"\"\n        # Implementation would store in knowledge base\n        return True\n\n# Integration with LionAGI\nknowledge_server = KnowledgeBaseMCP(\"./knowledge.db\")\n\ndomain_expert = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"),\n    tools=[knowledge_server.query_knowledge, knowledge_server.add_knowledge],\n    system=\"Domain expert with access to specialized knowledge base\"\n)\n\n# Use custom MCP\nresult = await domain_expert.ReAct(\n    instruct={\"instruction\": \"Research quantum computing applications\"},\n    max_extensions=3\n)\n</code></pre>"},{"location":"integrations/mcp-servers/#mcp-communication-patterns","title":"MCP Communication Patterns","text":"<pre><code>import json\nimport asyncio\n\nclass MCPClient:\n    \"\"\"Generic MCP client for custom protocols\"\"\"\n\n    def __init__(self, server_endpoint: str):\n        self.endpoint = server_endpoint\n\n    async def call_mcp_method(self, method: str, params: dict):\n        \"\"\"Generic MCP method call\"\"\"\n        # Implementation would handle MCP protocol\n        request = {\n            \"jsonrpc\": \"2.0\",\n            \"method\": method,\n            \"params\": params,\n            \"id\": 1\n        }\n\n        # Mock response for example\n        return {\"result\": f\"MCP response for {method}\"}\n\n# Use MCP client in agents\nmcp_client = MCPClient(\"http://localhost:8000/mcp\")\n\nasync def mcp_enhanced_workflow():\n    \"\"\"Workflow using multiple MCP servers\"\"\"\n\n    coordinator = Branch(\n        chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"),\n        system=\"Coordinates work using MCP servers\"\n    )\n\n    # Call multiple MCP servers\n    tasks = [\n        mcp_client.call_mcp_method(\"search\", {\"query\": \"AI research\"}),\n        mcp_client.call_mcp_method(\"analyze\", {\"data\": \"research_data\"}),\n        mcp_client.call_mcp_method(\"summarize\", {\"content\": \"long_text\"})\n    ]\n\n    # Execute MCP calls in parallel\n    mcp_results = await asyncio.gather(*tasks)\n\n    # Process results with LionAGI\n    synthesis = await coordinator.communicate(\n        \"Synthesize MCP server results\",\n        context=mcp_results\n    )\n\n    return synthesis\n\n# Usage\nresult = await mcp_enhanced_workflow()\n</code></pre>"},{"location":"integrations/mcp-servers/#resource-management","title":"Resource Management","text":"<pre><code>from contextlib import asynccontextmanager\n\nclass MCPResourceManager:\n    \"\"\"Manage MCP server connections and resources\"\"\"\n\n    def __init__(self):\n        self.connections = {}\n\n    @asynccontextmanager\n    async def get_mcp_connection(self, server_name: str):\n        \"\"\"Get managed MCP connection\"\"\"\n        try:\n            if server_name not in self.connections:\n                # Initialize connection\n                self.connections[server_name] = MCPClient(f\"http://{server_name}:8000\")\n\n            yield self.connections[server_name]\n        except Exception as e:\n            print(f\"MCP connection error: {e}\")\n            raise\n        finally:\n            # Cleanup if needed\n            pass\n\n# Usage in production\nmcp_manager = MCPResourceManager()\n\nasync def production_mcp_workflow():\n    \"\"\"Production workflow with managed MCP resources\"\"\"\n\n    async with mcp_manager.get_mcp_connection(\"memory_server\") as memory_mcp:\n        async with mcp_manager.get_mcp_connection(\"search_server\") as search_mcp:\n            # Use multiple MCP servers safely\n            memory_result = await memory_mcp.call_mcp_method(\"recall\", {\"query\": \"user_preferences\"})\n            search_result = await search_mcp.call_mcp_method(\"search\", {\"query\": \"latest_ai_news\"})\n\n            # Process with LionAGI\n            processor = Branch(\n                chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\")\n            )\n\n            return await processor.communicate(\n                \"Process MCP results\",\n                context={\"memory\": memory_result, \"search\": search_result}\n            )\n</code></pre>"},{"location":"integrations/mcp-servers/#error-handling-and-retry","title":"Error Handling and Retry","text":"<pre><code>async def robust_mcp_call(mcp_client, method: str, params: dict, max_retries: int = 3):\n    \"\"\"Robust MCP call with retry logic\"\"\"\n\n    for attempt in range(max_retries):\n        try:\n            result = await mcp_client.call_mcp_method(method, params)\n            return result\n        except Exception as e:\n            if attempt == max_retries - 1:\n                print(f\"MCP call failed after {max_retries} attempts: {e}\")\n                return None\n\n            # Exponential backoff\n            await asyncio.sleep(2 ** attempt)\n\n    return None\n\n# Usage in workflows\nasync def fault_tolerant_workflow():\n    \"\"\"Workflow with MCP fault tolerance\"\"\"\n\n    mcp_client = MCPClient(\"http://unreliable-server:8000\")\n\n    # Robust MCP calls\n    results = await asyncio.gather(\n        robust_mcp_call(mcp_client, \"method1\", {\"param\": \"value1\"}),\n        robust_mcp_call(mcp_client, \"method2\", {\"param\": \"value2\"}),\n        return_exceptions=True\n    )\n\n    # Filter successful results\n    successful_results = [r for r in results if r is not None]\n\n    # Process with LionAGI even if some MCP calls failed\n    if successful_results:\n        processor = Branch(\n            chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\")\n        )\n\n        return await processor.communicate(\n            \"Process available MCP results\",\n            context=successful_results\n        )\n\n    return \"No MCP results available\"\n</code></pre>"},{"location":"integrations/mcp-servers/#best-practices","title":"Best Practices","text":"<p>Connection Management:</p> <ul> <li>Use connection pooling for high-throughput scenarios</li> <li>Implement proper timeout handling</li> <li>Cache MCP responses when appropriate</li> </ul> <p>Error Handling:</p> <ul> <li>Implement retry logic with exponential backoff</li> <li>Graceful degradation when MCP servers unavailable</li> <li>Comprehensive logging for debugging</li> </ul> <p>Performance:</p> <ul> <li>Parallel MCP calls when possible</li> <li>Optimize payload sizes for network efficiency</li> <li>Monitor MCP server response times</li> </ul> <p>Security:</p> <ul> <li>Validate all MCP server responses</li> <li>Use secure connections (TLS) for production</li> <li>Implement proper authentication mechanisms</li> </ul>"},{"location":"integrations/tools/","title":"Tool Integration","text":"<p>Built-in and custom tools for extending agent capabilities.</p>"},{"location":"integrations/tools/#basic-tool-creation","title":"Basic Tool Creation","text":"<pre><code>from lionagi import Branch, iModel\n\n# Simple function becomes a tool\ndef calculate_sum(a: float, b: float) -&gt; float:\n    \"\"\"Add two numbers together\"\"\"\n    return a + b\n\ndef search_web(query: str) -&gt; str:\n    \"\"\"Search the web for information\"\"\"\n    return f\"Web search results for: {query}\"\n\n# Create tool-enabled branch (direct function passing)\nagent = Branch(\n    tools=[calculate_sum, search_web],\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"),\n    system=\"You have access to calculation and web search tools\"\n)\n\n# Usage\nresult = await agent.ReAct(\n    instruct={\"instruction\": \"Search for Python tutorials and calculate 15 + 27\"},\n    max_extensions=3\n)\n</code></pre>"},{"location":"integrations/tools/#readertool-integration","title":"ReaderTool Integration","text":"<pre><code>from lionagi.tools.types import ReaderTool\n\n# Document analysis agent\ndoc_analyzer = Branch(\n    tools=[ReaderTool],\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"),\n    system=\"Document analysis specialist with reading capabilities\"\n)\n\n# Analyze documents with ReaderTool\nanalysis = await doc_analyzer.ReAct(\n    instruct={\n        \"instruction\": \"Read and analyze the document, extract key findings\",\n        \"context\": {\"document_path\": \"research_paper.pdf\"}\n    },\n    tools=[\"reader_tool\"],\n    max_extensions=4,\n    verbose=True\n)\n</code></pre>"},{"location":"integrations/tools/#api-integration-tools","title":"API Integration Tools","text":"<pre><code>import httpx\nfrom typing import Dict, Any\n\ndef make_api_request(url: str, method: str = \"GET\", data: Dict[str, Any] = None) -&gt; str:\n    \"\"\"Make HTTP API requests\"\"\"\n    try:\n        with httpx.Client() as client:\n            if method.upper() == \"POST\":\n                response = client.post(url, json=data)\n            else:\n                response = client.get(url, params=data)\n\n            response.raise_for_status()\n            return response.text\n    except Exception as e:\n        return f\"API request failed: {e}\"\n\n# API-enabled agent\napi_agent = Branch(\n    tools=[make_api_request],\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"),\n    system=\"API integration specialist with HTTP request capabilities\"\n)\n\n# Usage\nresult = await api_agent.ReAct(\n    instruct={\"instruction\": \"Fetch weather data from api.weather.com\"},\n    max_extensions=2\n)\n</code></pre>"},{"location":"integrations/tools/#database-tool-integration","title":"Database Tool Integration","text":"<pre><code>import sqlite3\nfrom contextlib import contextmanager\n\n@contextmanager\ndef get_db_connection(db_path: str):\n    \"\"\"Database connection context manager\"\"\"\n    conn = sqlite3.connect(db_path)\n    try:\n        yield conn\n    finally:\n        conn.close()\n\ndef query_database(query: str, db_path: str = \"database.db\") -&gt; str:\n    \"\"\"Execute SQL query and return results\"\"\"\n    try:\n        with get_db_connection(db_path) as conn:\n            cursor = conn.cursor()\n            cursor.execute(query)\n            results = cursor.fetchall()\n            return str(results)\n    except Exception as e:\n        return f\"Database error: {e}\"\n\ndef insert_data(table: str, data: Dict[str, Any], db_path: str = \"database.db\") -&gt; str:\n    \"\"\"Insert data into database table\"\"\"\n    try:\n        columns = \", \".join(data.keys())\n        placeholders = \", \".join([\"?\" for _ in data])\n        query = f\"INSERT INTO {table} ({columns}) VALUES ({placeholders})\"\n\n        with get_db_connection(db_path) as conn:\n            cursor = conn.cursor()\n            cursor.execute(query, list(data.values()))\n            conn.commit()\n            return f\"Inserted data into {table}\"\n    except Exception as e:\n        return f\"Insert error: {e}\"\n\n# Database agent\ndb_agent = Branch(\n    tools=[query_database, insert_data],\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"),\n    system=\"Database specialist with SQL query and data insertion capabilities\"\n)\n\n# Usage\nresult = await db_agent.ReAct(\n    instruct={\"instruction\": \"Query user table and add a new user record\"},\n    max_extensions=3\n)\n</code></pre>"},{"location":"integrations/tools/#code-execution-tools","title":"Code Execution Tools","text":"<pre><code>import subprocess\nimport tempfile\nimport os\n\ndef execute_python_code(code: str) -&gt; str:\n    \"\"\"Safely execute Python code in isolated environment\"\"\"\n    try:\n        with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:\n            f.write(code)\n            f.flush()\n\n            # Execute with timeout\n            result = subprocess.run(\n                ['python', f.name],\n                capture_output=True,\n                text=True,\n                timeout=10  # 10 second timeout\n            )\n\n            # Cleanup\n            os.unlink(f.name)\n\n            if result.returncode == 0:\n                return result.stdout\n            else:\n                return f\"Error: {result.stderr}\"\n\n    except subprocess.TimeoutExpired:\n        return \"Code execution timeout\"\n    except Exception as e:\n        return f\"Execution error: {e}\"\n\ndef validate_code_syntax(code: str) -&gt; str:\n    \"\"\"Validate Python code syntax without execution\"\"\"\n    try:\n        compile(code, '&lt;string&gt;', 'exec')\n        return \"Syntax is valid\"\n    except SyntaxError as e:\n        return f\"Syntax error: {e}\"\n\n# Code execution agent\ncode_agent = Branch(\n    tools=[execute_python_code, validate_code_syntax],\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"),\n    system=\"Code execution specialist with Python runtime capabilities\"\n)\n\n# Usage\nresult = await code_agent.ReAct(\n    instruct={\"instruction\": \"Write and execute code to calculate fibonacci sequence\"},\n    max_extensions=3\n)\n</code></pre>"},{"location":"integrations/tools/#file-system-tools","title":"File System Tools","text":"<pre><code>import os\nimport json\nfrom pathlib import Path\n\ndef read_file(file_path: str) -&gt; str:\n    \"\"\"Read content from file\"\"\"\n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            return f.read()\n    except Exception as e:\n        return f\"Error reading file: {e}\"\n\ndef write_file(file_path: str, content: str) -&gt; str:\n    \"\"\"Write content to file\"\"\"\n    try:\n        Path(file_path).parent.mkdir(parents=True, exist_ok=True)\n        with open(file_path, 'w', encoding='utf-8') as f:\n            f.write(content)\n        return f\"Successfully wrote to {file_path}\"\n    except Exception as e:\n        return f\"Error writing file: {e}\"\n\ndef list_directory(dir_path: str) -&gt; str:\n    \"\"\"List contents of directory\"\"\"\n    try:\n        items = os.listdir(dir_path)\n        return json.dumps(items, indent=2)\n    except Exception as e:\n        return f\"Error listing directory: {e}\"\n\n# File system agent\nfs_agent = Branch(\n    tools=[read_file, write_file, list_directory],\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"),\n    system=\"File system specialist with read/write capabilities\"\n)\n\n# Usage\nresult = await fs_agent.ReAct(\n    instruct={\"instruction\": \"Create a project structure with README and config files\"},\n    max_extensions=4\n)\n</code></pre>"},{"location":"integrations/tools/#custom-tool-factory","title":"Custom Tool Factory","text":"<pre><code>from typing import Callable, Any\nimport inspect\n\nclass ToolFactory:\n    \"\"\"Factory for creating standardized tools\"\"\"\n\n    @staticmethod\n    def create_api_tool(base_url: str, api_key: str = None) -&gt; Callable:\n        \"\"\"Create API tool with base URL and authentication\"\"\"\n\n        def api_tool(endpoint: str, method: str = \"GET\", data: Dict = None) -&gt; str:\n            \"\"\"Generated API tool\"\"\"\n            url = f\"{base_url}/{endpoint.lstrip('/')}\"\n            headers = {}\n\n            if api_key:\n                headers[\"Authorization\"] = f\"Bearer {api_key}\"\n\n            try:\n                with httpx.Client() as client:\n                    response = client.request(method, url, json=data, headers=headers)\n                    response.raise_for_status()\n                    return response.text\n            except Exception as e:\n                return f\"API error: {e}\"\n\n        return api_tool\n\n    @staticmethod\n    def create_validation_tool(validation_func: Callable) -&gt; Callable:\n        \"\"\"Create validation tool from validation function\"\"\"\n\n        def validation_tool(data: str) -&gt; str:\n            \"\"\"Generated validation tool\"\"\"\n            try:\n                is_valid = validation_func(data)\n                return f\"Validation result: {'Valid' if is_valid else 'Invalid'}\"\n            except Exception as e:\n                return f\"Validation error: {e}\"\n\n        return validation_tool\n\n    @staticmethod\n    def create_processing_tool(processor_func: Callable) -&gt; Callable:\n        \"\"\"Create data processing tool\"\"\"\n\n        def processing_tool(input_data: str) -&gt; str:\n            \"\"\"Generated processing tool\"\"\"\n            try:\n                result = processor_func(input_data)\n                return str(result)\n            except Exception as e:\n                return f\"Processing error: {e}\"\n\n        return processing_tool\n\n# Usage example\ndef validate_email(email: str) -&gt; bool:\n    \"\"\"Email validation function\"\"\"\n    return \"@\" in email and \".\" in email\n\ndef process_text(text: str) -&gt; str:\n    \"\"\"Text processing function\"\"\"\n    return text.upper().replace(\" \", \"_\")\n\n# Create custom tools\nemail_validator = ToolFactory.create_validation_tool(validate_email)\ntext_processor = ToolFactory.create_processing_tool(process_text)\napi_tool = ToolFactory.create_api_tool(\"https://api.example.com\", \"your_api_key\")\n\n# Use in agent\ncustom_agent = Branch(\n    tools=[email_validator, text_processor, api_tool],\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"),\n    system=\"Multi-purpose agent with custom tools\"\n)\n</code></pre>"},{"location":"integrations/tools/#tool-composition-patterns","title":"Tool Composition Patterns","text":"<pre><code>from typing import List\nimport asyncio\n\nclass ToolComposer:\n    \"\"\"Compose multiple tools into workflows\"\"\"\n\n    def __init__(self, tools: List[Callable]):\n        self.tools = {tool.__name__: tool for tool in tools}\n\n    def create_pipeline_tool(self, tool_names: List[str]) -&gt; Callable:\n        \"\"\"Create a pipeline of tools\"\"\"\n\n        def pipeline_tool(input_data: str) -&gt; str:\n            \"\"\"Execute tools in sequence\"\"\"\n            current_data = input_data\n            results = []\n\n            for tool_name in tool_names:\n                if tool_name in self.tools:\n                    result = self.tools[tool_name](current_data)\n                    results.append(f\"{tool_name}: {result}\")\n                    current_data = result  # Pass result to next tool\n                else:\n                    results.append(f\"Tool {tool_name} not found\")\n\n            return \"\\n\".join(results)\n\n        return pipeline_tool\n\n    def create_parallel_tool(self, tool_names: List[str]) -&gt; Callable:\n        \"\"\"Create parallel execution of tools\"\"\"\n\n        def parallel_tool(input_data: str) -&gt; str:\n            \"\"\"Execute tools in parallel\"\"\"\n            results = []\n\n            for tool_name in tool_names:\n                if tool_name in self.tools:\n                    result = self.tools[tool_name](input_data)\n                    results.append(f\"{tool_name}: {result}\")\n                else:\n                    results.append(f\"Tool {tool_name} not found\")\n\n            return \"\\n\".join(results)\n\n        return parallel_tool\n\n# Usage\ncomposer = ToolComposer([validate_email, text_processor, calculate_sum])\n\n# Create composite tools\npipeline = composer.create_pipeline_tool([\"text_processor\", \"validate_email\"])\nparallel = composer.create_parallel_tool([\"text_processor\", \"validate_email\"])\n\n# Use in agent\ncomposite_agent = Branch(\n    tools=[pipeline, parallel],\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"),\n    system=\"Agent with composite tool capabilities\"\n)\n</code></pre>"},{"location":"integrations/tools/#tool-development-best-practices","title":"Tool Development Best Practices","text":"<p>Error Handling:</p> <ul> <li>Always wrap tool functions in try/except blocks</li> <li>Return meaningful error messages</li> <li>Implement graceful degradation</li> </ul> <p>Type Safety:</p> <ul> <li>Use type hints for all parameters and return values</li> <li>Validate input parameters before processing</li> <li>Document expected input/output formats</li> </ul> <p>Security:</p> <ul> <li>Validate all external inputs</li> <li>Use secure methods for file operations</li> <li>Implement proper authentication for API tools</li> </ul> <p>Performance:</p> <ul> <li>Use connection pooling for database/API tools</li> <li>Implement caching where appropriate</li> <li>Set reasonable timeouts for external operations</li> </ul> <p>Testing:</p> <ul> <li>Test tools independently before integration</li> <li>Mock external dependencies for testing</li> <li>Verify error handling scenarios</li> </ul>"},{"location":"integrations/vector-stores/","title":"Vector Store Integration","text":"<p>Integrate LionAGI with vector databases for RAG, memory, and semantic search.</p>"},{"location":"integrations/vector-stores/#basic-vector-store-pattern","title":"Basic Vector Store Pattern","text":"<pre><code>from lionagi import Branch, iModel\nimport numpy as np\n\n# Simple vector store interface\nclass VectorStore:\n    def __init__(self):\n        self.vectors = {}\n        self.metadata = {}\n\n    def add(self, doc_id: str, vector: np.ndarray, metadata: dict):\n        self.vectors[doc_id] = vector\n        self.metadata[doc_id] = metadata\n\n    def search(self, query_vector: np.ndarray, top_k: int = 5):\n        # Cosine similarity search\n        similarities = {}\n        for doc_id, vector in self.vectors.items():\n            similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n            similarities[doc_id] = similarity\n\n        # Return top-k results\n        sorted_results = sorted(similarities.items(), key=lambda x: x[1], reverse=True)[:top_k]\n        return [(doc_id, self.metadata[doc_id], score) for doc_id, score in sorted_results]\n\n# Vector search tool for agents\ndef vector_search(query: str, top_k: int = 3) -&gt; str:\n    \"\"\"Search vector store for relevant documents\"\"\"\n    # Mock implementation - replace with actual embedding + search\n    return f\"Found {top_k} relevant documents for: {query}\"\n\n# RAG-enabled branch\nrag_branch = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"),\n    tools=[vector_search],\n    system=\"You have access to a vector database. Use vector_search to find relevant information before answering.\"\n)\n\n# Usage\nresponse = await rag_branch.ReAct(\n    instruct={\"instruction\": \"What are the latest developments in AI safety?\"},\n    max_extensions=2\n)\n</code></pre>"},{"location":"integrations/vector-stores/#qdrant-integration","title":"Qdrant Integration","text":"<pre><code>import asyncio\nfrom typing import List, Dict, Any\n\nclass QdrantAdapter:\n    \"\"\"Qdrant vector database adapter for LionAGI\"\"\"\n\n    def __init__(self, host: str = \"localhost\", port: int = 6333):\n        self.host = host\n        self.port = port\n        # Initialize Qdrant client here\n\n    async def create_collection(self, collection_name: str, vector_size: int = 1536):\n        \"\"\"Create Qdrant collection\"\"\"\n        # Implementation would create actual Qdrant collection\n        print(f\"Created collection: {collection_name}\")\n\n    async def add_documents(self, collection: str, documents: List[Dict[str, Any]]):\n        \"\"\"Add documents with embeddings to collection\"\"\"\n        # Implementation would add to Qdrant\n        return f\"Added {len(documents)} documents to {collection}\"\n\n    async def search(self, collection: str, query_vector: List[float], limit: int = 5):\n        \"\"\"Vector similarity search\"\"\"\n        # Mock results - replace with actual Qdrant search\n        return [\n            {\"id\": \"doc1\", \"score\": 0.95, \"payload\": {\"text\": \"Sample document 1\"}},\n            {\"id\": \"doc2\", \"score\": 0.87, \"payload\": {\"text\": \"Sample document 2\"}},\n        ]\n\n# Qdrant-powered research agent\nasync def create_qdrant_researcher():\n    \"\"\"Create research agent with Qdrant vector search\"\"\"\n\n    qdrant = QdrantAdapter()\n\n    async def qdrant_search(query: str) -&gt; str:\n        \"\"\"Qdrant search tool for agents\"\"\"\n        # Generate embedding for query (mock)\n        query_vector = [0.1] * 1536  # Replace with actual embedding\n\n        results = await qdrant.search(\"research_papers\", query_vector)\n\n        # Format results for agent\n        formatted = \"\\n\".join([\n            f\"- {r['payload']['text']} (score: {r['score']:.2f})\"\n            for r in results\n        ])\n\n        return f\"Vector search results:\\n{formatted}\"\n\n    researcher = Branch(\n        chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"),\n        tools=[qdrant_search],\n        system=\"Research specialist with access to Qdrant vector database\"\n    )\n\n    return researcher\n\n# Usage\nresearcher = await create_qdrant_researcher()\nresult = await researcher.ReAct(\n    instruct={\"instruction\": \"Find papers on transformer architectures\"},\n    max_extensions=2\n)\n</code></pre>"},{"location":"integrations/vector-stores/#chromadb-local-vector-store","title":"ChromaDB Local Vector Store","text":"<pre><code>class ChromaAdapter:\n    \"\"\"ChromaDB adapter for local vector storage\"\"\"\n\n    def __init__(self, persist_directory: str = \"./chroma_db\"):\n        self.persist_directory = persist_directory\n        # Initialize ChromaDB client\n\n    def add_texts(self, collection_name: str, texts: List[str], metadatas: List[Dict] = None):\n        \"\"\"Add texts to ChromaDB collection\"\"\"\n        # Implementation would use ChromaDB\n        return f\"Added {len(texts)} texts to {collection_name}\"\n\n    def similarity_search(self, collection_name: str, query: str, k: int = 5):\n        \"\"\"Search for similar texts\"\"\"\n        # Mock results\n        return [\n            {\"text\": f\"Result {i}\", \"metadata\": {\"source\": f\"doc_{i}\"}, \"distance\": 0.1 * i}\n            for i in range(k)\n        ]\n\n# Local RAG with ChromaDB\nasync def local_rag_workflow(documents: List[str], query: str):\n    \"\"\"Local RAG workflow using ChromaDB\"\"\"\n\n    chroma = ChromaAdapter()\n\n    # Add documents to vector store\n    chroma.add_texts(\"knowledge_base\", documents)\n\n    # Create search tool\n    def search_knowledge(query: str) -&gt; str:\n        results = chroma.similarity_search(\"knowledge_base\", query, k=3)\n        return \"\\n\".join([f\"- {r['text']}\" for r in results])\n\n    # Create RAG agent\n    rag_agent = Branch(\n        chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"),\n        tools=[search_knowledge],\n        system=\"Answer questions using the knowledge base. Always search first.\"\n    )\n\n    # Query with RAG\n    response = await rag_agent.ReAct(\n        instruct={\"instruction\": query},\n        max_extensions=2\n    )\n\n    return response\n\n# Usage\ndocs = [\n    \"LionAGI is a graph-based multi-agent framework\",\n    \"Branches represent individual AI agents with memory\",\n    \"Builder pattern creates complex agent workflows\"\n]\nanswer = await local_rag_workflow(docs, \"How does LionAGI work?\")\n</code></pre>"},{"location":"integrations/vector-stores/#pinecone-cloud-integration","title":"Pinecone Cloud Integration","text":"<pre><code>class PineconeAdapter:\n    \"\"\"Pinecone cloud vector database adapter\"\"\"\n\n    def __init__(self, api_key: str, environment: str):\n        self.api_key = api_key\n        self.environment = environment\n        # Initialize Pinecone client\n\n    async def upsert_vectors(self, index_name: str, vectors: List[Dict]):\n        \"\"\"Upload vectors to Pinecone index\"\"\"\n        # Implementation would use Pinecone API\n        return f\"Upserted {len(vectors)} vectors to {index_name}\"\n\n    async def query(self, index_name: str, vector: List[float], top_k: int = 5):\n        \"\"\"Query Pinecone index\"\"\"\n        # Mock results\n        return {\n            \"matches\": [\n                {\"id\": f\"vec_{i}\", \"score\": 0.9 - i*0.1, \"metadata\": {\"text\": f\"Result {i}\"}}\n                for i in range(top_k)\n            ]\n        }\n\n# Production RAG with Pinecone\nasync def production_rag_system():\n    \"\"\"Production-ready RAG system with Pinecone\"\"\"\n\n    pinecone = PineconeAdapter(\"your-api-key\", \"us-west1-gcp\")\n\n    async def pinecone_search(query: str) -&gt; str:\n        \"\"\"Production vector search\"\"\"\n        # Generate embedding (mock)\n        query_vector = [0.1] * 1536\n\n        results = await pinecone.query(\"production-index\", query_vector)\n\n        context = \"\\n\".join([\n            f\"- {match['metadata']['text']} (relevance: {match['score']:.2f})\"\n            for match in results[\"matches\"]\n        ])\n\n        return f\"Relevant context:\\n{context}\"\n\n    # Production RAG agent\n    production_agent = Branch(\n        chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"),\n        tools=[pinecone_search],\n        system=\"Production AI assistant with cloud vector search capabilities\"\n    )\n\n    return production_agent\n\n# Usage\nagent = await production_rag_system()\nresponse = await agent.ReAct(\n    instruct={\"instruction\": \"Explain quantum computing applications\"},\n    max_extensions=3\n)\n</code></pre>"},{"location":"integrations/vector-stores/#memory-enhanced-agents","title":"Memory-Enhanced Agents","text":"<pre><code>from lionagi import types\n\nclass AgentMemory:\n    \"\"\"Vector-based agent memory system\"\"\"\n\n    def __init__(self, vector_store):\n        self.vector_store = vector_store\n        self.conversation_history = []\n\n    async def remember(self, content: str, metadata: dict = None):\n        \"\"\"Store memory with vector embedding\"\"\"\n        # Generate embedding and store\n        vector = self.generate_embedding(content)\n        memory_id = f\"memory_{len(self.conversation_history)}\"\n\n        self.vector_store.add(memory_id, vector, {\n            \"content\": content,\n            \"timestamp\": metadata.get(\"timestamp\"),\n            \"type\": metadata.get(\"type\", \"conversation\")\n        })\n\n        self.conversation_history.append(memory_id)\n\n    async def recall(self, query: str, k: int = 3):\n        \"\"\"Recall relevant memories\"\"\"\n        query_vector = self.generate_embedding(query)\n        results = self.vector_store.search(query_vector, k)\n\n        return [\n            {\"content\": r[1][\"content\"], \"relevance\": r[2]}\n            for r in results\n        ]\n\n    def generate_embedding(self, text: str):\n        \"\"\"Generate embedding for text\"\"\"\n        # Mock embedding - replace with actual model\n        return np.random.random(1536)\n\n# Memory-enhanced agent\nasync def create_memory_agent():\n    \"\"\"Agent with vector-based long-term memory\"\"\"\n\n    vector_store = VectorStore()\n    memory = AgentMemory(vector_store)\n\n    async def remember_conversation(content: str) -&gt; str:\n        await memory.remember(content, {\"type\": \"conversation\"})\n        return f\"Remembered: {content[:50]}...\"\n\n    async def recall_relevant(query: str) -&gt; str:\n        memories = await memory.recall(query)\n        if memories:\n            return \"Relevant memories:\\n\" + \"\\n\".join([\n                f\"- {m['content'][:100]}... (relevance: {m['relevance']:.2f})\"\n                for m in memories\n            ])\n        return \"No relevant memories found\"\n\n    memory_agent = Branch(\n        chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"),\n        tools=[remember_conversation, recall_relevant],\n        system=\"AI assistant with long-term vector memory. Remember important information and recall when relevant.\"\n    )\n\n    return memory_agent\n\n# Usage\nagent = await create_memory_agent()\nawait agent.communicate(\"I prefer technical explanations over simplified ones\")\nresponse = await agent.communicate(\"Explain machine learning\")  # Will recall preference\n</code></pre>"},{"location":"integrations/vector-stores/#choosing-vector-stores","title":"Choosing Vector Stores","text":"<p>Local Development:</p> <ul> <li>ChromaDB: Easy setup, good for prototyping</li> <li>FAISS: High performance, research-grade</li> <li>In-memory: Simple testing and demos</li> </ul> <p>Production Cloud:</p> <ul> <li>Pinecone: Managed service, great scaling</li> <li>Weaviate: Open source, GraphQL API</li> <li>Qdrant: Rust-based, high performance</li> </ul> <p>Hybrid Approaches:</p> <ul> <li>Local + Cloud: Development locally, production in cloud</li> <li>Multi-store: Different stores for different use cases</li> <li>Fallback: Local backup when cloud unavailable</li> </ul>"},{"location":"marketing/language-interoperability-manifesto/","title":"The Language Interoperability Revolution: How We Eliminated AI Framework Lock-In","text":""},{"location":"marketing/language-interoperability-manifesto/#the-problem-thats-breaking-ai-development","title":"The Problem That's Breaking AI Development","text":"<p>Every AI team faces the same impossible choice: Pick one framework and live with its limitations forever.</p> <p>Choose LangChain? You're locked out of CrewAI's multi-agent patterns. Go with AutoGen? No access to LlamaIndex's RAG capabilities. Build on LangGraph? DSPy's prompt optimization is off-limits.</p> <p>The result? Teams rewrite everything when they need capabilities their chosen framework doesn't have. Switching costs are astronomical. Innovation slows to a crawl.</p>"},{"location":"marketing/language-interoperability-manifesto/#what-we-built-the-language-interoperable-network-lion","title":"What We Built: The Language Interoperable Network (LION)","text":"<p>We took a different approach. Instead of building another framework that competes with existing ones, we built a meta-orchestration system that makes them all work together.</p> <p>LION stands for Language Interoperable Network - and it's not just a clever acronym. It's a working system that orchestrates any AI framework, any model, any tool, in any combination you need.</p>"},{"location":"marketing/language-interoperability-manifesto/#the-documentation-that-proves-it-works","title":"The Documentation That Proves It Works","text":"<p>Our recent documentation overhaul isn't just user guides - it's living proof that language interoperability is real. Here's what we built:</p>"},{"location":"marketing/language-interoperability-manifesto/#zero-migration-framework-integration","title":"Zero-Migration Framework Integration","text":"<p>LangGraph Migration Guide: Shows how to keep your existing LangGraph workflows unchanged while orchestrating them with LionAGI:</p> <pre><code># Your existing LangGraph code - ZERO changes needed\nasync def existing_langgraph_workflow(input_data):\n    return await your_current_workflow.invoke(input_data)\n\n# LionAGI orchestrates it alongside other tools\nbuilder.add_operation(operation=existing_langgraph_workflow)\n</code></pre> <p>Result: Teams can adopt LionAGI gradually without throwing away existing investments.</p>"},{"location":"marketing/language-interoperability-manifesto/#multi-framework-orchestration-examples","title":"Multi-Framework Orchestration Examples","text":"<p>LlamaIndex + DSPy Integration: Combine best-in-class RAG with prompt optimization:</p> <pre><code># LlamaIndex handles document retrieval\nasync def rag_research(branch, query):\n    response = llamaindex_engine.query(query)\n    return response\n\n# DSPy optimizes the analysis prompts  \nasync def optimized_analysis(branch, data):\n    return dspy_analyzer(data=data).analysis\n\n# LionAGI orchestrates both in parallel\nresearch_op = builder.add_operation(operation=rag_research)\nanalysis_op = builder.add_operation(operation=optimized_analysis)\n</code></pre> <p>Result: You get the best of every framework without the integration headaches.</p>"},{"location":"marketing/language-interoperability-manifesto/#meta-orchestration-in-practice","title":"Meta-Orchestration in Practice","text":"<p>Here's the revolutionary part: Any existing workflow becomes a LionAGI custom operation.</p> <p>CrewAI workflow? Wrap it. AutoGen conversation? Orchestrate it. Custom Python functions? Coordinate them. External APIs? Include them.</p> <pre><code># Orchestrate EVERYTHING together\ncrewai_op = builder.add_operation(operation=your_crewai_workflow)\nautogen_op = builder.add_operation(operation=your_autogen_chat)  \ncustom_op = builder.add_operation(operation=your_python_function)\napi_op = builder.add_operation(operation=your_api_integration)\n\n# LionAGI handles dependencies, parallelization, error handling\nresult = await session.flow(builder.get_graph())\n</code></pre>"},{"location":"marketing/language-interoperability-manifesto/#why-this-changes-everything","title":"Why This Changes Everything","text":""},{"location":"marketing/language-interoperability-manifesto/#before-lion","title":"Before LION:","text":"<ul> <li>\u274c Framework Lock-In: Choose one, live with its limitations</li> <li>\u274c Rewrite Costs: Switching means starting over</li> <li>\u274c Capability Gaps: Missing features mean missing opportunities</li> <li>\u274c Integration Hell: Making frameworks talk to each other</li> </ul>"},{"location":"marketing/language-interoperability-manifesto/#after-lion","title":"After LION:","text":"<ul> <li>\u2705 Framework Freedom: Use the best tool for each job</li> <li>\u2705 Investment Protection: Keep everything you've already built</li> <li>\u2705 Capability Expansion: Access every framework's strengths</li> <li>\u2705 Seamless Orchestration: Everything works together intelligently</li> </ul>"},{"location":"marketing/language-interoperability-manifesto/#the-technical-reality-behind-the-vision","title":"The Technical Reality Behind the Vision","text":"<p>This isn't vaporware. Our documentation demonstrates working patterns:</p> <p>Parallel Execution: Multiple frameworks running concurrently with automatic dependency resolution.</p> <p>Error Handling: Built-in resilience across framework boundaries.</p> <p>Memory Management: Persistent context that works with any underlying system.</p> <p>Cost Tracking: Unified monitoring across all integrated services.</p> <p>Production Ready: Real-world patterns for scaling multi-framework systems.</p>"},{"location":"marketing/language-interoperability-manifesto/#what-this-means-for-ai-development","title":"What This Means for AI Development","text":"<p>We're not just solving today's framework fragmentation - we're preventing tomorrow's lock-in from ever happening.</p> <p>For Individual Developers: Use any tool, any time, without migration costs.</p> <p>For Teams: Leverage everyone's expertise regardless of their framework preferences.</p> <p>For Organizations: Protect AI investments from technology churn.</p> <p>For the Industry: Accelerate innovation by eliminating artificial barriers between tools.</p>"},{"location":"marketing/language-interoperability-manifesto/#the-bigger-picture-language-as-the-universal-interface","title":"The Bigger Picture: Language as the Universal Interface","text":"<p>Here's the profound insight: Natural language is the only interface flexible enough to orchestrate any AI system.</p> <p>While frameworks fight over APIs and architectures, we use the one interface every AI system already speaks: language itself.</p> <p>That's why it's called the Language Interoperable Network. Language isn't just how humans talk to AI - it's how AI systems can talk to each other.</p>"},{"location":"marketing/language-interoperability-manifesto/#from-documentation-to-revolution","title":"From Documentation to Revolution","text":"<p>What started as a documentation project became a demonstration of something bigger: AI systems don't have to be silos.</p> <p>Every integration example in our docs is proof that the future of AI isn't about picking winners and losers among frameworks. It's about orchestrating them all to solve problems no single system could handle alone.</p> <p>The Language Interoperable Network isn't coming someday. It's shipping code today.</p>"},{"location":"marketing/language-interoperability-manifesto/#the-call-to-action","title":"The Call to Action","text":"<p>Stop choosing between frameworks. Start orchestrating them.</p> <p>Our documentation shows exactly how to:</p> <ul> <li>Migrate gradually from any existing framework</li> <li>Integrate multiple AI tools seamlessly</li> <li>Build production systems that leverage everything</li> </ul> <p>The age of AI framework lock-in is over. The age of Language Interoperability has begun.</p> <p>Welcome to LION. \ud83e\udd81</p> <p>Ready to see language interoperability in action? Explore our migration guides and integration examples at lionagi.ai</p>"},{"location":"migration/","title":"Migration Guides","text":"<p>Migrate from other AI frameworks to LionAGI.</p>"},{"location":"migration/#available-migration-guides","title":"Available Migration Guides","text":"<p>Choose Your Migration Path</p> <p>Coming from LangChain/LangGraph? \u2192 LangChain Migration Guide - Reduce complexity by 90% Coming from CrewAI? \u2192 CrewAI Migration Guide - Keep your crew concepts, gain performance Coming from AutoGen? \u2192 AutoGen Migration Guide - Replace conversations with graphs</p>"},{"location":"migration/#from-langchain","title":"From LangChain","text":"<p>Escape LCEL complexity and gain true parallel execution. Perfect if you're tired of complex state management and want cleaner abstractions.</p>"},{"location":"migration/#from-crewai","title":"From CrewAI","text":"<p>Keep your agent crew concepts while gaining LionAGI's superior orchestration. Ideal if you like CrewAI's simplicity but need better performance.</p>"},{"location":"migration/#from-autogen","title":"From AutoGen","text":"<p>Replace unpredictable agent conversations with deterministic workflow graphs. Great if you want AutoGen's multi-agent power with production reliability.</p>"},{"location":"migration/#migration-philosophy","title":"Migration Philosophy","text":"<p>Zero-Disruption Migration: Keep your existing code and gradually adopt LionAGI orchestration.</p> <pre><code># Your existing workflow runs unchanged\nasync def existing_workflow(input_data):\n    return await your_current_implementation(input_data)\n\n# Orchestrate with LionAGI\nbuilder.add_operation(operation=existing_workflow)\n</code></pre>"},{"location":"migration/#migration-strategies","title":"Migration Strategies","text":""},{"location":"migration/#gradual-adoption","title":"Gradual Adoption","text":"<ol> <li>Wrap existing workflows as custom operations</li> <li>Add LionAGI orchestration around them</li> <li>Gradually convert individual components</li> <li>Gain orchestration benefits without disruption</li> </ol>"},{"location":"migration/#hybrid-workflows","title":"Hybrid Workflows","text":"<ul> <li>Mix existing framework code with native LionAGI operations</li> <li>Coordinate multiple frameworks in single workflow</li> <li>Best of all worlds approach</li> </ul>"},{"location":"migration/#full-migration","title":"Full Migration","text":"<ul> <li>Translate framework patterns to LionAGI equivalents</li> <li>Leverage LionAGI's superior orchestration capabilities</li> <li>Gain performance and simplicity benefits</li> </ul>"},{"location":"migration/#why-migrate","title":"Why Migrate?","text":"<ul> <li>Parallel Execution: LionAGI runs operations concurrently by default</li> <li>Simpler Code: Less boilerplate, cleaner abstractions</li> <li>Framework Agnostic: Orchestrate any tool, not just one ecosystem</li> <li>Production Ready: Built-in monitoring, error handling, performance control</li> </ul>"},{"location":"migration/from-autogen/","title":"Migrating from AutoGen","text":"<p>Direct comparisons showing AutoGen patterns and LionAGI equivalents.</p>"},{"location":"migration/from-autogen/#two-agent-conversation","title":"Two-Agent Conversation","text":"<p>AutoGen:</p> <pre><code>from autogen import ConversableAgent, LLMConfig\n\nllm_config = LLMConfig(api_type=\"openai\", model=\"gpt-4o-mini\")\n\nassistant = ConversableAgent(\n    name=\"assistant\",\n    system_message=\"You are a helpful assistant.\",\n    llm_config=llm_config,\n)\n\nhuman = ConversableAgent(name=\"human\", human_input_mode=\"ALWAYS\")\nhuman.initiate_chat(assistant, message=\"Hello! What's 2 + 2?\")\n</code></pre> <p>LionAGI:</p> <pre><code>from lionagi import Branch, iModel\n\nassistant = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"),\n    system=\"You are a helpful assistant.\"\n)\n\nresponse = await assistant.communicate(\"Hello! What's 2 + 2?\")\n</code></pre>"},{"location":"migration/from-autogen/#multi-agent-groupchat","title":"Multi-Agent GroupChat","text":"<p>AutoGen:</p> <pre><code>from autogen import AssistantAgent, GroupChat, GroupChatManager\n\ncoder = AssistantAgent(name=\"Coder\", llm_config=config_list)\nreviewer = AssistantAgent(name=\"Reviewer\", llm_config=config_list)\n\ngroupchat = GroupChat(agents=[coder, reviewer], messages=[], max_round=5)\nmanager = GroupChatManager(groupchat=groupchat, llm_config=config_list)\n\nuser.initiate_chat(manager, message=\"Generate a Python function\")\n</code></pre> <p>LionAGI:</p> <pre><code>from lionagi import Session, Builder\n\nsession = Session()\nbuilder = Builder()\n\ncoder = builder.add_operation(\n    \"communicate\", \n    chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"),\n    instruction=\"Generate a Python function\"\n)\n\nreviewer = builder.add_operation(\n    \"communicate\",\n    depends_on=[coder],\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"), \n    instruction=\"Review the generated code\"\n)\n\nresult = await session.flow(builder.get_graph())\n</code></pre>"},{"location":"migration/from-autogen/#tool-integration","title":"Tool Integration","text":"<p>AutoGen:</p> <pre><code>from autogen import ConversableAgent, register_function\nfrom datetime import datetime\n\ndef get_weekday(date_string: str) -&gt; str:\n    date = datetime.strptime(date_string, \"%Y-%m-%d\")\n    return date.strftime(\"%A\")\n\ndate_agent = ConversableAgent(name=\"date_agent\", llm_config=llm_config)\nexecutor = ConversableAgent(name=\"executor\", human_input_mode=\"NEVER\")\n\nregister_function(get_weekday, caller=date_agent, executor=executor)\nresult = executor.initiate_chat(date_agent, message=\"What day was March 25, 1995?\")\n</code></pre> <p>LionAGI:</p> <pre><code>from lionagi import Branch\nfrom datetime import datetime\n\ndef get_weekday(date_string: str) -&gt; str:\n    date = datetime.strptime(date_string, \"%Y-%m-%d\")\n    return date.strftime(\"%A\")\n\ndate_branch = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"),\n    tools=[get_weekday]  # Direct function passing\n)\n\nresult = await date_branch.ReAct(\n    instruct={\"instruction\": \"What day was March 25, 1995?\"},\n    max_extensions=2\n)\n</code></pre>"},{"location":"migration/from-autogen/#parallel-research-workflow","title":"Parallel Research Workflow","text":"<p>AutoGen:</p> <pre><code># Sequential GroupChat approach\nagents = [researcher1, researcher2, analyst]\ngroupchat = GroupChat(agents=agents, max_round=10)\nmanager = GroupChatManager(groupchat=groupchat)\nresult = user.initiate_chat(manager, message=\"Research AI trends\")\n</code></pre> <p>LionAGI:</p> <pre><code>from lionagi import Session, Builder\nimport asyncio\n\nsession = Session()\nbuilder = Builder()\n\n# Parallel research\nresearch_nodes = []\nfor i, topic in enumerate([\"transformers\", \"multimodal\", \"reasoning\"]):\n    node = builder.add_operation(\n        \"communicate\",\n        instruction=f\"Research {topic} developments in 2024\"\n    )\n    research_nodes.append(node)\n\n# Synthesis\nsynthesis = builder.add_operation(\n    \"communicate\",\n    depends_on=research_nodes,\n    instruction=\"Synthesize research findings into comprehensive report\"\n)\n\nresult = await session.flow(builder.get_graph())\n</code></pre>"},{"location":"migration/from-autogen/#state-management","title":"State Management","text":"<p>AutoGen:</p> <pre><code>def state_transition(last_speaker, groupchat):\n    messages = groupchat.messages\n    if last_speaker is architect:\n        return implementer\n    elif last_speaker is implementer:\n        return tester\n    return architect\n\ngroupchat = GroupChat(\n    agents=[architect, implementer, tester],\n    speaker_selection_method=state_transition\n)\n</code></pre> <p>LionAGI:</p> <pre><code># Explicit dependencies handle state transitions\narchitect_node = builder.add_operation(\"communicate\", instruction=\"Design system\")\nimpl_node = builder.add_operation(\"communicate\", depends_on=[architect_node], instruction=\"Implement design\")  \ntest_node = builder.add_operation(\"communicate\", depends_on=[impl_node], instruction=\"Test implementation\")\n\nawait session.flow(builder.get_graph())\n</code></pre>"},{"location":"migration/from-autogen/#enterprise-advantages","title":"Enterprise Advantages","text":"<p>AutoGen's \"Too Auto\" Problem: AutoGen makes autonomous decisions that lack enterprise controls</p> <p>LionAGI's Enterprise Features:</p> <pre><code># Explicit control over agent behavior\nsession = Session()\nbuilder = Builder()\n\n# Predictable execution paths\narchitect_node = builder.add_operation(\"communicate\", instruction=\"Design system\")\nreview_node = builder.add_operation(\"communicate\", depends_on=[architect_node], instruction=\"Review design\")\n\n# Built-in cost tracking\nresult = await session.flow(builder.get_graph())\nfrom lionagi.protocols.messages.assistant_response import AssistantResponse\n\n# Built-in cost tracking\ncosts = 0\ndef get_context(node_id):\n    nonlocal costs\n    graph = builder.get_graph()\n    node = graph.internal_nodes[node_id]\n    branch = session.get_branch(node.branch_id, None)\n    if branch and len(branch.messages) &gt; 0:\n        if isinstance(msg := branch.messages[-1], AssistantResponse):\n            costs += msg.model_response.get(\"total_cost_usd\") or 0\n\n# Track costs across workflow  \nfor node in [architect_node, review_node]:\n    get_context(node)\n\n# Detailed audit trail\nfor node_id, node in builder.get_graph().internal_nodes.items():\n    branch = session.get_branch(node.branch_id, None)\n    print(f\"Node {node_id}: {len(branch.messages)} messages\")\n</code></pre> <p>Enterprise Requirements: </p> <p>\u2705 Predictable Costs: Built-in usage tracking vs AutoGen's unknown spending \u2705 Deterministic Flow: Explicit dependencies vs AutoGen's autonomous decisions \u2705 Audit Compliance: Full execution logs vs AutoGen's black-box conversations \u2705 Error Recovery: Granular failure handling vs AutoGen's all-or-nothing \u2705 Resource Control: Bounded execution vs AutoGen's unlimited autonomy</p>"},{"location":"migration/from-autogen/#key-migration-points","title":"Key Migration Points","text":"<ul> <li>Conversation \u2192 Graph: AutoGen's linear conversations become explicit dependency graphs  </li> <li>Agents \u2192 Branches: ConversableAgent functionality maps to Branch instances  </li> <li>GroupChat \u2192 Builder: Multi-agent coordination uses Builder pattern with dependencies  </li> <li>Speaker Selection \u2192 Dependencies: State transitions become explicit <code>depends_on</code> relationships  </li> <li>Initiate Chat \u2192 Session Flow: Conversation starts become graph execution  </li> <li>Autonomous \u2192 Controlled: Replace AutoGen's unpredictable autonomy with enterprise controls</li> </ul>"},{"location":"migration/from-crewai/","title":"Migrating from CrewAI","text":"<p>CrewAI's verbose role-based agents \u2192 LionAGI's clean graph orchestration.</p>"},{"location":"migration/from-crewai/#basic-agent-creation","title":"Basic Agent Creation","text":"<p>CrewAI (Verbose):</p> <pre><code>from crewai import Agent, Task, Crew\n\ncoding_agent = Agent(\n    role=\"Senior Python Developer\",\n    goal=\"Craft well-designed and thought-out code\", \n    backstory=\"You are a senior Python developer with extensive experience in software architecture and best practices.\",\n    allow_code_execution=True\n)\n\ntask = Task(\n    description=\"Create a Python function to analyze data\",\n    expected_output=\"A well-documented Python function\",\n    agent=coding_agent\n)\n\ncrew = Crew(agents=[coding_agent], tasks=[task])\nresult = crew.kickoff()\n</code></pre> <p>LionAGI (Clean):</p> <pre><code>from lionagi import Branch, iModel\n\ncoder = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"),\n    system=\"Senior Python developer\"\n)\n\nresult = await coder.communicate(\"Create a Python function to analyze data\")\n</code></pre>"},{"location":"migration/from-crewai/#multi-agent-research","title":"Multi-Agent Research","text":"<p>CrewAI (Complex Setup):</p> <pre><code>researcher = Agent(\n    role=\"Senior Research Analyst\",\n    goal=\"Uncover cutting-edge developments in AI\",\n    backstory=\"You work at a leading tech think tank...\",\n    verbose=True\n)\n\nwriter = Agent(\n    role=\"Tech Content Strategist\", \n    goal=\"Craft compelling content on tech advancements\",\n    backstory=\"You are a renowned Content Strategist...\",\n    verbose=True\n)\n\nresearch_task = Task(\n    description=\"Conduct thorough research about AI Agents in 2025\",\n    expected_output=\"A list with 10 bullet points\",\n    agent=researcher\n)\n\nwriting_task = Task(\n    description=\"Write a compelling blog post based on research\",\n    expected_output=\"A 4 paragraph blog post\",\n    agent=writer\n)\n\ncrew = Crew(\n    agents=[researcher, writer],\n    tasks=[research_task, writing_task],\n    verbose=True\n)\n\nresult = crew.kickoff()\n</code></pre> <p>LionAGI (Simple Graph):</p> <pre><code>from lionagi import Session, Builder\n\nsession = Session()\nbuilder = Builder()\n\nresearch = builder.add_operation(\n    \"communicate\",\n    instruction=\"Research AI Agents developments in 2025\"\n)\n\nwriting = builder.add_operation(\n    \"communicate\", \n    depends_on=[research],\n    instruction=\"Write compelling blog post based on research\"\n)\n\nresult = await session.flow(builder.get_graph())\n</code></pre>"},{"location":"migration/from-crewai/#parallel-research-crew","title":"Parallel Research Crew","text":"<p>CrewAI (Sequential by Default):</p> <pre><code># CrewAI runs tasks sequentially unless explicitly configured\nagents = [researcher1, researcher2, researcher3]\ntasks = [task1, task2, task3]\n\ncrew = Crew(\n    agents=agents,\n    tasks=tasks,\n    process=Process.sequential  # Default behavior\n)\n\nresult = crew.kickoff()\n</code></pre> <p>LionAGI (Parallel by Nature):</p> <pre><code># Automatic parallel execution\nresearch_nodes = []\nfor topic in [\"transformers\", \"multimodal\", \"reasoning\"]:\n    node = builder.add_operation(\n        \"communicate\",\n        instruction=f\"Research {topic} in 2025\"\n    )\n    research_nodes.append(node)\n\nsynthesis = builder.add_operation(\n    \"communicate\",\n    depends_on=research_nodes,\n    instruction=\"Synthesize findings\"\n)\n\nresult = await session.flow(builder.get_graph())  # Parallel execution\n</code></pre>"},{"location":"migration/from-crewai/#tool-integration","title":"Tool Integration","text":"<p>CrewAI (Limited):</p> <pre><code>from crewai_tools import SerperDevTool\n\nsearch_tool = SerperDevTool()\n\nagent = Agent(\n    role=\"Research Analyst\",\n    goal=\"Research topics\",\n    backstory=\"...\",\n    tools=[search_tool]\n)\n</code></pre> <p>LionAGI (Flexible):</p> <pre><code>def custom_search(query: str) -&gt; str:\n    # Any custom logic\n    return f\"Results for {query}\"\n\nresearcher = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"),\n    tools=[custom_search]  # Direct function passing\n)\n\nresult = await researcher.ReAct(\n    instruct={\"instruction\": \"Research AI trends\"},\n    max_extensions=3\n)\n</code></pre>"},{"location":"migration/from-crewai/#error-handling","title":"Error Handling","text":"<p>CrewAI (Basic):</p> <pre><code># Limited error handling options\ntry:\n    result = crew.kickoff()\nexcept Exception as e:\n    print(f\"Crew failed: {e}\")\n</code></pre> <p>LionAGI (Robust):</p> <pre><code>import asyncio\n\nasync def robust_workflow():\n    try:\n        results = await asyncio.gather(\n            session.flow(research_graph),\n            session.flow(analysis_graph),\n            return_exceptions=True\n        )\n\n        successful = [r for r in results if not isinstance(r, Exception)]\n        return successful\n\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return None\n</code></pre>"},{"location":"migration/from-crewai/#cost-and-performance-tracking","title":"Cost and Performance Tracking","text":"<p>CrewAI (Not Built-in):</p> <pre><code># No built-in cost tracking\n# Must implement custom solutions\n</code></pre> <p>LionAGI (Native Support):</p> <pre><code>from lionagi.protocols.messages.assistant_response import AssistantResponse\n\n# Built-in cost tracking during execution\ncosts = 0\ndef get_context(node_id):\n    nonlocal costs\n    graph = builder.get_graph()\n    node = graph.internal_nodes[node_id]\n    branch = session.get_branch(node.branch_id, None)\n    if branch and len(branch.messages) &gt; 0:\n        if isinstance(msg := branch.messages[-1], AssistantResponse):\n            costs += msg.model_response.get(\"total_cost_usd\") or 0\n\n# Track costs across workflow\nfor node in research_nodes:\n    get_context(node)\n\nprint(f\"Total workflow cost: ${costs:.4f}\")\n</code></pre>"},{"location":"migration/from-crewai/#key-advantages","title":"Key Advantages","text":"<p>Shorter Code: LionAGI requires 70% less boilerplate than CrewAI Natural Parallelism: Built-in parallel execution vs CrewAI's sequential default Flexible Tools: Convert any function vs limited tool ecosystem Direct Control: Graph-based dependencies vs rigid role assignments Cost Tracking: Native monitoring vs manual implementation Error Recovery: Robust async patterns vs basic exception handling</p>"},{"location":"migration/from-crewai/#migration-benefits","title":"Migration Benefits","text":"<p>\u2705 Less Configuration: No verbose roles, goals, backstories \u2705 Better Performance: Automatic parallel execution \u2705 More Control: Explicit dependencies and error handling \u2705 Cost Visibility: Built-in usage tracking \u2705 Simpler Code: Focus on logic, not boilerplate</p>"},{"location":"migration/from-langchain/","title":"Migrating from LangChain","text":"<p>LangChain's complex abstraction layers \u2192 LionAGI's direct simplicity.</p>"},{"location":"migration/from-langchain/#basic-llm-chain","title":"Basic LLM Chain","text":"<p>LangChain (Verbose LCEL):</p> <pre><code>from langchain_core.prompts import ChatPromptTemplate\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.output_parsers import StrOutputParser\n\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant\"),\n    (\"human\", \"{input}\")\n])\n\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\noutput_parser = StrOutputParser()\n\nchain = prompt | model | output_parser\n\nresult = chain.invoke({\"input\": \"What is 2 + 2?\"})\n</code></pre> <p>LionAGI (Direct):</p> <pre><code>from lionagi import Branch, iModel\n\nassistant = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"),\n    system=\"You are a helpful assistant\"\n)\n\nresult = await assistant.communicate(\"What is 2 + 2?\")\n</code></pre> <p>Why This Is Better</p> <p>90% less boilerplate: No need for separate prompt templates, output parsers, or chain assembly Native async: Built for async/await from the ground up, not retrofitted Direct API: Call <code>communicate()</code> instead of learning LCEL pipe syntax Automatic memory: No manual memory management required</p>"},{"location":"migration/from-langchain/#agent-with-tools","title":"Agent with Tools","text":"<p>LangChain (Complex Setup):</p> <pre><code>from langchain.agents import create_openai_functions_agent, AgentExecutor\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain.tools import tool\nfrom langchain_community.tools.tavily_search import TavilySearchResults\n\n@tool\ndef multiply(x: float, y: float) -&gt; float:\n    \"\"\"Multiply two numbers\"\"\"\n    return x * y\n\nsearch = TavilySearchResults()\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n\nprompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant\"),\n    (\"human\", \"{input}\"),\n    MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n])\n\nagent = create_openai_functions_agent(llm, [multiply, search], prompt)\nagent_executor = AgentExecutor(agent=agent, tools=[multiply, search], verbose=True)\n\nresult = agent_executor.invoke({\"input\": \"What is 25 times 4?\"})\n</code></pre> <p>LionAGI (Clean):</p> <pre><code>from lionagi import Branch\n\ndef multiply(x: float, y: float) -&gt; float:\n    \"\"\"Multiply two numbers\"\"\"\n    return x * y\n\ndef search(query: str) -&gt; str:\n    \"\"\"Search for information\"\"\"\n    return f\"Search results for {query}\"\n\nagent = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"),\n    tools=[multiply, search]  # Direct function passing\n)\n\nresult = await agent.ReAct(\n    instruct={\"instruction\": \"What is 25 times 4?\"},\n    max_extensions=3\n)\n</code></pre> <p>Why This Is Better</p> <p>Direct function passing: No decorators or complex tool registration Built-in ReAct: No need to implement reasoning loops manually Cleaner imports: Everything from one package, not scattered across langchain- **Simpler debugging*: Direct function calls, not abstracted away behind agents and executors</p>"},{"location":"migration/from-langchain/#multi-agent-rag-workflow","title":"Multi-Agent RAG Workflow","text":"<p>LangChain (LangGraph Required):</p> <pre><code>from langgraph.graph import StateGraph, END\nfrom langchain_core.messages import HumanMessage, AIMessage\nfrom typing import TypedDict, List\nimport operator\n\nclass AgentState(TypedDict):\n    messages: List[HumanMessage | AIMessage]\n    research_results: List[str]\n    final_answer: str\n\ndef researcher_node(state: AgentState):\n    # Research implementation\n    research_llm = ChatOpenAI(model=\"gpt-4o-mini\")\n    result = research_llm.invoke(state[\"messages\"][-1])\n    return {\"research_results\": [result.content]}\n\ndef analyst_node(state: AgentState):\n    # Analysis implementation  \n    analyst_llm = ChatOpenAI(model=\"gpt-4o-mini\")\n    context = \"\\n\".join(state[\"research_results\"])\n    result = analyst_llm.invoke(f\"Analyze: {context}\")\n    return {\"final_answer\": result.content}\n\nworkflow = StateGraph(AgentState)\nworkflow.add_node(\"researcher\", researcher_node)\nworkflow.add_node(\"analyst\", analyst_node)\nworkflow.set_entry_point(\"researcher\")\nworkflow.add_edge(\"researcher\", \"analyst\")\nworkflow.add_edge(\"analyst\", END)\n\napp = workflow.compile()\nresult = app.invoke({\"messages\": [HumanMessage(content=\"Research AI trends\")]})\n</code></pre> <p>LionAGI (Natural Graph):</p> <pre><code>from lionagi import Session, Builder\n\nsession = Session()\nbuilder = Builder()\n\nresearch = builder.add_operation(\n    \"communicate\",\n    instruction=\"Research AI trends in detail\"\n)\n\nanalysis = builder.add_operation(\n    \"communicate\",\n    depends_on=[research],\n    instruction=\"Analyze research findings and provide insights\"\n)\n\nresult = await session.flow(builder.get_graph())\n</code></pre> <p>Why This Is Better</p> <p>No state classes: LionAGI eliminates the need for TypedDict state definitions - the framework handles data flow automatically Zero boilerplate: Compare 6 lines of LionAGI vs 30+ lines of LangGraph for the same workflow Automatic orchestration: Dependencies are declared once with <code>depends_on</code>, not manually configured with edges Natural syntax: Code reads like the business logic you want to accomplish, not framework mechanics Built-in parallelism: When dependencies allow, operations run concurrently without additional configuration</p>"},{"location":"migration/from-langchain/#parallel-processing","title":"Parallel Processing","text":"<p>LangChain (Complex State Management):</p> <pre><code>from langgraph.graph import StateGraph\nfrom typing import Annotated\nimport operator\n\nclass ParallelState(TypedDict):\n    topics: List[str]\n    results: Annotated[List[str], operator.add]\n\ndef research_node_1(state):\n    # Research topic 1\n    pass\n\ndef research_node_2(state):\n    # Research topic 2  \n    pass\n\ndef research_node_3(state):\n    # Research topic 3\n    pass\n\ndef synthesis_node(state):\n    # Combine results\n    pass\n\nworkflow = StateGraph(ParallelState)\nworkflow.add_node(\"research_1\", research_node_1)  \nworkflow.add_node(\"research_2\", research_node_2)\nworkflow.add_node(\"research_3\", research_node_3)\nworkflow.add_node(\"synthesis\", synthesis_node)\n\n# Complex parallel configuration\nworkflow.set_entry_point(\"research_1\")\nworkflow.set_entry_point(\"research_2\") \nworkflow.set_entry_point(\"research_3\")\nworkflow.add_edge([\"research_1\", \"research_2\", \"research_3\"], \"synthesis\")\n\napp = workflow.compile()\n</code></pre> <p>LionAGI (Automatic Parallel):</p> <pre><code># Natural parallel execution\ntopics = [\"transformers\", \"multimodal\", \"reasoning\"]\nresearch_nodes = []\n\nfor topic in topics:\n    node = builder.add_operation(\n        \"communicate\", \n        instruction=f\"Research {topic} developments\"\n    )\n    research_nodes.append(node)\n\nsynthesis = builder.add_operation(\n    \"communicate\",\n    depends_on=research_nodes,\n    instruction=\"Synthesize all research findings\"\n)\n\nresult = await session.flow(builder.get_graph())  # Automatic parallel\n</code></pre> <p>Why This Is Better</p> <p>True parallelism: LionAGI runs independent operations simultaneously No manual coordination: Dependencies automatically determine execution order Dynamic graphs: Can generate parallel operations programmatically Optimal performance: 3-4x faster than sequential execution for multi-step workflows</p>"},{"location":"migration/from-langchain/#memory-and-context","title":"Memory and Context","text":"<p>LangChain (Manual Memory Management):</p> <pre><code>from langchain.memory import ConversationBufferMemory\nfrom langchain.schema import BaseMessage\n\nmemory = ConversationBufferMemory(\n    memory_key=\"chat_history\",\n    return_messages=True\n)\n\n# Manual memory updates\nmemory.chat_memory.add_user_message(\"Hello\")\nmemory.chat_memory.add_ai_message(\"Hi there!\")\n\n# Complex integration with chains\nchain_with_memory = ConversationChain(\n    llm=llm,\n    memory=memory,\n    prompt=prompt,\n    verbose=True\n)\n</code></pre> <p>LionAGI (Built-in Memory):</p> <pre><code># Automatic persistent memory\nassistant = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\")\n)\n\n# Memory handled automatically\nresponse1 = await assistant.communicate(\"Hello\")\nresponse2 = await assistant.communicate(\"What did I just say?\")  # Remembers context\n</code></pre> <p>Why This Is Better</p> <p>Automatic persistence: No manual memory add operations Isolated contexts: Each Branch maintains its own conversation history No configuration: Works out of the box with sensible defaults Multi-agent memory: Each agent remembers its own interactions independently</p>"},{"location":"migration/from-langchain/#error-handling-and-debugging","title":"Error Handling and Debugging","text":"<p>LangChain (Limited Observability):</p> <pre><code>import langsmith\nfrom langchain_openai import ChatOpenAI\n\n# Requires external LangSmith setup for debugging\nos.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\nos.environ[\"LANGSMITH_API_KEY\"] = \"your-api-key\"\n\ntry:\n    result = chain.invoke(input_data)\nexcept Exception as e:\n    # Limited error context\n    print(f\"Chain failed: {e}\")\n</code></pre> <p>LionAGI (Rich Error Context):</p> <pre><code>try:\n    result = await session.flow(builder.get_graph())\nexcept Exception as e:\n    import traceback\n    traceback.print_exc()  # Full context\n\n    # Access detailed execution state\n    for node_id, node in builder.get_graph().internal_nodes.items():\n        branch = session.get_branch(node.branch_id, None)\n        if branch:\n            print(f\"Node {node_id}: {len(branch.messages)} messages\")\n</code></pre> <p>Why This Is Better</p> <p>Full execution context: Access to all Branch states and message history No external dependencies: Built-in debugging tools, no LangSmith needed Rich error information: See exactly where workflows fail with full context Production debugging: Easy to add monitoring and observability</p>"},{"location":"migration/from-langchain/#key-simplifications","title":"Key Simplifications","text":"<ul> <li>No LCEL Syntax: Direct function calls instead of pipe operators  </li> <li>No State Management: Automatic memory and context handling  </li> <li>No Complex Setup: Simple imports and initialization  </li> <li>No External Dependencies: Built-in observability and debugging  </li> <li>No Manual Orchestration: Automatic parallel execution</li> </ul>"},{"location":"migration/from-langchain/#migration-benefits","title":"Migration Benefits","text":"<p>\u2705 90% Less Code: Remove LCEL, state management, memory setup \u2705 Natural Async: Built for async/await from the ground up \u2705 Automatic Parallelism: No complex graph configuration needed \u2705 Simpler Debugging: Direct access to execution state \u2705 Built-in Memory: No manual memory management required \u2705 Cost Tracking: Native usage monitoring vs external tools</p>"},{"location":"patterns/","title":"Patterns","text":"<p>You're in Step 4 of the Learning Path</p> <p>You've mastered the core concepts. Now let's apply them with proven workflow patterns that solve real-world problems.</p> <p>These patterns represent battle-tested approaches to common multi-agent scenarios. Each pattern includes complete working code, use cases, and performance characteristics.</p>"},{"location":"patterns/#available-patterns","title":"Available Patterns","text":""},{"location":"patterns/#fan-outin","title":"Fan-Out/In","text":"<p>Distribute work to parallel agents, then synthesize results. Use for, Research, analysis, brainstorming</p>"},{"location":"patterns/#sequential-analysis","title":"Sequential Analysis","text":"<p>Build understanding step-by-step through dependent operations. Use for, Document processing, complex reasoning</p>"},{"location":"patterns/#conditional-flows","title":"Conditional Flows","text":"<p>Execute different paths based on runtime conditions. Use for, Dynamic workflows, decision trees</p>"},{"location":"patterns/#react-with-rag","title":"ReAct with RAG","text":"<p>Combine reasoning with retrieval-augmented generation. Use for, Knowledge-intensive tasks</p>"},{"location":"patterns/#tournament-validation","title":"Tournament Validation","text":"<p>Multiple approaches compete, best solution wins. Use for, Quality-critical outputs</p>"},{"location":"patterns/#pattern-selection-guide","title":"Pattern Selection Guide","text":"<pre><code>def select_pattern(task):\n    if task.needs_multiple_perspectives:\n        return \"fan-out-in\"\n    elif task.requires_step_by_step_building:\n        return \"sequential-analysis\"\n    elif task.has_conditional_logic:\n        return \"conditional-flows\"\n    elif task.needs_external_knowledge:\n        return \"react-with-rag\"\n    elif task.requires_best_quality:\n        return \"tournament-validation\"\n    else:\n        return \"simple-branch\"\n</code></pre>"},{"location":"patterns/#common-combinations","title":"Common Combinations","text":""},{"location":"patterns/#research-pipeline","title":"Research Pipeline","text":"<pre><code>Fan-Out (gather) \u2192 Sequential (analyze) \u2192 Fan-Out (verify)\n</code></pre>"},{"location":"patterns/#document-processing","title":"Document Processing","text":"<pre><code>Sequential (extract) \u2192 Conditional (classify) \u2192 Fan-Out (process by type)\n</code></pre>"},{"location":"patterns/#decision-making","title":"Decision Making","text":"<pre><code>Fan-Out (options) \u2192 Tournament (evaluate) \u2192 Sequential (implement)\n</code></pre> <p>Ready for Production Examples?</p> <p>You've learned the patterns - now see them implemented in complete, production-ready workflows:</p> <p>Next: Cookbook - Complete working examples you can copy and modify Or: Advanced Topics - Deep dive into custom operations and optimization</p>"},{"location":"patterns/conditional-flows/","title":"Conditional Flows Pattern","text":"<p>Dynamic workflow paths based on runtime conditions and decision points.</p>"},{"location":"patterns/conditional-flows/#when-to-use-this-pattern","title":"When to Use This Pattern","text":"<p>Use conditional flows when:</p> <ul> <li>Different inputs need different processing paths</li> <li>Decision points determine next steps</li> <li>Workflow adapts based on intermediate results</li> <li>Error conditions need special handling</li> <li>Quality gates determine continuation</li> </ul>"},{"location":"patterns/conditional-flows/#basic-conditional-flow","title":"Basic Conditional Flow","text":"<pre><code>from lionagi import Branch, Session, Builder, iModel\n\nsession = Session()\nbuilder = Builder(\"conditional_example\")\n\n# Classifier branch\nclassifier = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"),\n    system=\"Classify requests as: question, task, or creative.\"\n)\n\n# Specialized handlers  \nqa_expert = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"),\n    system=\"Answer questions clearly and accurately.\"\n)\n\ntask_expert = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"),\n    system=\"Provide step-by-step task guidance.\"\n)\n\ncreative_expert = Branch(\n    chat_model=iModel(provider=\"anthropic\", model=\"claude-3-sonnet-20240229\"),\n    system=\"Help with creative projects and brainstorming.\"\n)\n\nsession.include_branches([classifier, qa_expert, task_expert, creative_expert])\n\nuser_input = \"How do I learn Python programming?\"\n\n# Step 1: Classify the request\nclassify_op = builder.add_operation(\n    \"communicate\",\n    branch=classifier,\n    instruction=f\"Classify as 'question', 'task', or 'creative': {user_input}\"\n)\n\n# Step 2: Route based on classification\nresult = await session.flow(builder.get_graph())\nclassification = result[\"operation_results\"][classify_op]\n\n# Route to appropriate handler\nif \"question\" in classification.lower():\n    handler = qa_expert\nelif \"task\" in classification.lower():\n    handler = task_expert\nelse:\n    handler = creative_expert\n\n# Execute chosen path\nresponse = await handler.communicate(f\"Handle this request: {user_input}\")\n</code></pre>"},{"location":"patterns/conditional-flows/#quality-gate-pattern","title":"Quality Gate Pattern","text":"<p>Content processing with quality thresholds:</p> <pre><code># Content creation and review branches\ncreator = Branch(\n    chat_model=iModel(provider=\"anthropic\", model=\"claude-3-sonnet-20240229\"),\n    system=\"Create high-quality content.\"\n)\n\nreviewer = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"),\n    system=\"Review content quality and provide scores 1-10.\"\n)\n\neditor = Branch(\n    chat_model=iModel(provider=\"anthropic\", model=\"claude-3-sonnet-20240229\"),\n    system=\"Improve and refine content.\"\n)\n\ntopic = \"The future of renewable energy\"\n\n# Stage 1: Create initial content\ninitial_content = await creator.communicate(f\"Write article about: {topic}\")\n\n# Stage 2: Quality review \nquality_review = await reviewer.communicate(\n    f\"Rate this content 1-10: {initial_content}\"\n)\n\n# Quality gate: improve if below threshold\nif any(str(i) in quality_review for i in range(1, 7)):\n    # Below threshold - improve content\n    final_content = await editor.communicate(\n        f\"Improve based on review: {initial_content}\\n\\nReview: {quality_review}\"\n    )\nelse:\n    # Quality acceptable\n    final_content = initial_content\n</code></pre>"},{"location":"patterns/conditional-flows/#error-handling-with-fallbacks","title":"Error Handling with Fallbacks","text":"<p>Graceful degradation when primary processing fails:</p> <pre><code># Main and fallback processors\nprocessor = Branch(\n    chat_model=iModel(provider=\"anthropic\", model=\"claude-3-sonnet-20240229\"),\n    system=\"Process complex requests thoroughly.\"\n)\n\nfallback = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"), \n    system=\"Handle requests with simplified approaches.\"\n)\n\n# Process with fallback\ntry:\n    # Try main processor first\n    result = await processor.communicate(request)\nexcept Exception:\n    # Fallback to simpler processor\n    try:\n        result = await fallback.communicate(f\"Simplified: {request}\")\n    except Exception:\n        result = \"Unable to process request\"\n</code></pre>"},{"location":"patterns/conditional-flows/#best-practices","title":"Best Practices","text":""},{"location":"patterns/conditional-flows/#clear-decision-criteria","title":"Clear Decision Criteria","text":"<pre><code># Good: Specific, measurable criteria\n\"Rate complexity as 'simple' (1-3 steps) or 'complex' (4+ steps)\"\n\n# Avoid: Vague criteria  \n\"Is this hard?\"\n</code></pre>"},{"location":"patterns/conditional-flows/#explicit-routing-logic","title":"Explicit Routing Logic","text":"<pre><code># Clear routing based on parsed results\nif \"urgent\" in priority_assessment.lower():\n    handler = urgent_processor\nelse:\n    handler = standard_processor\n</code></pre>"},{"location":"patterns/conditional-flows/#always-have-fallbacks","title":"Always Have Fallbacks","text":"<pre><code># Graceful degradation\ntry:\n    result = await complex_processor.communicate(request)\nexcept Exception:\n    result = await simple_processor.communicate(f\"Simplified: {request}\")\n</code></pre>"},{"location":"patterns/conditional-flows/#when-to-use","title":"When to Use","text":"<p>Perfect for: Content routing, difficulty adaptation, quality control, error recovery, resource optimization</p> <p>Key advantage: Runtime decision-making optimizes processing paths and handles varying input conditions intelligently.</p> <p>Conditional flows create adaptive workflows that route processing based on content, quality thresholds, and error conditions.</p>"},{"location":"patterns/fan-out-in/","title":"Fan-Out/In Pattern","text":"<p>Distribute work to multiple agents in parallel, then combine their results.</p>"},{"location":"patterns/fan-out-in/#basic-pattern","title":"Basic Pattern","text":"<pre><code>from lionagi import Session, Builder, Branch, iModel\n\nsession = Session()\nbuilder = Builder()\n\n# Fan-out: Create parallel research tasks\ntopics = [\"market analysis\", \"competitor review\", \"customer feedback\"]\nresearch_nodes = []\n\nfor topic in topics:\n    node = builder.add_operation(\"communicate\", instruction=f\"Research {topic}\")\n    research_nodes.append(node)\n\n# Fan-in: Synthesize all results\nsynthesis = builder.add_operation(\n    \"communicate\",\n    depends_on=research_nodes,\n    instruction=\"Combine all research into comprehensive report\"\n)\n\nresult = await session.flow(builder.get_graph())\n</code></pre> <p>This pattern demonstrates the power of parallel processing in LionAGI. The fan-out phase creates three research operations that run simultaneously, each focusing on a different aspect. The fan-in phase waits for all research to complete, then synthesizes the findings into a comprehensive report. This approach is 3x faster than sequential processing while providing more thorough coverage than any single analysis.</p>"},{"location":"patterns/fan-out-in/#production-implementation-with-cost-tracking","title":"Production Implementation with Cost Tracking","text":"<p><pre><code>from lionagi import Branch, iModel, Session, Builder\nfrom lionagi.fields import LIST_INSTRUCT_FIELD_MODEL, Instruct\nfrom lionagi.models import AssistantResponse\n\nasync def production_fan_out_in():\n    \"\"\"Production version with error handling and cost tracking\"\"\"\n\n    try:\n        orc_branch = Branch(\n            chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"),\n            name=\"orchestrator\"\n        )\n        session = Session(default_branch=orc_branch)\n        builder = Builder(\"ProductionFanOut\")\n\n        # Initial decomposition\n        root = builder.add_operation(\n            \"operate\",\n            instruct=Instruct(\n                instruction=\"Break down the analysis task into parallel components\",\n                context=\"market_analysis\"\n            ),\n            reason=True,\n            field_models=[LIST_INSTRUCT_FIELD_MODEL]\n        )\n\n        result = await session.flow(builder.get_graph())\n        instruct_models = result[\"operation_results\"][root].instruct_models\n\n        # Create research nodes\n        research_nodes = []\n        for instruction in instruct_models:\n            node = builder.add_operation(\n                \"communicate\",\n                depends_on=[root],\n                chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"),\n                **instruction.to_dict()\n            )\n            research_nodes.append(node)\n\n        # Execute research with cost tracking\n        costs = 0\n\n        def get_context(node_id):\n            nonlocal costs\n            graph = builder.get_graph()\n            node = graph.internal_nodes[node_id]\n            branch = session.get_branch(node.branch_id, None)\n            if (branch and len(branch.messages) &gt; 0 and \n                isinstance(msg := branch.messages[-1], AssistantResponse)):\n                costs += msg.model_response.get(\"total_cost_usd\") or 0\n                return f\"\"\"\nResponse: {msg.model_response.get(\"result\") or \"Not available\"}\nSummary: {msg.model_response.get(\"summary\") or \"Not available\"}\n                \"\"\".strip()\n\n        await session.flow(builder.get_graph())\n        ctx = [get_context(i) for i in research_nodes]\n\n        # Synthesis\n        synthesis = builder.add_operation(\n            \"communicate\",\n            depends_on=research_nodes,\n            branch=orc_branch,\n            instruction=\"Synthesize researcher findings\",\n            context=[i for i in ctx if i is not None]\n        )\n\n        final_result = await session.flow(builder.get_graph())\n        result_synthesis = final_result[\"operation_results\"][synthesis]\n\n        # Optional: Visualize execution graph\n        builder.visualize(\"Fan-out/in execution pattern\")\n\n        print(f\"Analysis complete. Total cost: ${costs:.4f}\")\n        return result_synthesis\n\n    except Exception as e:\n        print(f\"Fan-out-in failed: {e}\")\n        import traceback\n        traceback.print_exc()\n        return None\n\nasyncio.run(production_fan_out_in())\n````\n\n## When to Use\n\n!!! success \"Perfect For\"\n    - **Complex research**: Multiple perspectives on the same topic\n    - **Code reviews**: Security, performance, style analysis in parallel\n    - **Market analysis**: Different domain experts working simultaneously  \n    - **Large datasets**: Parallel investigation and analysis\n\n!!! tip \"Pattern Indicators\"\n    Use fan-out/in when:\n\n    - Problem benefits from simultaneous analysis\n    - Individual tasks can run independently  \n    - Final answer requires synthesis of perspectives\n    - Time constraints favor parallel over sequential execution\n\n## Execution Flow\n</code></pre> [Orchestrator]      \u2193 (decompose task) [Task Breakdown]      \u2193 (fan-out) \u250c\u2500[Researcher 1]\u2500\u2510 \u251c\u2500[Researcher 2]\u2500\u2524 \u2192 (parallel execution) \u251c\u2500[Researcher 3]\u2500\u2524 \u2514\u2500[Researcher 4]\u2500\u2518      \u2193 (fan-in) [Synthesis]      \u2193 [Final Result] <pre><code>## Key Implementation Notes\n\n### Context Extraction Pattern\n\n```python\ndef get_context(node_id):\n    graph = builder.get_graph()\n    node = graph.internal_nodes[node_id]\n    branch = session.get_branch(node.branch_id, None)\n    if (branch and len(branch.messages) &gt; 0 and \n        isinstance(msg := branch.messages[-1], AssistantResponse)):\n        return msg.model_response.get(\"result\") or \"Not available\"\n</code></pre></p>"},{"location":"patterns/fan-out-in/#cost-tracking-pattern","title":"Cost Tracking Pattern","text":"<pre><code>costs = 0\ndef track_costs(msg):\n    nonlocal costs\n    costs += msg.model_response.get(\"total_cost_usd\") or 0\n</code></pre>"},{"location":"patterns/fan-out-in/#error-handling","title":"Error Handling","text":"<pre><code>try:\n    result = await session.flow(builder.get_graph())\nexcept Exception as e:\n    print(f\"Execution failed: {e}\")\n    import traceback\n    traceback.print_exc()\n    return None\n</code></pre>"},{"location":"patterns/fan-out-in/#performance-characteristics","title":"Performance Characteristics","text":"<p>Expected Performance</p> <ul> <li>Speed: 3-4x faster than sequential for complex analysis</li> <li>Quality: Higher insights through diverse perspectives  </li> <li>Success Rate: ~95% completion rate</li> <li>Scale: Optimal with 3-5 parallel researchers</li> </ul> <p>Cost Considerations</p> <p>Cost scales proportionally with number of parallel researchers. Balance thoroughness vs. expense based on your use case.</p> <p>Fan-out/in delivers comprehensive analysis through parallel specialization, making complex investigations both faster and more thorough.</p>"},{"location":"patterns/react-with-rag/","title":"ReAct with RAG Pattern","text":"<p>Tool-augmented reasoning with retrieval for knowledge-intensive tasks.</p>"},{"location":"patterns/react-with-rag/#when-to-use-this-pattern","title":"When to Use This Pattern","text":"<p>Use ReAct with RAG when:</p> <ul> <li>Tasks require external knowledge or data</li> <li>Multi-step reasoning is needed</li> <li>Information must be gathered and synthesized</li> <li>Complex problem-solving requires both thinking and acting</li> </ul>"},{"location":"patterns/react-with-rag/#pattern-structure","title":"Pattern Structure","text":"<p>ReAct cycles through: Reason \u2192 Act \u2192 Observe \u2192 Repeat</p>"},{"location":"patterns/react-with-rag/#basic-implementation","title":"Basic Implementation","text":"<pre><code>from lionagi import Branch, Session, Builder, iModel\n\ndef search_knowledge(query: str) -&gt; dict:\n    \"\"\"Your knowledge retrieval function\"\"\"\n    return {\"query\": query, \"results\": [...]}\n\n# ReAct-enabled Branch\nresearcher = Branch(\n    chat_model=iModel(provider=\"anthropic\", model=\"claude-3-sonnet-20240229\"),\n    system=\"Research and reason step by step using available tools.\",\n    tools=[search_knowledge]\n)\n\nsession = Session()\nbuilder = Builder()\n\n# ReAct operation\ntask = builder.add_operation(\n    \"ReAct\",\n    branch=researcher,\n    instruct={\n        \"instruction\": \"Research neural networks in machine learning\",\n        \"context\": \"Provide comprehensive analysis with specific details\"\n    },\n    max_extensions=3  # Reasoning steps limit\n)\n\nresult = await session.flow(builder.get_graph())\n</code></pre>"},{"location":"patterns/react-with-rag/#multi-tool-react","title":"Multi-Tool ReAct","text":"<pre><code>def search_papers(query: str) -&gt; dict:\n    \"\"\"Search academic sources\"\"\"\n    return {\"source\": \"papers\", \"results\": [...]}\n\ndef search_docs(query: str) -&gt; dict:\n    \"\"\"Search documentation\"\"\"\n    return {\"source\": \"docs\", \"results\": [...]}\n\n# Multi-tool researcher\nresearcher = Branch(\n    chat_model=iModel(provider=\"anthropic\", model=\"claude-3-opus-20240229\"),\n    system=\"Expert researcher with access to multiple knowledge sources.\",\n    tools=[search_papers, search_docs]\n)\n\n# Complex research task\ntask = builder.add_operation(\n    \"ReAct\",\n    branch=researcher,\n    instruct={\n        \"instruction\": \"Compare transformer architectures and performance metrics\",\n        \"guidance\": \"Use academic sources, then analyze technical documentation\"\n    },\n    max_extensions=5,\n    reason=True  # Include reasoning in output\n)\n</code></pre>"},{"location":"patterns/react-with-rag/#file-based-rag","title":"File-Based RAG","text":"<pre><code>from lionagi.tools.file.reader import ReaderTool\n\n# Document analyst\nanalyst = Branch(\n    chat_model=iModel(provider=\"anthropic\", model=\"claude-3-sonnet-20240229\"),\n    system=\"Analyze documents systematically, extracting key information.\",\n    tools=[ReaderTool]\n)\n\n# Document analysis task\nanalysis = builder.add_operation(\n    \"ReAct\",\n    branch=analyst,\n    instruct={\n        \"instruction\": \"Analyze project documentation for core architecture\",\n        \"guidance\": \"Start with README, then extract key technical details\"\n    },\n    max_extensions=4\n)\n</code></pre>"},{"location":"patterns/react-with-rag/#key-patterns","title":"Key Patterns","text":""},{"location":"patterns/react-with-rag/#tool-selection-strategy","title":"Tool Selection Strategy","text":"<pre><code># Phase-based tool usage\ninformation_gathering = [\"search\", \"retrieve\", \"extract\"]\nanalysis_phase = [\"analyze\", \"calculate\", \"compare\"] \nsynthesis_phase = [\"verify\", \"synthesize\", \"conclude\"]\n</code></pre>"},{"location":"patterns/react-with-rag/#reasoning-control","title":"Reasoning Control","text":"<pre><code>instruct = {\n    \"instruction\": \"Clear task definition\",\n    \"guidance\": \"Step-by-step process guidance\", \n    \"context\": \"Background for decision making\"\n}\n</code></pre>"},{"location":"patterns/react-with-rag/#performance-limits","title":"Performance Limits","text":"<pre><code>max_extensions=5  # Prevent infinite reasoning loops\nreason=True      # Include reasoning traces\n</code></pre>"},{"location":"patterns/react-with-rag/#best-practices","title":"Best Practices","text":"<ul> <li>Tool Design: Return structured data with consistent formats</li> <li>Reasoning Guidance: Provide clear step-by-step instructions</li> <li>Context Management: Keep context focused and relevant</li> <li>Error Handling: Set reasonable limits on reasoning steps</li> </ul> <p>ReAct with RAG enables systematic information gathering and reasoning for complex, knowledge-intensive tasks.</p>"},{"location":"patterns/sequential-analysis/","title":"Sequential Analysis Pattern","text":"<p>Build complex understanding step-by-step through dependent operations.</p>"},{"location":"patterns/sequential-analysis/#when-to-use-this-pattern","title":"When to Use This Pattern","text":"<p>Use sequential analysis when:</p> <ul> <li>Each step builds upon previous findings</li> <li>Processing requires logical progression</li> <li>Context accumulation improves quality</li> <li>Complex documents need structured analysis</li> </ul>"},{"location":"patterns/sequential-analysis/#basic-pattern-structure","title":"Basic Pattern Structure","text":"<pre><code>from lionagi import Branch, Session, Builder, iModel\n\nsession = Session()\nbuilder = Builder(\"document_analysis\")\n\n# Create analyzer branch\nanalyzer = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"),\n    system=\"You are a document analysis expert.\"\n)\nsession.include_branches([analyzer])\n\ndocument = \"Your document content here...\"\n\n# Step 1: Extract key topics\nextract_topics = builder.add_operation(\n    \"communicate\",\n    branch=analyzer,\n    instruction=f\"Extract 3-5 key topics from this document: {document}\"\n)\n\n# Step 2: Analyze each topic (depends on step 1)\nanalyze_topics = builder.add_operation(\n    \"communicate\",\n    branch=analyzer,\n    instruction=\"For each topic, provide detailed analysis\",\n    depends_on=[extract_topics]\n)\n\n# Step 3: Synthesize insights (depends on step 2)\nsynthesize = builder.add_operation(\n    \"communicate\",\n    branch=analyzer,\n    instruction=\"What are the 3 most important insights?\",\n    depends_on=[analyze_topics]\n)\n\n# Execute the sequential workflow\nresult = await session.flow(builder.get_graph())\n</code></pre>"},{"location":"patterns/sequential-analysis/#multi-step-analysis","title":"Multi-Step Analysis","text":"<p>Research paper analysis with sequential dependency:</p> <pre><code># Specialized research analyzer\nresearcher = Branch(\n    chat_model=iModel(provider=\"anthropic\", model=\"claude-3-sonnet-20240229\"),\n    system=\"Research analyst specializing in academic papers.\"\n)\n\npaper_text = \"Your research paper content...\"\n\n# Step 1: Structure identification\nidentify_structure = builder.add_operation(\n    \"communicate\",\n    branch=researcher,\n    instruction=f\"Identify and summarize each section: {paper_text}\"\n)\n\n# Step 2: Technical analysis\nanalyze_technical = builder.add_operation(\n    \"communicate\",\n    branch=researcher,\n    instruction=\"Analyze technical contributions and methodology\",\n    depends_on=[identify_structure]\n)\n\n# Step 3: Evaluate novelty\nevaluate_novelty = builder.add_operation(\n    \"communicate\",\n    branch=researcher,\n    instruction=\"Assess novelty and significance of contributions\",\n    depends_on=[analyze_technical]\n)\n\n# Step 4: Final assessment\nfinal_assessment = builder.add_operation(\n    \"communicate\",\n    branch=researcher,\n    instruction=\"Provide comprehensive evaluation\",\n    depends_on=[evaluate_novelty]\n)\n\nresult = await session.flow(builder.get_graph())\n</code></pre>"},{"location":"patterns/sequential-analysis/#context-building","title":"Context Building","text":"<p>Each step accumulates context for deeper analysis:</p> <pre><code>investigator = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"),\n    system=\"Thorough investigator building understanding incrementally.\"\n)\n\n# Sequential investigation steps\nobserve = builder.add_operation(\n    \"communicate\",\n    branch=investigator,\n    instruction=\"Make initial observations about the data\"\n)\n\nhypothesize = builder.add_operation(\n    \"communicate\", \n    branch=investigator,\n    instruction=\"Generate 3 hypotheses based on observations\",\n    depends_on=[observe]\n)\n\nanalyze = builder.add_operation(\n    \"communicate\",\n    branch=investigator, \n    instruction=\"Analyze each hypothesis for evidence\",\n    depends_on=[hypothesize]\n)\n\nconclude = builder.add_operation(\n    \"communicate\",\n    branch=investigator,\n    instruction=\"Draw conclusions with confidence levels\",\n    depends_on=[analyze]\n)\n\nresult = await session.flow(builder.get_graph())\n</code></pre>"},{"location":"patterns/sequential-analysis/#best-practices","title":"Best Practices","text":""},{"location":"patterns/sequential-analysis/#clear-dependencies","title":"Clear Dependencies","text":"<pre><code># Good: Clear progression\nstep1 = builder.add_operation(\"communicate\", instruction=\"Extract facts\")\nstep2 = builder.add_operation(\"communicate\", instruction=\"Analyze facts\", depends_on=[step1])\nstep3 = builder.add_operation(\"communicate\", instruction=\"Draw conclusions\", depends_on=[step2])\n</code></pre>"},{"location":"patterns/sequential-analysis/#consistent-context","title":"Consistent Context","text":"<pre><code># Use same branch for context continuity\nanalyzer = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"),\n    system=\"Maintain context across analysis steps.\"\n)\n</code></pre>"},{"location":"patterns/sequential-analysis/#quality-assessment","title":"Quality Assessment","text":"<pre><code># Include data assessment as first step\nassess_data = builder.add_operation(\n    \"communicate\",\n    instruction=\"Assess data quality and identify limitations\"\n)\n</code></pre>"},{"location":"patterns/sequential-analysis/#when-to-use","title":"When to Use","text":"<p>Perfect for: Document analysis, research workflows, investigations, decision making, problem solving</p> <p>Key advantage: Each step builds meaningfully on previous work, leading to more thorough and accurate results than parallel analysis.</p> <p>Sequential analysis creates structured understanding through logical progression and context accumulation.</p>"},{"location":"patterns/tournament-validation/","title":"Tournament Validation Pattern","text":"<p>Competitive refinement where multiple agents propose solutions, then compete for the best result.</p>"},{"location":"patterns/tournament-validation/#when-to-use-this-pattern","title":"When to Use This Pattern","text":"<p>Use tournament validation when:</p> <ul> <li>Quality matters more than speed</li> <li>Multiple valid approaches exist</li> <li>Objective evaluation criteria can be defined</li> <li>Stakes are high (important decisions, critical code, etc.)</li> <li>You want to minimize bias from single perspectives</li> </ul>"},{"location":"patterns/tournament-validation/#pattern-structure","title":"Pattern Structure","text":"<ol> <li>Generation: Multiple agents create different solutions</li> <li>Evaluation: Judge agents rate each solution</li> <li>Tournament: Solutions compete head-to-head</li> <li>Refinement: Winners refine their approach</li> <li>Selection: Best solution emerges</li> </ol>"},{"location":"patterns/tournament-validation/#basic-tournament-pattern","title":"Basic Tournament Pattern","text":"<pre><code>from lionagi import Branch, Session, Builder, iModel\n\nsession = Session()\nbuilder = Builder(\"solution_tournament\")\n\n# Create diverse problem solvers\ncreative_solver = Branch(\n    chat_model=iModel(provider=\"anthropic\", model=\"claude-3-sonnet-20240229\"),\n    system=\"Approach problems creatively with unconventional solutions.\"\n)\n\nanalytical_solver = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"),\n    system=\"Solve problems with systematic, analytical approaches.\"\n)\n\npractical_solver = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"),\n    system=\"Focus on practical, implementable solutions.\"\n)\n\n# Judge for evaluation\njudge = Branch(\n    chat_model=iModel(provider=\"anthropic\", model=\"claude-3-opus-20240229\"),\n    system=\"Evaluate solutions objectively on feasibility and effectiveness.\"\n)\n\nsession.include_branches([creative_solver, analytical_solver, practical_solver, judge])\n\nproblem = \"Design a system to reduce food waste while maintaining profitability\"\n\n# Phase 1: Generate competing solutions\ncreative_solution = builder.add_operation(\n    \"communicate\",\n    branch=creative_solver,\n    instruction=f\"Propose creative solution: {problem}\"\n)\n\nanalytical_solution = builder.add_operation(\n    \"communicate\", \n    branch=analytical_solver,\n    instruction=f\"Propose systematic solution: {problem}\"\n)\n\npractical_solution = builder.add_operation(\n    \"communicate\",\n    branch=practical_solver,\n    instruction=f\"Propose practical solution: {problem}\"\n)\n\n# Execute solution generation\nresult = await session.flow(builder.get_graph())\n\nsolutions = {\n    \"creative\": result[\"operation_results\"][creative_solution],\n    \"analytical\": result[\"operation_results\"][analytical_solution],\n    \"practical\": result[\"operation_results\"][practical_solution]\n}\n\n# Phase 2: Judge evaluates all solutions\nevaluation = await judge.communicate(f\"\"\"\nEvaluate these solutions for: {problem}\n\nCreative: {solutions['creative']}\nAnalytical: {solutions['analytical']}\nPractical: {solutions['practical']}\n\nRate each 1-10 on feasibility, effectiveness, innovation.\nDeclare winner with reasoning.\n\"\"\")\n</code></pre>"},{"location":"patterns/tournament-validation/#multi-round-tournament","title":"Multi-Round Tournament","text":"<p>Elimination rounds with refinement:</p> <pre><code># Create multiple competitors with different approaches\ncompetitors = [\n    Branch(system=\"Prioritize user experience\", chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\")),\n    Branch(system=\"Focus on technical excellence\", chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\")),\n    Branch(system=\"Emphasize cost-effectiveness\", chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"))\n]\n\n# Panel of specialized judges\njudges = [\n    Branch(system=\"Judge business viability\", chat_model=iModel(provider=\"anthropic\", model=\"claude-3-sonnet-20240229\")),\n    Branch(system=\"Judge technical feasibility\", chat_model=iModel(provider=\"anthropic\", model=\"claude-3-sonnet-20240229\"))\n]\n\nchallenge = \"Create a mobile app for better sleep habits\"\n\n# Round 1: Initial proposals\nproposals = {}\nfor i, competitor in enumerate(competitors):\n    proposal = await competitor.communicate(f\"Propose solution: {challenge}\")\n    proposals[f\"competitor_{i}\"] = proposal\n\n# Round 1: Judging\nscores = {}\nfor judge in judges:\n    judge_scores = await judge.communicate(f\"\"\"\n    Score these proposals 1-10: {proposals}\n    Format: competitor_0: X/10, competitor_1: Y/10, etc.\n    \"\"\")\n    scores[judge] = judge_scores\n\n# Round 2: Top performers refine solutions\n# (Parse scores to select finalists)\nfinalists = competitors[:2]  # Top 2\nfor finalist in finalists:\n    refined = await finalist.communicate(\"Refine your solution based on judge feedback\")\n\n# Final judging\nwinner = await judges[0].communicate(\"Select winner from refined proposals\")\n</code></pre>"},{"location":"patterns/tournament-validation/#code-tournament","title":"Code Tournament","text":"<p>Specialized competition for code solutions:</p> <pre><code># Different coding philosophies\nperformance_coder = Branch(\n    system=\"Write optimized, performance-focused code\",\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\")\n)\n\nreadable_coder = Branch(\n    system=\"Write clean, maintainable code\",\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\")\n)\n\nsecure_coder = Branch(\n    system=\"Write secure, robust code with error handling\",\n    chat_model=iModel(provider=\"anthropic\", model=\"claude-3-sonnet-20240229\")\n)\n\ncode_judge = Branch(\n    system=\"Senior developer evaluating code quality and best practices\",\n    chat_model=iModel(provider=\"anthropic\", model=\"claude-3-opus-20240229\")\n)\n\ncoding_challenge = \"Write Python function to get top 10 users by score from list of dicts\"\n\n# Generate solutions\nsolutions = {}\nsolutions[\"performance\"] = await performance_coder.communicate(f\"Optimize for speed: {coding_challenge}\")\nsolutions[\"readable\"] = await readable_coder.communicate(f\"Optimize for readability: {coding_challenge}\")  \nsolutions[\"secure\"] = await secure_coder.communicate(f\"Optimize for security: {coding_challenge}\")\n\n# Judge evaluates all solutions\nevaluation = await code_judge.communicate(f\"\"\"\nRate these solutions 1-10 on correctness, performance, readability, security:\n{solutions}\n\nDeclare winner and suggest hybrid approach.\n\"\"\")\n</code></pre>"},{"location":"patterns/tournament-validation/#collaborative-tournament","title":"Collaborative Tournament","text":"<p>Competition with cross-pollination:</p> <pre><code># Competing approaches\ninnovative = Branch(system=\"Focus on disruption\", chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"))\npractical = Branch(system=\"Focus on execution\", chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"))\nuser_focused = Branch(system=\"Focus on user experience\", chat_model=iModel(provider=\"anthropic\", model=\"claude-3-sonnet-20240229\"))\n\nproject = \"Sustainable urban transportation\"\n\n# Initial proposals\nproposals = {}\nfor team in [innovative, practical, user_focused]:\n    proposals[team] = await team.communicate(f\"Propose approach: {project}\")\n\n# Cross-pollination round\nrefined_proposals = {}\nfor team in [innovative, practical, user_focused]:\n    refined_proposals[team] = await team.communicate(f\"\"\"\n    Review other proposals: {proposals}\n    Refine your approach by incorporating best elements from others.\n    \"\"\")\n\n# Collaborative synthesis\nmediator = Branch(system=\"Identify synergies between approaches\", chat_model=iModel(provider=\"anthropic\", model=\"claude-3-opus-20240229\"))\n\nfinal_solution = await mediator.communicate(f\"\"\"\nCreate hybrid solution combining strengths: {refined_proposals}\n\"\"\")\n</code></pre>"},{"location":"patterns/tournament-validation/#best-practices","title":"Best Practices","text":""},{"location":"patterns/tournament-validation/#clear-evaluation-criteria","title":"Clear Evaluation Criteria","text":"<pre><code># Define specific, measurable criteria\nevaluation_criteria = {\n    \"feasibility\": \"Can this be realistically implemented?\",\n    \"effectiveness\": \"Will this solve the problem effectively?\", \n    \"innovation\": \"How creative/novel is this approach?\",\n    \"scalability\": \"Can this work at larger scales?\"\n}\n</code></pre>"},{"location":"patterns/tournament-validation/#diverse-perspectives","title":"Diverse Perspectives","text":"<pre><code># Different specialties and approaches\ncompetitors = [\n    Branch(system=\"Focus on technical excellence\"),\n    Branch(system=\"Prioritize user experience\"),\n    Branch(system=\"Emphasize cost-effectiveness\"),\n    Branch(system=\"Consider sustainability\")\n]\n</code></pre>"},{"location":"patterns/tournament-validation/#objective-judging","title":"Objective Judging","text":"<pre><code># Use specific scoring rubrics\njudge_prompt = \"\"\"\nRate 1-10 on:\n1. Technical feasibility\n2. Market viability  \n3. Implementation complexity\n4. Expected impact\n\nProvide scores and reasoning.\n\"\"\"\n</code></pre>"},{"location":"patterns/tournament-validation/#iterative-refinement","title":"Iterative Refinement","text":"<pre><code># Allow winners to improve based on feedback\nrefinement_prompt = f\"\"\"\nYour solution scored highest but judges noted: {feedback}\nRefine to address concerns while maintaining strengths.\n\"\"\"\n</code></pre>"},{"location":"patterns/tournament-validation/#when-to-use","title":"When to Use","text":"<p>Perfect for: High-stakes decisions, creative problems, quality-critical tasks, complex analysis, innovation challenges</p> <p>Key advantage: Competitive dynamics drive higher quality through diverse perspectives, objective evaluation, and iterative refinement.</p> <p>Tournament validation creates the highest quality solutions by leveraging competition and collaborative improvement.</p>"},{"location":"quickstart/installation/","title":"Installation","text":""},{"location":"quickstart/installation/#install-lionagi","title":"Install LionAGI","text":"<p>Recommended: Use <code>uv</code> for faster dependency resolution: <pre><code>uv add lionagi\n</code></pre></p> <p>Alternative: Standard pip installation: <pre><code>pip install lionagi\n</code></pre></p>"},{"location":"quickstart/installation/#configure-api-keys","title":"Configure API Keys","text":"<p>Create a <code>.env</code> file in your project root:</p> <pre><code># At minimum, add one provider\nOPENAI_API_KEY=your_key_here\n\n# Optional: Additional providers\nANTHROPIC_API_KEY=your_key_here\nOPENROUTER_API_KEY=your_key_here\nNVIDIA_NIM_API_KEY=your_key_here\nGROQ_API_KEY=your_key_here\nPERPLEXITY_API_KEY=your_key_here\nEXA_API_KEY=your_key_here\n</code></pre> <p>LionAGI automatically loads from <code>.env</code> - no manual configuration needed.</p>"},{"location":"quickstart/installation/#verify-installation","title":"Verify Installation","text":"<p>Run this test to confirm everything works:</p> <pre><code>from lionagi import Branch, iModel\n\nasync def test():\n    gpt4 = iModel(provider=\"openai\", model=\"gpt-4o-mini\")\n    branch = Branch(chat_model=gpt4)\n    reply = await branch.chat(\"Hello from LionAGI!\")\n    print(f\"LionAGI says: {reply}\")\n\nif __name__ == \"__main__\":\n    import anyio\n    anyio.run(test)\n</code></pre> <p>Expected output: A conversational response from the model.</p>"},{"location":"quickstart/installation/#supported-providers","title":"Supported Providers","text":"<p>LionAGI comes pre-configured for these providers:</p> <ul> <li><code>openai</code> - GPT-5, GPT-4.1, o4-mini</li> <li><code>anthropic</code> - Claude 4.5 Sonnet, Claude 4.1 Opus</li> <li><code>claude_code</code> - Claude Code SDK integration</li> <li><code>ollama</code> - Local model hosting</li> <li><code>openrouter</code> - Access 200+ models via single API</li> <li><code>nvidia_nim</code> - NVIDIA inference microservices</li> <li><code>groq</code> - Fast inference on LPU hardware</li> <li><code>perplexity</code> - Search-augmented responses</li> </ul>"},{"location":"quickstart/installation/#custom-providers","title":"Custom Providers","text":"<p>OpenAI-compatible endpoints: <pre><code>custom = iModel(\n    provider=\"openai_compatible\",\n    model=\"custom-model\",\n    api_key=\"your_key\",\n    base_url=\"https://custom-endpoint.com/v1\"\n)\n</code></pre></p>"},{"location":"quickstart/your-first-flow/","title":"Your First Flow","text":"<p>This quickstart shows LionAGI's core pattern: multiple specialized branches working together.</p> <p>We'll build a self-critiquing joke generator - one branch writes jokes, another critiques them, then they iterate to improve.</p>"},{"location":"quickstart/your-first-flow/#complete-example","title":"Complete Example","text":"<pre><code>from lionagi import Branch, iModel\n\nchat_model = iModel(provider=\"openai\", model=\"gpt-4.1-mini\")\n\nasync def generate_joke(joke_request):\n    # Create two specialized branches\n    comedian = Branch(\n        chat_model=chat_model,\n        system=\"You are a comedian who makes technical concepts funny.\"\n    )\n\n    editor = Branch(\n        chat_model=chat_model,\n        system=\"You are an editor who improves clarity and punch.\"\n    )\n\n    # Flow: Generate \u2192 Critique \u2192 Revise \u2192 Verify\n    joke = await comedian.communicate(\n        \"Write a short joke\",\n        context={\"user_input\": joke_request}\n    )\n\n    feedback = await editor.communicate(\n        \"Give humorous critical feedback to improve this joke by addressing clarity and punch.\",\n        context={\"joke\": joke}\n    )\n\n    revision = await comedian.communicate(\n        \"Revise the joke based on this feedback.\",\n        context={\"feedback\": feedback}\n    )\n\n    final_check = await editor.communicate(\n        \"Is this version better than the original? If yes, reply the following token (including the square brackets) '[YES]' only, otherwise elaborate on why not without including the token.\",\n        context={\"revised_joke\": revision}\n    )\n\n    # Return improved version if approved\n    if \"[yes]\" in final_check.lower():\n        return revision\n    return joke  # Fallback to original if not improved\n\n# Run it\nif __name__ == \"__main__\":\n    import anyio\n    result = anyio.run(generate_joke, \"machine learning\")\n    print(result)\n</code></pre>"},{"location":"quickstart/your-first-flow/#what-just-happened","title":"What Just Happened","text":"<p>Branch: Independent conversation context with its own system prompt and memory - <code>comedian</code> = creative joke writing - <code>editor</code> = critical feedback</p> <p>Flow: Sequential operations across branches 1. Comedian generates initial joke 2. Editor provides feedback 3. Comedian revises based on feedback 4. Editor validates improvement</p> <p>Context: Each <code>communicate()</code> call can include context from previous steps</p>"},{"location":"quickstart/your-first-flow/#try-it","title":"Try It","text":"<p>Save the code above and run: <pre><code>uv run your_script.py\n</code></pre></p> <p>Expected output: A refined joke about machine learning, improved through iteration.</p>"},{"location":"reference/changelog/","title":"Changelog","text":"<p>All notable changes to LionAGI are documented here.</p>"},{"location":"reference/changelog/#unreleased","title":"[Unreleased]","text":""},{"location":"reference/changelog/#added","title":"Added","text":"<ul> <li>Comprehensive documentation overhaul with streamlined examples</li> <li>Migration guides for AutoGen, CrewAI, and LangChain</li> <li>Enterprise-focused cookbook examples</li> <li>Integration guides for tools, vector stores, and MCP servers</li> </ul>"},{"location":"reference/changelog/#changed","title":"Changed","text":"<ul> <li>Simplified tool integration: functions can be passed directly without   <code>func_to_tool</code> wrapper</li> <li>Documentation style updated to focus on practical, production-ready patterns</li> <li>Tool API streamlined for better developer experience</li> </ul>"},{"location":"reference/changelog/#01511-2025-08-24","title":"[0.15.11] - 2025-08-24","text":""},{"location":"reference/changelog/#added_1","title":"Added","text":"<ul> <li><code>extract_json</code> and <code>fuzzy_json</code> functions in new <code>lionagi.ln</code> module for   robust JSON handling</li> <li>Availability check functions for optional dependencies:   <code>check_docling_available</code>, <code>check_networkx_available</code>,   <code>check_matplotlib_available</code></li> <li><code>list_adapters</code> method in Pile class for adapter enumeration</li> <li>Content serialization and validation methods in Node class</li> <li>New concurrency-related classes exported in <code>__all__</code> for better accessibility</li> </ul>"},{"location":"reference/changelog/#changed_1","title":"Changed","text":"<ul> <li>Performance: Replaced standard <code>json</code> with <code>orjson</code> for faster JSON   operations</li> <li>PostgreSQL Adapter: Major cleanup and refactoring (374 lines removed, 88   added) with enhanced table creation logic</li> <li>Utils Refactoring: Moved utilities from monolithic <code>utils.py</code> to organized   <code>lionagi.ln</code> module (393 lines removed)</li> <li>Node Serialization: Updated adaptation methods to use <code>as_jsonable</code>   instead of custom serialization</li> <li>Element Methods: Refactored serialization methods and enhanced <code>to_dict</code>   functionality</li> <li>MessageManager: Simplified methods by leveraging <code>filter_by_type</code>   functionality</li> <li>Type Consistency: Updated Progression class type variable from <code>E</code> to <code>T</code></li> <li>Updated pydapter dependency to v1.0.2</li> </ul>"},{"location":"reference/changelog/#fixed","title":"Fixed","text":"<ul> <li>Parameter name in MessageManager methods: <code>reversed</code> \u2192 <code>reverse</code></li> <li>Import statement for <code>fix_json_string</code> in test files</li> <li>Output examples in <code>persist_to_postgres_supabase</code> notebook</li> <li>Docling import handling in ReaderTool initialization</li> <li>Item type validation improvements in Pile class</li> </ul>"},{"location":"reference/changelog/#removed","title":"Removed","text":"<ul> <li>Package Management Module: Deleted entire <code>lionagi.libs.package</code> module   (138 lines removed)</li> <li>Removed <code>imports.py</code>, <code>management.py</code>, <code>params.py</code>, <code>system.py</code></li> <li>Redundant import statements and dead code cleanup</li> <li>StepModel and related tests from step module</li> </ul>"},{"location":"reference/changelog/#0159-2025-08-20","title":"[0.15.9] - 2025-08-20","text":""},{"location":"reference/changelog/#added_2","title":"Added","text":"<ul> <li>JSON serialization utilities with orjson support</li> <li>Enhanced Element class with orjson-based JSON serialization methods</li> <li>User serialization method in Session class</li> </ul>"},{"location":"reference/changelog/#changed_2","title":"Changed","text":"<ul> <li>JSON Performance: Replaced <code>json.dumps</code> with <code>ln.json_dumps</code> using orjson   for consistent, faster serialization</li> <li>EventStatus Refactoring: Updated to use <code>ln.Enum</code> with improved JSON   serialization</li> <li>CI/CD: Upgraded actions/checkout to v5, removed documentation build   workflow</li> </ul>"},{"location":"reference/changelog/#0158-2025-08-20","title":"[0.15.8] - 2025-08-20","text":""},{"location":"reference/changelog/#changed_3","title":"Changed","text":"<ul> <li>Lowered psutil dependency requirements for broader compatibility</li> </ul>"},{"location":"reference/changelog/#0157-2025-08-18","title":"[0.15.7] - 2025-08-18","text":""},{"location":"reference/changelog/#fixed_1","title":"Fixed","text":"<ul> <li>Enhanced Params initialization to validate allowed keys</li> <li>Parameter validation improvements</li> </ul>"},{"location":"reference/changelog/#0156-2025-08-18","title":"[0.15.6] - 2025-08-18","text":""},{"location":"reference/changelog/#added_3","title":"Added","text":"<ul> <li><code>aicall_params</code> to register_operation for async parallel execution support</li> </ul>"},{"location":"reference/changelog/#fixed_2","title":"Fixed","text":"<ul> <li>Flow execution refactored to use <code>alcall</code> for improved concurrency handling</li> <li>Updated <code>alcall</code> and <code>bcall</code> parameter handling with better kwargs support</li> <li>Import statements for ConcurrencyEvent and Semaphore consistency</li> </ul>"},{"location":"reference/changelog/#0155-2025-08-17","title":"[0.15.5] - 2025-08-17","text":""},{"location":"reference/changelog/#added_4","title":"Added","text":"<ul> <li><code>aiofiles</code> dependency for async file operations</li> <li>Utility functions for union type handling and type annotations</li> <li>Async PostgreSQL adapter registration with availability checks</li> </ul>"},{"location":"reference/changelog/#changed_4","title":"Changed","text":"<ul> <li>Enhanced Pile class with validation and serialization methods</li> <li>Refactored PostgreSQL adapter checks into utility functions</li> </ul>"},{"location":"reference/changelog/#fixed_3","title":"Fixed","text":"<ul> <li>Parameter name correction: <code>strict</code> \u2192 <code>strict_type</code> in Pile initialization</li> <li>Exception handling: <code>TypeError</code> \u2192 <code>ValidationError</code> in collection validation</li> <li>Explicit boolean checks for async PostgreSQL availability</li> </ul>"},{"location":"reference/changelog/#0154-2025-08-17","title":"[0.15.4] - 2025-08-17","text":""},{"location":"reference/changelog/#added_5","title":"Added","text":"<ul> <li>User serialization functionality in Branch class</li> </ul>"},{"location":"reference/changelog/#0153-2025-08-16","title":"[0.15.3] - 2025-08-16","text":""},{"location":"reference/changelog/#added_6","title":"Added","text":"<ul> <li>Comprehensive tests for operation cancellation and edge conditions</li> <li>SKIPPED status to EventStatus for better execution tracking</li> </ul>"},{"location":"reference/changelog/#changed_5","title":"Changed","text":"<ul> <li>Execution status set to CANCELLED for cancelled API calls</li> <li>Enhanced operation handling with edge condition validation and filtering   aggregation metadata</li> </ul>"},{"location":"reference/changelog/#fixed_4","title":"Fixed","text":"<ul> <li>Flow regression issues in operation execution</li> <li>Parameter handling cleanup and improved cancellation error handling</li> </ul>"},{"location":"reference/changelog/#0152-2025-08-16","title":"[0.15.2] - 2025-08-16","text":""},{"location":"reference/changelog/#added_7","title":"Added","text":"<ul> <li>Operation decorator to simplify function registration as operations</li> <li>Comprehensive tests for Session.operation() decorator functionality</li> </ul>"},{"location":"reference/changelog/#changed_6","title":"Changed","text":"<ul> <li>Updated author information in README</li> </ul>"},{"location":"reference/changelog/#0151-2025-08-16","title":"[0.15.1] - 2025-08-16","text":""},{"location":"reference/changelog/#added_8","title":"Added","text":"<ul> <li>Mock operation methods for improved async operation handling in tests</li> <li><code>to_dict</code> method to Execution class for better serialization</li> </ul>"},{"location":"reference/changelog/#changed_7","title":"Changed","text":"<ul> <li>Integrated OperationManager into Branch and Session classes for enhanced   operation management</li> <li>Simplified OperationManager initialization with enhanced operation   registration logic</li> <li>Enhanced Execution class response serialization handling</li> </ul>"},{"location":"reference/changelog/#fixed_5","title":"Fixed","text":"<ul> <li>Import statement cleanup across multiple files for consistency</li> </ul>"},{"location":"reference/changelog/#01411-2025-08-14","title":"[0.14.11] - 2025-08-14","text":""},{"location":"reference/changelog/#added_9","title":"Added","text":"<ul> <li>Updated operation_builder notebook to demonstrate graph serialization</li> </ul>"},{"location":"reference/changelog/#fixed_6","title":"Fixed","text":"<ul> <li>Graph serialization/deserialization in Graph class</li> <li>Field parameters in Operation class</li> <li>Redundant file mode in open() calls</li> </ul>"},{"location":"reference/changelog/#changed_8","title":"Changed","text":"<ul> <li>Organized imports in throttle.py, cleaned up unused imports in test files</li> </ul>"},{"location":"reference/changelog/#01410-2025-08-08","title":"[0.14.10] - 2025-08-08","text":""},{"location":"reference/changelog/#added_10","title":"Added","text":"<ul> <li>XML parsing and conversion utilities with new XMLParser class</li> </ul>"},{"location":"reference/changelog/#fixed_7","title":"Fixed","text":"<ul> <li>Hook registry async function calls with proper await usage</li> <li>Import path for to_num utility in test files</li> <li>Error handling improvements in HookRegistry</li> </ul>"},{"location":"reference/changelog/#0137-2025-07-21","title":"[0.13.7] - 2025-07-21","text":""},{"location":"reference/changelog/#added_11","title":"Added","text":"<ul> <li>Notebook for sequential analysis of academic claims using Operation Graphs</li> </ul>"},{"location":"reference/changelog/#removed_1","title":"Removed","text":"<ul> <li><code>action_batch_size</code> parameter from operate and branch methods</li> </ul>"},{"location":"reference/changelog/#changed_9","title":"Changed","text":"<ul> <li>Updated documentation to reflect parameter changes</li> </ul>"},{"location":"reference/troubleshooting/","title":"Troubleshooting Guide","text":"<p>Quick solutions to common LionAGI issues.</p>"},{"location":"reference/troubleshooting/#installation-issues","title":"Installation Issues","text":"<p>Import Error: Module not found</p> <pre><code># Error: ModuleNotFoundError: No module named 'lionagi'\npip install lionagi\n\n# For latest development version\npip install git+https://github.com/khive-ai/lionagi.git\n</code></pre> <p>Missing Optional Dependencies</p> <pre><code># Error: docling not available\npip install lionagi[pdf]  # For PDF processing\n\n# Error: matplotlib not available  \npip install matplotlib\n\n# Error: networkx not available\npip install networkx\n</code></pre> <p>Python Version Issues</p> <pre><code># LionAGI requires Python 3.10+\npython --version  # Check version\npip install lionagi  # Only works on 3.10+\n</code></pre>"},{"location":"reference/troubleshooting/#api-key-errors","title":"API Key Errors","text":"<p>OpenAI Authentication</p> <pre><code>import os\nfrom lionagi import Branch, iModel\n\n# Set API key\nos.environ[\"OPENAI_API_KEY\"] = \"your-key-here\"\n\nbranch = Branch(chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"))\n</code></pre> <p>Multiple Providers</p> <pre><code># Different ways to set keys\nos.environ[\"ANTHROPIC_API_KEY\"] = \"your-anthropic-key\"\nos.environ[\"OPENAI_API_KEY\"] = \"your-openai-key\"\n\n# Or in iModel directly\nmodel = iModel(provider=\"openai\", model=\"gpt-4o-mini\", api_key=\"your-key\")\n</code></pre>"},{"location":"reference/troubleshooting/#asyncawait-problems","title":"Async/Await Problems","text":"<p>Missing await</p> <pre><code># \u274c Wrong - missing await\nbranch = Branch()\nresult = branch.communicate(\"Hello\")  # Returns coroutine\n\n# \u2705 Correct\nresult = await branch.communicate(\"Hello\")\n</code></pre> <p>Running in Jupyter</p> <pre><code># Jupyter handles async automatically\nbranch = Branch()\nresult = await branch.communicate(\"Hello\")  # Works in Jupyter\n\n# For scripts, use asyncio.run()\nimport asyncio\n\nasync def main():\n    branch = Branch()\n    result = await branch.communicate(\"Hello\")\n    return result\n\n# \u274c Wrong in script\nresult = await main()\n\n# \u2705 Correct in script  \nresult = asyncio.run(main())\n</code></pre> <p>Mixing sync/async</p> <pre><code># \u274c Wrong - can't await in sync function\ndef sync_function():\n    branch = Branch()\n    return await branch.communicate(\"Hello\")  # SyntaxError\n\n# \u2705 Correct - make function async\nasync def async_function():\n    branch = Branch()\n    return await branch.communicate(\"Hello\")\n</code></pre>"},{"location":"reference/troubleshooting/#performance-issues","title":"Performance Issues","text":"<p>Slow Parallel Execution</p> <pre><code># \u274c Slow - sequential execution\nresults = []\nfor topic in topics:\n    result = await branch.communicate(f\"Research {topic}\")\n    results.append(result)\n\n# \u2705 Fast - parallel execution\nimport asyncio\n\ntasks = [branch.communicate(f\"Research {topic}\") for topic in topics]\nresults = await asyncio.gather(*tasks)\n</code></pre> <p>Graph vs Direct Calls</p> <pre><code># Use graphs for complex workflows\nfrom lionagi import Session, Builder\n\nsession = Session()\nbuilder = Builder()\n\n# Parallel operations\nfor topic in topics:\n    builder.add_operation(\"communicate\", instruction=f\"Research {topic}\")\n\nresults = await session.flow(builder.get_graph())\n</code></pre> <p>Model Rate Limits</p> <pre><code># Add delays between requests\nimport asyncio\n\nasync def rate_limited_call():\n    try:\n        result = await branch.communicate(\"Hello\")\n        return result\n    except Exception as e:\n        if \"rate limit\" in str(e).lower():\n            await asyncio.sleep(1)  # Wait 1 second\n            return await branch.communicate(\"Hello\")\n        raise e\n</code></pre>"},{"location":"reference/troubleshooting/#memory-issues","title":"Memory Issues","text":"<p>Token Limit Exceeded</p> <pre><code># \u274c Error: Context too long\nlong_message = \"very long text...\" * 1000\nresult = await branch.communicate(long_message)  # May fail\n\n# \u2705 Solution: Chunk large inputs\ndef chunk_text(text, chunk_size=4000):\n    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n\nchunks = chunk_text(long_message)\nresults = []\nfor chunk in chunks:\n    result = await branch.communicate(f\"Process this: {chunk}\")\n    results.append(result)\n</code></pre> <p>Branch Memory Accumulation</p> <pre><code># Branch remembers all messages\nbranch = Branch()\nfor i in range(1000):\n    await branch.communicate(f\"Message {i}\")  # Memory keeps growing\n\n# Solution: Create new branch when needed\ndef get_fresh_branch():\n    return Branch(chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"))\n\n# Or clear messages (if implemented)\n# branch.messages.clear()  # Check if available\n</code></pre>"},{"location":"reference/troubleshooting/#graph-execution-issues","title":"Graph Execution Issues","text":"<p>Circular Dependencies</p> <pre><code># \u274c Error: Circular dependency\nnode_a = builder.add_operation(\"communicate\", depends_on=[node_b])\nnode_b = builder.add_operation(\"communicate\", depends_on=[node_a])  # Error\n\n# \u2705 Solution: Linear dependencies\nnode_a = builder.add_operation(\"communicate\", instruction=\"Step 1\")\nnode_b = builder.add_operation(\"communicate\", depends_on=[node_a], instruction=\"Step 2\")\n</code></pre> <p>Empty Graph Results</p> <pre><code># Check graph execution\ntry:\n    result = await session.flow(builder.get_graph())\n    print(\"Graph result:\", result)\n\n    # Access specific nodes\n    graph = builder.get_graph()\n    for node_id, node in graph.internal_nodes.items():\n        print(f\"Node {node_id}: {node}\")\n\nexcept Exception as e:\n    import traceback\n    traceback.print_exc()\n</code></pre>"},{"location":"reference/troubleshooting/#cost-tracking-issues","title":"Cost Tracking Issues","text":"<p>Missing Cost Data</p> <pre><code>def get_costs(node_id, builder, session):\n    try:\n        graph = builder.get_graph()\n        node = graph.internal_nodes[node_id]\n        branch = session.get_branch(node.branch_id, None)\n\n        if branch and len(branch.messages) &gt; 0:\n            msg = branch.messages[-1]\n            if hasattr(msg, 'model_response'):\n                return msg.model_response.get(\"total_cost_usd\", 0)\n    except Exception as e:\n        print(f\"Cost tracking error: {e}\")\n    return 0\n</code></pre>"},{"location":"reference/troubleshooting/#error-diagnosis","title":"Error Diagnosis","text":"<p>Enable Verbose Logging</p> <pre><code>import logging\nlogging.basicConfig(level=logging.DEBUG)\n\n# Or for specific operations\nresult = await session.flow(builder.get_graph(), verbose=True)\n</code></pre> <p>Debugging Graph State</p> <pre><code># Check graph structure\ngraph = builder.get_graph()\nprint(f\"Nodes: {len(graph.internal_nodes)}\")\nfor node_id, node in graph.internal_nodes.items():\n    print(f\"  {node_id}: {node}\")\n\n# Check session state\nprint(f\"Branches: {len(session.branches)}\")\n</code></pre>"},{"location":"reference/troubleshooting/#getting-help","title":"Getting Help","text":"<p>GitHub Issues: Report bugs at khive-ai/lionagi/issues</p> <p>Check Version: <code>pip show lionagi</code> for installed version</p> <p>Minimal Reproduction: Include minimal code that reproduces the issue</p>"},{"location":"reference/api/","title":"API Reference","text":"<p>Core classes and functions for LionAGI framework.</p>"},{"location":"reference/api/#core-modules","title":"Core Modules","text":""},{"location":"reference/api/#session","title":"Session","text":"<pre><code>from lionagi import Session\n\nsession = Session()\n</code></pre> <p>Workspace for coordinating multiple branches and managing graph execution.</p>"},{"location":"reference/api/#branch","title":"Branch","text":"<pre><code>from lionagi import Branch, iModel\n\nbranch = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"),\n    system=\"You are a helpful assistant\"\n)\n</code></pre> <p>Individual agent abstraction with persistent memory, tools, and specialized behavior.</p>"},{"location":"reference/api/#builder","title":"Builder","text":"<pre><code>from lionagi import Builder\n\nbuilder = Builder()\nnode = builder.add_operation(\"communicate\", instruction=\"Hello\")\n</code></pre> <p>Graph construction pattern for building complex multi-agent workflows with dependencies.</p>"},{"location":"reference/api/#imodel","title":"iModel","text":"<pre><code>from lionagi import iModel\n\n# OpenAI\nmodel = iModel(provider=\"openai\", model=\"gpt-4o-mini\")\n\n# Anthropic  \nmodel = iModel(provider=\"anthropic\", model=\"claude-3-5-sonnet-20241022\")\n\n# Claude Code\nmodel = iModel(provider=\"claude_code\", model=\"sonnet\")\n</code></pre> <p>Universal model interface supporting OpenAI, Anthropic, Ollama, and Claude Code.</p>"},{"location":"reference/api/#operations","title":"Operations","text":""},{"location":"reference/api/#communicate","title":"communicate","text":"<pre><code>result = await branch.communicate(\"What is 2 + 2?\")\n</code></pre> <p>Direct conversation with a single branch.</p>"},{"location":"reference/api/#react","title":"ReAct","text":"<pre><code>result = await branch.ReAct(\n    instruct={\"instruction\": \"Research AI trends\"},\n    tools=[\"search_tool\"],\n    max_extensions=3\n)\n</code></pre> <p>Reasoning + Acting workflow for systematic problem solving with tools.</p>"},{"location":"reference/api/#operate","title":"operate","text":"<pre><code>result = await branch.operate(\n    instruct=Instruct(instruction=\"Analyze data\", context=data),\n    response_format=AnalysisReport\n)\n</code></pre> <p>Structured operation with Pydantic output validation.</p>"},{"location":"reference/api/#tools","title":"Tools","text":""},{"location":"reference/api/#function-tools","title":"Function Tools","text":"<pre><code>def multiply(x: float, y: float) -&gt; float:\n    return x * y\n\nbranch = Branch(tools=[multiply])  # Direct function passing\n</code></pre>"},{"location":"reference/api/#readertool","title":"ReaderTool","text":"<pre><code>from lionagi.tools.types import ReaderTool\n\nbranch = Branch(tools=[ReaderTool])\nresult = await branch.ReAct(\n    instruct={\"instruction\": \"Read document.pdf and summarize\"},\n    tools=[\"reader_tool\"]\n)\n</code></pre>"},{"location":"reference/api/#custom-tool-class","title":"Custom Tool Class","text":"<pre><code>from lionagi import Tool\nfrom pydantic import BaseModel\n\nclass SearchParams(BaseModel):\n    query: str\n    max_results: int = 5\n\ndef web_search(query: str, max_results: int = 5) -&gt; str:\n    return f\"Search results for: {query}\"\n\nsearch_tool = Tool(\n    func_callable=web_search,\n    request_options=SearchParams\n)\n</code></pre>"},{"location":"reference/api/#graph-execution","title":"Graph Execution","text":""},{"location":"reference/api/#basic-flow","title":"Basic Flow","text":"<pre><code>from lionagi import Session, Builder\n\nsession = Session()\nbuilder = Builder()\n\n# Sequential operations\nstep1 = builder.add_operation(\"communicate\", instruction=\"Step 1\")\nstep2 = builder.add_operation(\"communicate\", depends_on=[step1], instruction=\"Step 2\")\n\nresult = await session.flow(builder.get_graph())\n</code></pre>"},{"location":"reference/api/#parallel-flow","title":"Parallel Flow","text":"<pre><code># Parallel research\ntopics = [\"AI\", \"ML\", \"NLP\"]\nresearch_nodes = []\n\nfor topic in topics:\n    node = builder.add_operation(\"communicate\", instruction=f\"Research {topic}\")\n    research_nodes.append(node)\n\n# Synthesis\nsynthesis = builder.add_operation(\n    \"communicate\",\n    depends_on=research_nodes,\n    instruction=\"Synthesize research findings\"\n)\n\nresult = await session.flow(builder.get_graph())\n</code></pre>"},{"location":"reference/api/#data-types","title":"Data Types","text":""},{"location":"reference/api/#instruct","title":"Instruct","text":"<pre><code>from lionagi.types import Instruct\n\ninstruct = Instruct(\n    instruction=\"Analyze the data\",\n    context={\"data\": [1, 2, 3]},\n    guidance=\"Focus on trends\"\n)\n</code></pre>"},{"location":"reference/api/#node","title":"Node","text":"<pre><code># Access node data after execution\ngraph = builder.get_graph()\nfor node_id, node in graph.internal_nodes.items():\n    branch = session.get_branch(node.branch_id)\n    print(f\"Node {node_id}: {len(branch.messages)} messages\")\n</code></pre>"},{"location":"reference/api/#cost-tracking","title":"Cost Tracking","text":"<pre><code>def get_costs(node_id, builder, session):\n    graph = builder.get_graph()\n    node = graph.internal_nodes[node_id]\n    branch = session.get_branch(node.branch_id, None)\n\n    if branch and len(branch.messages) &gt; 0:\n        msg = branch.messages[-1]\n        if hasattr(msg, 'model_response'):\n            return msg.model_response.get(\"total_cost_usd\", 0)\n    return 0\n\ntotal_cost = sum(get_costs(node, builder, session) for node in research_nodes)\n</code></pre>"},{"location":"reference/api/#error-handling","title":"Error Handling","text":"<pre><code>try:\n    result = await session.flow(builder.get_graph())\nexcept Exception as e:\n    import traceback\n    traceback.print_exc()\n\n    # Access execution state for debugging\n    for node_id, node in builder.get_graph().internal_nodes.items():\n        branch = session.get_branch(node.branch_id, None)\n        if branch:\n            print(f\"Node {node_id} messages: {len(branch.messages)}\")\n</code></pre>"},{"location":"reference/api/#configuration","title":"Configuration","text":"<pre><code>import os\n\n# API Keys\nos.environ[\"OPENAI_API_KEY\"] = \"your-key\"\nos.environ[\"ANTHROPIC_API_KEY\"] = \"your-key\"\n\n# Model Configuration\nmodel = iModel(\n    provider=\"openai\",\n    model=\"gpt-4o-mini\", \n    temperature=0.7,\n    max_tokens=1000\n)\n</code></pre>"},{"location":"thinking-in-lionagi/","title":"Thinking in LionAGI","text":"<p>You're in Step 2 of the Learning Path</p> <p>Coming from the Quick Start? Great! Now let's understand the mental model that makes LionAGI different from other frameworks.</p> <p>The key to successfully using LionAGI is understanding its fundamental paradigm shift: from conversations to graphs, from sequential to parallel, from rigid to flexible.</p>"},{"location":"thinking-in-lionagi/#core-ideas","title":"Core Ideas","text":""},{"location":"thinking-in-lionagi/#why-lionagi","title":"Why LionAGI?","text":"<p>The technical differences that matter.</p>"},{"location":"thinking-in-lionagi/#branches-as-agents","title":"Branches as Agents","text":"<p>Why we call them branches, not agents.</p>"},{"location":"thinking-in-lionagi/#graphs-over-chains","title":"Graphs Over Chains","text":"<p>Parallel execution with dependency management.</p>"},{"location":"thinking-in-lionagi/#the-builder-pattern","title":"The Builder Pattern","text":"<p>Constructing workflows programmatically.</p>"},{"location":"thinking-in-lionagi/#the-paradigm-shift","title":"The Paradigm Shift","text":""},{"location":"thinking-in-lionagi/#from-conversations-to-graphs","title":"From Conversations to Graphs","text":"<p>Old way: Agents have conversations</p> <pre><code>agent1: \"I found X\"\nagent2: \"Based on X, I think Y\"\nagent3: \"Given X and Y, we should Z\"\n</code></pre> <p>LionAGI way: Agents execute operations</p> <pre><code>x = await agent1.analyze()\ny = await agent2.evaluate(x)\nz = await agent3.synthesize([x, y])\n</code></pre>"},{"location":"thinking-in-lionagi/#from-sequential-to-parallel","title":"From Sequential to Parallel","text":"<p>Old way: Wait for each step</p> <pre><code>step1()  # 2 sec\nstep2()  # 2 sec  \nstep3()  # 2 sec\n# Total: 6 seconds\n</code></pre> <p>LionAGI way: Run simultaneously</p> <pre><code>await gather(step1(), step2(), step3())\n# Total: 2 seconds\n</code></pre>"},{"location":"thinking-in-lionagi/#from-rigid-to-flexible","title":"From Rigid to Flexible","text":"<p>Old way: Fixed agent roles</p> <pre><code>researcher = Agent(role=\"researcher\")\n# Always a researcher, can't adapt\n</code></pre> <p>LionAGI way: Dynamic capabilities</p> <pre><code>branch = Branch(system=context_specific_prompt)\n# Adapts to the task at hand\n</code></pre> <p>Ready for Core Concepts?</p> <p>Now that you understand LionAGI's mental model, dive into the practical mechanics:</p> <p>Next: Core Concepts - Learn Sessions, Branches, Operations, and Memory Or jump to: Patterns if you want to see these concepts in action</p>"},{"location":"thinking-in-lionagi/branches-as-agents/","title":"Branches as Agents","text":"<p>Understanding the Branch abstraction - your agents with memory and tools.</p>"},{"location":"thinking-in-lionagi/branches-as-agents/#what-is-a-branch","title":"What is a Branch?","text":"<p>A Branch is LionAGI's core agent abstraction with persistent memory, tools, and specialized behavior.</p> <pre><code>from lionagi import Branch, iModel\n\n# Create a branch with model and system prompt\nbranch = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"),\n    system=\"You are a helpful research assistant\"\n)\n\n# Branch maintains conversation memory\nresult1 = await branch.communicate(\"Research AI trends\")\nresult2 = await branch.communicate(\"What are the key risks?\")  # Remembers context\n\n# Check conversation history\nprint(f\"Messages in memory: {len(branch.messages)}\")\n</code></pre>"},{"location":"thinking-in-lionagi/branches-as-agents/#key-advantage-persistent-memory","title":"Key Advantage: Persistent Memory","text":"<p>Unlike stateless API calls, Branches maintain conversation context automatically.</p> <pre><code># LionAGI Branch - stateful conversations\nbranch = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\")\n)\n\nresult1 = await branch.communicate(\"What is machine learning?\")\nresult2 = await branch.communicate(\"What are its applications?\")  # Remembers context\n</code></pre>"},{"location":"thinking-in-lionagi/branches-as-agents/#branch-components","title":"Branch Components","text":"<p>Branches combine memory, tools, and specialized behavior.</p> <pre><code>def calculate(expression: str) -&gt; float:\n    \"\"\"Your calculation function\"\"\"\n    return eval(expression)  # Use safe evaluation in production\n\n# Create branch with tools and system prompt\nbranch = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"),\n    system=\"You are a math tutor. Use tools for calculations.\",\n    tools=[calculate]  # Direct function passing\n)\n\n# Branch has persistent memory and can use tools\nresult = await branch.communicate(\"What is 15 * 23? Explain the calculation.\")\n\n# Inspect branch components\nprint(f\"Messages: {len(branch.messages)} stored\")\nprint(f\"Tools: {len(branch.tools)} available\")\nprint(f\"Model: {branch.chat_model.model}\")\n</code></pre>"},{"location":"thinking-in-lionagi/branches-as-agents/#specialized-branches","title":"Specialized Branches","text":"<p>Design branches with focused roles and appropriate tools.</p> <pre><code>def web_search(query: str) -&gt; str:\n    \"\"\"Your search function\"\"\"\n    return f\"Search results for: {query}\"\n\n# Specialized branches\nresearcher = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"),\n    system=\"Research specialist. Gather comprehensive information.\",\n    tools=[web_search]\n)\n\nanalyst = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"),\n    system=\"Data analyst. Provide statistical insights.\"\n)\n\nwriter = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"),\n    system=\"Technical writer. Create structured reports.\"\n)\n\n# Each branch specializes in its domain\nresearch = await researcher.communicate(\"Research AI market trends\")\nanalysis = await analyst.communicate(\"Analyze the market data\")\nreport = await writer.communicate(f\"Write summary: {analysis}\")\n</code></pre>"},{"location":"thinking-in-lionagi/branches-as-agents/#multi-branch-coordination","title":"Multi-Branch Coordination","text":"<p>Coordinate multiple branches within a Session.</p> <pre><code>from lionagi import Session\n\n# Create session to manage branches\nsession = Session()\n\n# Create specialized branches\nsecurity = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"),\n    system=\"Security expert focused on identifying vulnerabilities\"\n)\n\nperformance = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"),\n    system=\"Performance expert focused on optimization\"\n)\n\n# Add branches to session\nsession.include_branches([security, performance])\n\n# Coordinate parallel analysis\ncode_sample = \"def login(user, pwd): return db.query(f'SELECT * FROM users WHERE name={user}')\"\n\nresults = await asyncio.gather(\n    security.communicate(f\"Review security: {code_sample}\"),\n    performance.communicate(f\"Review performance: {code_sample}\")\n)\n</code></pre>"},{"location":"thinking-in-lionagi/branches-as-agents/#memory-management","title":"Memory Management","text":"<p>Control conversation memory for long-running workflows.</p> <pre><code>branch = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\")\n)\n\n# Build up conversation\nfor i in range(5):\n    await branch.communicate(f\"Topic {i+1}: Brief analysis\")\n    print(f\"Messages: {len(branch.messages)}\")\n\n# Clear memory when context gets too long\nif len(branch.messages) &gt; 8:\n    branch.messages.clear()\n\n# Continue with fresh context\nresult = await branch.communicate(\"Summarize key findings\")\n</code></pre>"},{"location":"thinking-in-lionagi/branches-as-agents/#best-practices","title":"Best Practices","text":""},{"location":"thinking-in-lionagi/branches-as-agents/#1-design-clear-roles","title":"1. Design Clear Roles","text":"<pre><code># Good: Specific, focused role\nresearcher = Branch(\n    system=\"Research specialist. Gather and verify information from multiple sources.\"\n)\n\n# Avoid: Generic, unfocused role\ngeneric = Branch(\n    system=\"You are a helpful assistant.\"\n)\n</code></pre>"},{"location":"thinking-in-lionagi/branches-as-agents/#2-use-appropriate-tools","title":"2. Use Appropriate Tools","text":"<pre><code># Match tools to branch purpose\nanalyst = Branch(\n    system=\"Data analyst\",\n    tools=[calculate, analyze_data]  # Direct function passing\n)\n</code></pre>"},{"location":"thinking-in-lionagi/branches-as-agents/#3-manage-memory-wisely","title":"3. Manage Memory Wisely","text":"<pre><code># Clear memory for long workflows\nif len(branch.messages) &gt; 20:\n    branch.messages.clear()\n\n# Preserve important context\nimportant_context = branch.messages[-2:]  # Keep last exchange\nbranch.messages.clear()\nbranch.messages.extend(important_context)\n</code></pre>"},{"location":"thinking-in-lionagi/branches-as-agents/#4-coordinate-through-sessions","title":"4. Coordinate Through Sessions","text":"<pre><code># Use Session for multi-branch workflows\nsession = Session()\nsession.include_branches([branch1, branch2, branch3])\n</code></pre> <p>Branches provide stateful, specialized agents with persistent memory, custom tools, and clear behavioral roles for sophisticated multi-agent workflows.</p>"},{"location":"thinking-in-lionagi/builder-pattern/","title":"The Builder Pattern","text":"<p>Build complex workflows incrementally with dependencies and parallel execution.</p>"},{"location":"thinking-in-lionagi/builder-pattern/#simple-builder-example","title":"Simple Builder Example","text":"<p>Create operations and define their execution order.</p> <pre><code>from lionagi import Session, Builder, Branch, iModel\nimport asyncio\n\nasync def basic_builder():\n    \"\"\"Simple workflow with Builder pattern\"\"\"\n    session = Session()\n    builder = Builder(\"analysis\")\n\n    branch = Branch(\n        chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\")\n    )\n    session.include_branches([branch])\n\n    # Add operations\n    research = builder.add_operation(\n        \"communicate\",\n        branch=branch,\n        instruction=\"Research market trends\"\n    )\n\n    analysis = builder.add_operation(\n        \"communicate\", \n        branch=branch,\n        instruction=\"Analyze the research findings\",\n        depends_on=[research]  # Runs after research\n    )\n\n    # Execute workflow\n    result = await session.flow(builder.get_graph())\n    return result\n\nasyncio.run(basic_builder())\n</code></pre>"},{"location":"thinking-in-lionagi/builder-pattern/#parallel-operations","title":"Parallel Operations","text":"<p>Execute independent operations simultaneously.</p> <pre><code>async def parallel_builder():\n    \"\"\"Parallel operations with aggregation\"\"\"\n    session = Session()\n    builder = Builder(\"parallel_analysis\")\n\n    branch = Branch(\n        chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\")\n    )\n    session.include_branches([branch])\n\n    # Parallel operations (no dependencies)\n    market_op = builder.add_operation(\n        \"communicate\",\n        branch=branch,\n        instruction=\"Analyze market conditions\"\n    )\n\n    competitor_op = builder.add_operation(\n        \"communicate\",\n        branch=branch, \n        instruction=\"Research competitors\"\n    )\n\n    tech_op = builder.add_operation(\n        \"communicate\",\n        branch=branch,\n        instruction=\"Evaluate technology trends\"\n    )\n\n    # Combine results\n    synthesis = builder.add_aggregation(\n        \"communicate\",\n        branch=branch,\n        source_node_ids=[market_op, competitor_op, tech_op],\n        instruction=\"Synthesize all analyses\"\n    )\n\n    result = await session.flow(builder.get_graph(), max_concurrent=3)\n    return result\n\nasyncio.run(parallel_builder())\n</code></pre>"},{"location":"thinking-in-lionagi/builder-pattern/#multi-phase-workflows","title":"Multi-Phase Workflows","text":"<p>Combine sequential and parallel patterns.</p> <pre><code>async def multi_phase_builder():\n    \"\"\"Multi-phase workflow with mixed patterns\"\"\"\n    session = Session()\n    builder = Builder(\"complex_analysis\")\n\n    branch = Branch(\n        chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\")\n    )\n    session.include_branches([branch])\n\n    # Phase 1: Initial research\n    initial = builder.add_operation(\n        \"communicate\",\n        branch=branch,\n        instruction=\"Initial market research\"\n    )\n\n    # Phase 2: Parallel analysis (depends on Phase 1)\n    financial = builder.add_operation(\n        \"communicate\",\n        branch=branch,\n        instruction=\"Financial analysis\",\n        depends_on=[initial]\n    )\n\n    technical = builder.add_operation(\n        \"communicate\", \n        branch=branch,\n        instruction=\"Technical analysis\",\n        depends_on=[initial]\n    )\n\n    # Phase 3: Final synthesis (depends on Phase 2)\n    final = builder.add_aggregation(\n        \"communicate\",\n        branch=branch,\n        source_node_ids=[financial, technical],\n        instruction=\"Create final report\"\n    )\n\n    result = await session.flow(builder.get_graph())\n    return result\n\nasyncio.run(multi_phase_builder())\n</code></pre>"},{"location":"thinking-in-lionagi/builder-pattern/#multiple-branches","title":"Multiple Branches","text":"<p>Use specialized branches for different types of work.</p> <pre><code>async def multi_branch_builder():\n    \"\"\"Workflow with specialized branches\"\"\"\n    session = Session()\n    builder = Builder(\"specialized_workflow\")\n\n    # Specialized branches\n    researcher = Branch(\n        chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"),\n        system=\"Research specialist\"\n    )\n\n    analyst = Branch(\n        chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"),\n        system=\"Data analyst\"\n    )\n\n    writer = Branch(\n        chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"),\n        system=\"Report writer\"\n    )\n\n    session.include_branches([researcher, analyst, writer])\n\n    # Research phase\n    research = builder.add_operation(\n        \"communicate\",\n        branch=researcher,\n        instruction=\"Research AI market trends\"\n    )\n\n    # Analysis phase\n    analysis = builder.add_operation(\n        \"communicate\",\n        branch=analyst,\n        instruction=\"Analyze market data\",\n        depends_on=[research]\n    )\n\n    # Writing phase\n    report = builder.add_operation(\n        \"communicate\",\n        branch=writer,\n        instruction=\"Write executive summary\",\n        depends_on=[analysis]\n    )\n\n    result = await session.flow(builder.get_graph())\n    return result\n\nasyncio.run(multi_branch_builder())\n</code></pre>"},{"location":"thinking-in-lionagi/builder-pattern/#builder-vs-direct-execution","title":"Builder vs Direct Execution","text":"<p>When to use Builder vs direct calls.</p> <pre><code># Use Builder for: Complex workflows with dependencies\nasync def use_builder():\n    builder = Builder(\"workflow\")\n    # Multiple operations with dependencies\n    op1 = builder.add_operation(\"communicate\", branch=branch, instruction=\"Step 1\")\n    op2 = builder.add_operation(\"communicate\", branch=branch, instruction=\"Step 2\", depends_on=[op1])\n    return await session.flow(builder.get_graph())\n\n# Use direct calls for: Simple single operations\nasync def use_direct():\n    branch = Branch(chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"))\n    result = await branch.communicate(\"Simple analysis\")\n    return result\n\n# Use asyncio.gather for: Independent parallel operations\nasync def use_gather():\n    results = await asyncio.gather(\n        branch.communicate(\"Task 1\"),\n        branch.communicate(\"Task 2\"),\n        branch.communicate(\"Task 3\")\n    )\n    return results\n</code></pre>"},{"location":"thinking-in-lionagi/builder-pattern/#best-practices","title":"Best Practices","text":""},{"location":"thinking-in-lionagi/builder-pattern/#1-start-simple","title":"1. Start Simple","text":"<pre><code># Good: Clear linear flow\nresearch -&gt; analysis -&gt; report\n\n# Avoid: Over-complex initial design\nresearch -&gt; [analysis1, analysis2, analysis3] -&gt; synthesis -&gt; validation\n</code></pre>"},{"location":"thinking-in-lionagi/builder-pattern/#2-use-dependencies-wisely","title":"2. Use Dependencies Wisely","text":"<pre><code># Sequential: Each step depends on previous\nstep2 = builder.add_operation(..., depends_on=[step1])\nstep3 = builder.add_operation(..., depends_on=[step2])\n\n# Parallel: No dependencies, runs simultaneously\nop1 = builder.add_operation(...)  # No depends_on\nop2 = builder.add_operation(...)  # No depends_on\n</code></pre>"},{"location":"thinking-in-lionagi/builder-pattern/#3-aggregate-parallel-results","title":"3. Aggregate Parallel Results","text":"<pre><code># Combine parallel operations\nsynthesis = builder.add_aggregation(\n    \"communicate\",\n    source_node_ids=[op1, op2, op3],\n    instruction=\"Combine all results\"\n)\n</code></pre>"},{"location":"thinking-in-lionagi/builder-pattern/#4-control-concurrency","title":"4. Control Concurrency","text":"<pre><code># Limit parallel execution\nresult = await session.flow(\n    builder.get_graph(),\n    max_concurrent=3  # Only 3 operations at once\n)\n</code></pre> <p>The Builder pattern in LionAGI enables sophisticated workflows through incremental construction, dependency management, and controlled parallel execution.</p>"},{"location":"thinking-in-lionagi/graphs-over-chains/","title":"Graphs Over Chains","text":"<p>Why directed acyclic graphs beat sequential chains for agent orchestration.</p>"},{"location":"thinking-in-lionagi/graphs-over-chains/#the-limitation-of-chains","title":"The Limitation of Chains","text":"<p>Sequential chains force linear execution even when tasks could run in parallel.</p> <pre><code># Traditional chain approach (conceptual)\nasync def chain_workflow():\n    \"\"\"Sequential chain - inefficient for parallel tasks\"\"\"\n\n    # Each step waits for previous to complete\n    step1 = await research_task(\"market analysis\")     # 30 seconds\n    step2 = await research_task(\"competitor analysis\") # 30 seconds  \n    step3 = await research_task(\"trend analysis\")      # 30 seconds\n\n    # Total time: 90 seconds (sequential)\n    synthesis = await synthesize_results([step1, step2, step3])\n    return synthesis\n\n# These tasks could have run in parallel!\n</code></pre>"},{"location":"thinking-in-lionagi/graphs-over-chains/#graph-based-execution","title":"Graph-Based Execution","text":"<p>Graphs enable parallel execution with proper dependencies.</p> <pre><code>from lionagi import Session, Builder, Branch, iModel\nimport asyncio\n\nasync def graph_workflow():\n    \"\"\"Graph-based execution - parallel where possible\"\"\"\n    session = Session()\n    builder = Builder(\"parallel_research\")\n\n    branch = Branch(\n        chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\")\n    )\n    session.include_branches([branch])\n\n    # Independent parallel operations\n    market = builder.add_operation(\n        \"communicate\",\n        branch=branch,\n        instruction=\"Research market analysis\"\n    )\n\n    competitor = builder.add_operation(\n        \"communicate\",\n        branch=branch, \n        instruction=\"Research competitor analysis\"\n    )\n\n    trends = builder.add_operation(\n        \"communicate\",\n        branch=branch,\n        instruction=\"Research trend analysis\"  \n    )\n\n    # Synthesis depends on all three (proper dependency)\n    synthesis = builder.add_aggregation(\n        \"communicate\",\n        branch=branch,\n        source_node_ids=[market, competitor, trends],\n        instruction=\"Synthesize all research findings\"\n    )\n\n    # Execute with parallelism - total time: ~30 seconds\n    result = await session.flow(builder.get_graph(), max_concurrent=3)\n    return result\n\nasyncio.run(graph_workflow())\n</code></pre>"},{"location":"thinking-in-lionagi/graphs-over-chains/#complex-dependencies","title":"Complex Dependencies","text":"<p>Graphs handle complex dependency patterns naturally.</p> <pre><code>async def complex_graph():\n    \"\"\"Complex dependency graph\"\"\"\n    session = Session()\n    builder = Builder(\"complex_analysis\")\n\n    branch = Branch(\n        chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\")\n    )\n    session.include_branches([branch])\n\n    # Phase 1: Initial research (parallel)\n    market = builder.add_operation(\n        \"communicate\", branch=branch,\n        instruction=\"Market research\"\n    )\n\n    tech = builder.add_operation(\n        \"communicate\", branch=branch,\n        instruction=\"Technology research\"\n    )\n\n    # Phase 2: Analysis (depends on Phase 1)\n    market_analysis = builder.add_operation(\n        \"communicate\", branch=branch,\n        instruction=\"Analyze market data\",\n        depends_on=[market]  # Waits for market research\n    )\n\n    tech_analysis = builder.add_operation(\n        \"communicate\", branch=branch,\n        instruction=\"Analyze technology trends\", \n        depends_on=[tech]  # Waits for tech research\n    )\n\n    # Phase 3: Risk assessment (depends on both analyses)\n    risk = builder.add_operation(\n        \"communicate\", branch=branch,\n        instruction=\"Assess combined risks\",\n        depends_on=[market_analysis, tech_analysis]  # Waits for both\n    )\n\n    # Phase 4: Final strategy (depends on everything)\n    strategy = builder.add_aggregation(\n        \"communicate\", branch=branch,\n        source_node_ids=[market_analysis, tech_analysis, risk],\n        instruction=\"Create final strategy\"\n    )\n\n    result = await session.flow(builder.get_graph())\n    return result\n\nasyncio.run(complex_graph())\n</code></pre>"},{"location":"thinking-in-lionagi/graphs-over-chains/#conditional-graph-paths","title":"Conditional Graph Paths","text":"<p>Graphs can represent conditional execution paths.</p> <pre><code>async def conditional_graph():\n    \"\"\"Graph with conditional branches\"\"\"\n    session = Session()\n    builder = Builder(\"conditional_workflow\")\n\n    branch = Branch(\n        chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\")\n    )\n    session.include_branches([branch])\n\n    # Initial assessment\n    assessment = builder.add_operation(\n        \"communicate\", branch=branch,\n        instruction=\"Assess project complexity (simple/complex)\"\n    )\n\n    # Simple path\n    simple_plan = builder.add_operation(\n        \"communicate\", branch=branch,\n        instruction=\"Create simple implementation plan\",\n        depends_on=[assessment]\n    )\n\n    # Complex path  \n    detailed_research = builder.add_operation(\n        \"communicate\", branch=branch,\n        instruction=\"Detailed technical research\",\n        depends_on=[assessment]\n    )\n\n    complex_plan = builder.add_operation(\n        \"communicate\", branch=branch,\n        instruction=\"Create complex implementation plan\",\n        depends_on=[detailed_research]\n    )\n\n    # Both paths can execute - actual execution depends on conditions\n    result = await session.flow(builder.get_graph())\n    return result\n\nasyncio.run(conditional_graph())\n</code></pre>"},{"location":"thinking-in-lionagi/graphs-over-chains/#fan-outfan-in-pattern","title":"Fan-Out/Fan-In Pattern","text":"<p>Common graph pattern for parallel processing and aggregation.</p> <pre><code>async def fan_out_fan_in():\n    \"\"\"Fan-out to parallel processing, fan-in to aggregation\"\"\"\n    session = Session()\n    builder = Builder(\"fan_pattern\")\n\n    branch = Branch(\n        chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\")\n    )\n    session.include_branches([branch])\n\n    # Fan-out: Single input spawns multiple parallel tasks\n    initial = builder.add_operation(\n        \"communicate\", branch=branch,\n        instruction=\"Define research scope\"\n    )\n\n    # Parallel analysis tasks (fan-out)\n    analyses = []\n    topics = [\"market\", \"technical\", \"financial\", \"legal\", \"competitive\"]\n\n    for topic in topics:\n        analysis = builder.add_operation(\n            \"communicate\", branch=branch,\n            instruction=f\"Analyze {topic} aspects\",\n            depends_on=[initial]  # All depend on scope definition\n        )\n        analyses.append(analysis)\n\n    # Fan-in: Aggregate all parallel results\n    final_report = builder.add_aggregation(\n        \"communicate\", branch=branch,\n        source_node_ids=analyses,  # Collect all analyses\n        instruction=\"Create comprehensive final report\"\n    )\n\n    result = await session.flow(builder.get_graph(), max_concurrent=5)\n    return result\n\nasyncio.run(fan_out_fan_in())\n</code></pre>"},{"location":"thinking-in-lionagi/graphs-over-chains/#graph-vs-chain-comparison","title":"Graph vs Chain Comparison","text":"<p>When to use graphs vs direct execution.</p> <pre><code># Use graphs for: Complex workflows with dependencies\nasync def use_graphs():\n    # Multiple phases with mixed parallel/sequential execution\n    # Dependencies between operations\n    # Need for aggregation or synthesis\n    builder = Builder(\"complex\")\n    # ... build graph with dependencies\n    return await session.flow(builder.get_graph())\n\n# Use direct execution for: Simple single operations  \nasync def use_direct():\n    branch = Branch(chat_model=iModel(provider=\"openai\", model=\"gpt-4o-mini\"))\n    return await branch.communicate(\"Simple task\")\n\n# Use asyncio.gather for: Independent parallel operations\nasync def use_gather():\n    return await asyncio.gather(\n        branch.communicate(\"Task 1\"),\n        branch.communicate(\"Task 2\"), \n        branch.communicate(\"Task 3\")\n    )\n</code></pre>"},{"location":"thinking-in-lionagi/graphs-over-chains/#best-practices","title":"Best Practices","text":""},{"location":"thinking-in-lionagi/graphs-over-chains/#1-design-for-parallelism","title":"1. Design for Parallelism","text":"<pre><code># Good: Parallel where possible\nmarket_op = builder.add_operation(...)    # No depends_on\ntech_op = builder.add_operation(...)      # No depends_on  \nsynthesis = builder.add_aggregation(..., source_node_ids=[market_op, tech_op])\n\n# Avoid: Unnecessary sequential dependencies\nstep2 = builder.add_operation(..., depends_on=[step1])  # Only if truly needed\n</code></pre>"},{"location":"thinking-in-lionagi/graphs-over-chains/#2-use-aggregation-for-synthesis","title":"2. Use Aggregation for Synthesis","text":"<pre><code># Combine multiple parallel results\nsynthesis = builder.add_aggregation(\n    \"communicate\",\n    source_node_ids=[op1, op2, op3],\n    instruction=\"Synthesize all findings\"\n)\n</code></pre>"},{"location":"thinking-in-lionagi/graphs-over-chains/#3-control-concurrency","title":"3. Control Concurrency","text":"<pre><code># Limit parallel execution to avoid overwhelming APIs\nresult = await session.flow(\n    builder.get_graph(),\n    max_concurrent=3  # Reasonable limit\n)\n</code></pre>"},{"location":"thinking-in-lionagi/graphs-over-chains/#4-keep-dependencies-simple","title":"4. Keep Dependencies Simple","text":"<pre><code># Good: Clear, necessary dependencies\nanalysis = builder.add_operation(..., depends_on=[research])\n\n# Avoid: Complex circular or unnecessary dependencies\n# operation = builder.add_operation(..., depends_on=[many, complex, deps])\n</code></pre> <p>Graphs in LionAGI enable sophisticated execution patterns through parallel processing, proper dependency management, and flexible workflow topologies that sequential chains cannot achieve.</p>"},{"location":"thinking-in-lionagi/why-lionagi/","title":"Why LionAGI?","text":""},{"location":"thinking-in-lionagi/why-lionagi/#the-fundamental-shift","title":"The Fundamental Shift","text":"<p>LionAGI is evolving into an orchestration engine - you bring the operations, we provide the coordination machinery.</p>"},{"location":"thinking-in-lionagi/why-lionagi/#the-core-difference","title":"The Core Difference","text":"<p>While others provide operations, LionAGI provides orchestration. We treat coordination as a graph problem, not a conversation problem.</p>"},{"location":"thinking-in-lionagi/why-lionagi/#what-this-means-in-practice","title":"What This Means in Practice","text":""},{"location":"thinking-in-lionagi/why-lionagi/#other-frameworks-sequential-chains","title":"Other Frameworks: Sequential Chains","text":"<pre><code># LangChain approach\nchain = Agent1 &gt;&gt; Agent2 &gt;&gt; Agent3  # Always sequential\nresult = chain.run(input)  # If Agent2 fails, everything stops\n</code></pre>"},{"location":"thinking-in-lionagi/why-lionagi/#lionagi-parallel-graphs","title":"LionAGI: Parallel Graphs","text":"<pre><code># LionAGI approach\nbuilder = Builder(\"analysis\")\nop1 = builder.add_operation(\"chat\", branch=agent1, instruction=task)\nop2 = builder.add_operation(\"chat\", branch=agent2, instruction=task)\nop3 = builder.add_operation(\"chat\", branch=agent3, instruction=task)\nsynthesis = builder.add_operation(\"chat\", depends_on=[op1, op2, op3])\n\nresult = await session.flow(builder.get_graph())  # Parallel execution\n</code></pre>"},{"location":"thinking-in-lionagi/why-lionagi/#key-technical-advantages","title":"Key Technical Advantages","text":""},{"location":"thinking-in-lionagi/why-lionagi/#1-true-parallel-execution","title":"1. True Parallel Execution","text":"<pre><code># This actually runs in parallel\nresults = await asyncio.gather(\n    agent1.chat(\"Analyze from perspective A\"),\n    agent2.chat(\"Analyze from perspective B\"),\n    agent3.chat(\"Analyze from perspective C\")\n)\n</code></pre>"},{"location":"thinking-in-lionagi/why-lionagi/#2-dependency-management","title":"2. Dependency Management","text":"<pre><code># Define complex dependencies easily\nresearch = builder.add_operation(\"chat\", instruction=\"Research the topic\")\nanalysis = builder.add_operation(\"chat\", depends_on=[research])\nreview = builder.add_operation(\"chat\", depends_on=[analysis])\nfinal = builder.add_operation(\"chat\", depends_on=[research, analysis, review])\n</code></pre>"},{"location":"thinking-in-lionagi/why-lionagi/#3-isolated-agent-state","title":"3. Isolated Agent State","text":"<p>Each Branch maintains its own:</p> <ul> <li>Conversation history</li> <li>System prompt</li> <li>Tools</li> <li>Model configuration</li> </ul> <pre><code># Each agent has independent memory\nawait agent1.chat(\"Remember X\")\nawait agent2.chat(\"Remember Y\")\n# agent1 doesn't know about Y, agent2 doesn't know about X\n</code></pre>"},{"location":"thinking-in-lionagi/why-lionagi/#4-flexible-orchestration","title":"4. Flexible Orchestration","text":"<pre><code># Same agents, different workflows\nfor strategy in [\"parallel\", \"sequential\", \"tournament\"]:\n    result = await run_analysis(agents, strategy)\n</code></pre>"},{"location":"thinking-in-lionagi/why-lionagi/#real-performance-differences","title":"Real Performance Differences","text":""},{"location":"thinking-in-lionagi/why-lionagi/#task-analyze-50-documents","title":"Task: Analyze 50 documents","text":"<p>Sequential Approach (most frameworks):</p> <ul> <li>Time: 50 \u00d7 3 seconds = 150 seconds</li> <li>Failure handling: Stops on first error</li> </ul> <p>LionAGI Parallel Approach:</p> <ul> <li>Time: 3 seconds (all 50 in parallel)</li> <li>Failure handling: Continues with successful docs</li> </ul>"},{"location":"thinking-in-lionagi/why-lionagi/#task-multi-perspective-analysis","title":"Task: Multi-perspective analysis","text":"<p>Conversation-based (AutoGen, CrewAI):</p> <pre><code># Agents talk to each other, hard to control\nagent1: \"I think...\"\nagent2: \"But consider...\"\nagent3: \"Actually...\"\n# Can spiral out of control\n</code></pre> <p>Graph-based (LionAGI):</p> <pre><code># Clear, predictable execution\nperspectives = await gather(agent1.analyze(), agent2.analyze(), agent3.analyze())\nsynthesis = await synthesizer.combine(perspectives)\n</code></pre>"},{"location":"thinking-in-lionagi/why-lionagi/#when-lionagi-makes-sense","title":"When LionAGI Makes Sense","text":"<p>Good Fit For</p> <ul> <li>Multiple perspectives: Research, analysis, code review from different angles</li> <li>Parallel processing: Speed up workflows with concurrent execution  </li> <li>Predictable workflows: Deterministic graphs instead of unpredictable conversations</li> <li>Production systems: Built-in monitoring, error handling, and performance control</li> </ul> <p>Not Ideal For</p> <ul> <li>Simple single-agent tasks: Use a basic chatbot library instead</li> <li>Pure chatbot applications: LionAGI is overkill for simple Q&amp;A</li> <li>Experimental conversational AI: Other frameworks may be more flexible for research</li> </ul>"},{"location":"thinking-in-lionagi/why-lionagi/#migration-example","title":"Migration Example","text":""},{"location":"thinking-in-lionagi/why-lionagi/#from-sequential-to-parallel","title":"From Sequential to Parallel","text":"<p>Before (sequential):</p> <pre><code>def analyze_sequential(doc):\n    extracted = extractor.process(doc)     # 2 sec\n    analyzed = analyzer.process(extracted)  # 2 sec\n    summary = summarizer.process(analyzed)  # 2 sec\n    return summary  # Total: 6 seconds\n</code></pre> <p>After (parallel where possible):</p> <pre><code>async def analyze_parallel(doc):\n    # Extraction must happen first\n    extracted = await extractor.process(doc)  # 2 sec\n\n    # Analysis and summary can happen in parallel\n    analyzed, summary = await gather(\n        analyzer.process(extracted),\n        summarizer.process(extracted)\n    )  # 2 sec (parallel)\n\n    return {\"analysis\": analyzed, \"summary\": summary}  # Total: 4 seconds\n</code></pre>"},{"location":"thinking-in-lionagi/why-lionagi/#the-bottom-line","title":"The Bottom Line","text":"<p>LionAGI is built for developers who need:</p> <ul> <li>Predictable multi-agent workflows</li> <li>Parallel execution for performance</li> <li>Clear separation of agent concerns</li> <li>Production-ready orchestration</li> </ul> <p>If you're building toy examples or chatbots, other frameworks might be simpler. If you're building systems that need to work reliably at scale, LionAGI provides the right abstractions.</p>"}]}