{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"LionAGI","text":"<p>Provider-agnostic LLM orchestration for structured, multi-step AI workflows.</p> <p>LionAGI gives you typed, observable control over LLM calls. Instead of dumping a prompt into a black-box chain, you build explicit workflows from <code>Branch</code> (a single conversation thread), <code>iModel</code> (any provider behind a uniform interface), and <code>Session</code> (multi-branch orchestration with graph-based execution). Every call is async, every response can be parsed into Pydantic models, and every tool invocation is logged.</p>"},{"location":"#why-lionagi","title":"Why LionAGI","text":"<ul> <li>One interface, any provider. Switch between OpenAI, Anthropic, Gemini, Ollama, NVIDIA NIM, Groq, Perplexity, and OpenRouter without changing your workflow code. Bring your own OpenAI-compatible endpoint if none of these fit.</li> <li>Structured output by default. <code>communicate()</code> and <code>operate()</code> parse LLM responses directly into Pydantic models. No regex, no fragile string extraction.</li> <li>Tool calling built in. Register Python functions as tools, and <code>operate()</code> handles schema generation, LLM function-call requests, invocation, and result injection automatically.</li> <li>Graph-based workflows. <code>Builder</code> composes operations into a DAG. <code>Session.flow()</code> executes it across branches with dependency tracking and parallel execution.</li> </ul>"},{"location":"#installation","title":"Installation","text":"pipuv <pre><code>pip install lionagi\n</code></pre> <pre><code>uv add lionagi\n</code></pre> <p>Set your API key (at minimum, one provider):</p> <pre><code>export OPENAI_API_KEY=sk-...\n</code></pre> <p>LionAGI defaults to <code>openai</code> with <code>gpt-4.1-mini</code>. See Installation for all providers and optional extras.</p>"},{"location":"#hello-world","title":"Hello World","text":"<pre><code>import asyncio\nfrom lionagi import Branch\n\nasync def main():\n    branch = Branch(system=\"You are a helpful assistant.\")\n    result = await branch.communicate(\"What is the capital of France?\")\n    print(result)\n\nasyncio.run(main())\n</code></pre> <p><code>communicate()</code> sends a message, adds both the instruction and the response to the conversation history, and returns the response as a string. No iModel needed for defaults -- Branch auto-creates one from your <code>OPENAI_API_KEY</code>.</p>"},{"location":"#what-makes-it-different","title":"What Makes It Different","text":""},{"location":"#multi-provider-same-code","title":"Multi-provider, same code","text":"<pre><code>from lionagi import Branch, iModel\n\nopenai_branch = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4.1-mini\")\n)\n\nanthropic_branch = Branch(\n    chat_model=iModel(provider=\"anthropic\", model=\"claude-sonnet-4-20250514\")\n)\n\ngemini_branch = Branch(\n    chat_model=iModel(provider=\"gemini\", model=\"gemini-2.5-flash\")\n)\n</code></pre>"},{"location":"#structured-output-with-response_format","title":"Structured output with <code>response_format</code>","text":"<pre><code>from pydantic import BaseModel\nfrom lionagi import Branch\n\nclass City(BaseModel):\n    name: str\n    country: str\n    population: int\n\nasync def main():\n    branch = Branch()\n    city = await branch.communicate(\n        \"Tell me about Tokyo.\",\n        response_format=City,\n    )\n    print(city.name, city.population)  # Typed Pydantic model\n</code></pre>"},{"location":"#tool-calling-with-operate","title":"Tool calling with <code>operate()</code>","text":"<pre><code>from lionagi import Branch\n\ndef get_weather(city: str, unit: str = \"celsius\") -&gt; str:\n    \"\"\"Get current weather for a city.\"\"\"\n    return f\"22 degrees {unit} in {city}, partly cloudy\"\n\nasync def main():\n    branch = Branch(tools=[get_weather])\n    result = await branch.operate(\n        instruction=\"What's the weather in Tokyo?\",\n        actions=True,\n    )\n    print(result)  # LLM called get_weather, got the result, and responded\n</code></pre>"},{"location":"#graph-workflows-with-session-and-builder","title":"Graph workflows with <code>Session</code> and <code>Builder</code>","text":"<pre><code>from lionagi import Session, Builder\n\nsession = Session()\nbuilder = Builder()\n\n# Define a two-step workflow\nstep1 = builder.add_operation(\n    \"communicate\",\n    instruction=\"List 3 startup ideas in AI.\",\n)\nstep2 = builder.add_operation(\n    \"communicate\",\n    instruction=\"Pick the most feasible idea and explain why.\",\n    depends_on=[step1],\n)\n\nasync def main():\n    results = await session.flow(builder.get_graph())\n    print(results)\n</code></pre>"},{"location":"#supported-providers","title":"Supported Providers","text":"Provider <code>provider=</code> Environment Variable Example Model OpenAI <code>\"openai\"</code> <code>OPENAI_API_KEY</code> <code>gpt-4.1-mini</code> Anthropic <code>\"anthropic\"</code> <code>ANTHROPIC_API_KEY</code> <code>claude-sonnet-4-20250514</code> Google Gemini <code>\"gemini\"</code> <code>GEMINI_API_KEY</code> <code>gemini-2.5-flash</code> Ollama <code>\"ollama\"</code> (none, local) <code>llama3.2</code> NVIDIA NIM <code>\"nvidia_nim\"</code> <code>NVIDIA_NIM_API_KEY</code> <code>meta/llama-3.1-70b-instruct</code> Groq <code>\"groq\"</code> <code>GROQ_API_KEY</code> <code>llama-3.3-70b-versatile</code> Perplexity <code>\"perplexity\"</code> <code>PERPLEXITY_API_KEY</code> <code>sonar-pro</code> OpenRouter <code>\"openrouter\"</code> <code>OPENROUTER_API_KEY</code> <code>anthropic/claude-sonnet-4</code> OpenAI-compatible (any string) (pass <code>api_key=</code> directly) (your model) <pre><code># OpenAI-compatible custom endpoint\ncustom = iModel(\n    provider=\"my_provider\",\n    model=\"my-model\",\n    api_key=\"your-key\",\n    base_url=\"https://your-endpoint.com/v1\",\n)\n</code></pre>"},{"location":"#learning-path","title":"Learning Path","text":"Step Topic Link 1 Install and verify Installation 2 First working examples Quick Start 3 Core abstractions Core Concepts 4 Operations in depth Operations 5 Branch and Session Sessions &amp; Branches 6 Workflow patterns Patterns 7 Provider details LLM Providers 8 Advanced topics Advanced 9 API reference Reference"},{"location":"#key-concepts-at-a-glance","title":"Key Concepts at a Glance","text":"<p>Branch -- A single conversation thread. Manages messages, tools, and model instances. All LLM operations (<code>chat</code>, <code>communicate</code>, <code>operate</code>, <code>parse</code>, <code>ReAct</code>) are Branch methods.</p> <p>iModel -- Wraps any LLM provider behind a uniform async interface. Handles rate limiting, retries, and request/response translation.</p> <p>Session -- Manages multiple Branches. Executes graph-based workflows via <code>session.flow()</code>.</p> <p>Builder -- Constructs operation graphs (DAGs) for <code>Session.flow()</code>. Supports sequential dependencies, parallel fan-out, and dynamic expansion from results.</p>"},{"location":"#requirements","title":"Requirements","text":"<ul> <li>Python &gt;= 3.10</li> <li>At least one LLM provider API key (or Ollama running locally)</li> </ul> <p>Apache 2.0 License | GitHub | Discord</p>"},{"location":"DOCUMENTATION_STANDARDS/","title":"LionAGI Documentation Standards","text":"<p>Practical guidelines for writing docs that actually help.</p>"},{"location":"DOCUMENTATION_STANDARDS/#core-principle","title":"Core Principle","text":"<p>Show, don't tell. Every page should have working code that solves a real problem.</p>"},{"location":"DOCUMENTATION_STANDARDS/#page-types-templates","title":"Page Types &amp; Templates","text":""},{"location":"DOCUMENTATION_STANDARDS/#1-pattern-pages-patterns","title":"1. Pattern Pages (<code>/patterns/</code>)","text":"<pre><code># [Pattern Name]\n\nWhen you need to [problem this solves].\n\n## The Pattern\n\n```python\n# Complete working example\nfrom lionagi import Session, Branch, Builder\n\nasync def pattern_name():\n    # Implementation\n    pass\n\n# Usage\nresult = await pattern_name()\n```\n</code></pre>"},{"location":"DOCUMENTATION_STANDARDS/#when-it-works","title":"When It Works","text":"<ul> <li>Scenario 1: [specific use case]</li> <li>Scenario 2: [another use case]</li> </ul>"},{"location":"DOCUMENTATION_STANDARDS/#success-rate","title":"Success Rate","text":"<p>~95% based on [context]</p> <pre><code>### 2. Cookbook Pages (`/cookbook/`)\n\n```markdown\n# [Solution Name]\n\n[One sentence: what this builds]\n\n## Problem\n\n[2-3 sentences on the specific challenge]\n\n## Solution\n\n```python\n# Full implementation\n# Can be 50-200 lines\n# Must be copy-paste ready\n</code></pre>"},{"location":"DOCUMENTATION_STANDARDS/#results","title":"Results","text":"<pre><code>[Actual output from running the code]\n</code></pre>"},{"location":"DOCUMENTATION_STANDARDS/#customization","title":"Customization","text":"<ul> <li>To adapt for X: [change this]</li> <li>To scale up: [modify that]</li> </ul> <pre><code>### 3. Concept Pages (`/core-concepts/`)\n\n```markdown\n# [Concept Name]\n\n[One sentence definition]\n\n## Quick Example\n\n```python\n# Minimal example showing the concept\n</code></pre>"},{"location":"DOCUMENTATION_STANDARDS/#key-points","title":"Key Points","text":"<ul> <li>Point 1: [essential info]</li> <li>Point 2: [essential info]</li> <li>Point 3: [essential info]</li> </ul>"},{"location":"DOCUMENTATION_STANDARDS/#common-usage","title":"Common Usage","text":"<pre><code># Realistic example\n</code></pre> <pre><code>### 4. Quickstart Pages\n\n```markdown\n# [Getting Started with X]\n\n## Install\n\n```bash\nuv add lionagi\n</code></pre>"},{"location":"DOCUMENTATION_STANDARDS/#first-example","title":"First Example","text":"<pre><code># Simplest possible working example\n</code></pre>"},{"location":"DOCUMENTATION_STANDARDS/#next-steps","title":"Next Steps","text":"<ul> <li>Try [pattern]</li> <li>Read about [concept]</li> <li>See [cookbook example]</li> </ul> <pre><code>## Code Standards\n\n### Every Code Block Must:\n\n1. **Run without modification** - Include all imports\n2. **Show realistic usage** - Not just toy examples\n3. **Handle errors gracefully** - At least try/except where it matters\n\n```python\n# GOOD: Complete and runnable\nfrom lionagi import Branch, iModel\nimport asyncio\n\nasync def example():\n    branch = Branch(chat_model=iModel(provider=\"openai\", model=\"gpt-4.1-mini\"))\n    try:\n        result = await branch.chat(\"Analyze this\")\n        return result\n    except Exception as e:\n        print(f\"Error: {e}\")\n        return None\n\n# Run it\n# result = asyncio.run(example())\n</code></pre> <pre><code># BAD: Fragment without context\nbranch.chat(\"Analyze this\")  # What's branch? How to run?\n</code></pre>"},{"location":"DOCUMENTATION_STANDARDS/#writing-style","title":"Writing Style","text":""},{"location":"DOCUMENTATION_STANDARDS/#keep-it-simple","title":"Keep It Simple","text":"<ul> <li>Short sentences (max 20 words)</li> <li>Active voice (\"Use X to...\" not \"X can be used to...\")</li> <li>Direct instructions (\"Do this\" not \"You might want to consider\")</li> <li>Skip the fluff (No \"In this section we will explore...\")</li> </ul>"},{"location":"DOCUMENTATION_STANDARDS/#show-success-metrics","title":"Show Success Metrics","text":"<p>When claiming something works, show evidence:</p> <ul> <li>\"95% success rate\" not \"usually works\"</li> <li>\"2.3 second average\" not \"fast\"</li> <li>\"Handles 1000 req/sec\" not \"scalable\"</li> </ul>"},{"location":"DOCUMENTATION_STANDARDS/#for-ai-agents","title":"For AI Agents","text":""},{"location":"DOCUMENTATION_STANDARDS/#pattern-recognition-format","title":"Pattern Recognition Format","text":"<p>Help AI agents understand when to use patterns:</p> <pre><code>## When to Use\n\nIF task requires parallel analysis: USE fan-out-in pattern ELIF task needs\nstep-by-step building: USE sequential-analysis pattern ELSE: USE single-branch\nReAct\n</code></pre>"},{"location":"DOCUMENTATION_STANDARDS/#executable-templates","title":"Executable Templates","text":"<p>Provide parameterized code AI can modify:</p> <pre><code>async def orchestrate(roles: list[str], task: str):\n    \"\"\"Template AI agents can adapt.\"\"\"\n    branches = [Branch(system=f\"You are a {role}\") for role in roles]\n    results = await asyncio.gather(*[b.chat(task) for b in branches])\n    return synthesize(results)\n</code></pre>"},{"location":"DOCUMENTATION_STANDARDS/#documentation-workflow","title":"Documentation Workflow","text":""},{"location":"DOCUMENTATION_STANDARDS/#adding-new-docs","title":"Adding New Docs","text":"<ol> <li>Check if needed - Does this solve a new problem?</li> <li>Pick the right type - Pattern, cookbook, concept, or quickstart?</li> <li>Use the template - Don't reinvent the structure</li> <li>Test the code - Every example must run</li> <li>Get it merged - Perfect is the enemy of done</li> </ol>"},{"location":"DOCUMENTATION_STANDARDS/#updating-docs","title":"Updating Docs","text":"<ul> <li>Fix errors immediately - Don't wait</li> <li>Update metrics quarterly - Keep data fresh</li> <li>Add examples from issues - Real problems, real solutions</li> </ul>"},{"location":"DOCUMENTATION_STANDARDS/#quality-checklist","title":"Quality Checklist","text":"<p>Before merging any doc:</p> <ul> <li> Code runs without errors</li> <li> Solves a real problem</li> <li> Uses appropriate template</li> <li> Includes actual output/metrics</li> <li> Links to related content</li> </ul>"},{"location":"DOCUMENTATION_STANDARDS/#what-not-to-document","title":"What NOT to Document","text":"<ul> <li>Obvious things - We have good docstrings</li> <li>Every parameter - API reference handles that</li> <li>Theory without practice - This isn't an academic paper</li> <li>Features not in main - Document what's shipped</li> </ul>"},{"location":"DOCUMENTATION_STANDARDS/#examples-of-good-docs","title":"Examples of Good Docs","text":""},{"location":"DOCUMENTATION_STANDARDS/#good-pattern-doc","title":"Good Pattern Doc","text":"<ul> <li>Clear problem statement</li> <li>Complete working code</li> <li>Success metrics</li> <li>When to use/not use</li> </ul>"},{"location":"DOCUMENTATION_STANDARDS/#good-cookbook-entry","title":"Good Cookbook Entry","text":"<ul> <li>Specific real-world scenario</li> <li>Full implementation</li> <li>Actual results</li> <li>How to customize</li> </ul>"},{"location":"DOCUMENTATION_STANDARDS/#good-concept-page","title":"Good Concept Page","text":"<ul> <li>Simple definition</li> <li>Minimal example</li> <li>Key points only</li> <li>Practical usage</li> </ul>"},{"location":"DOCUMENTATION_STANDARDS/#maintenance","title":"Maintenance","text":""},{"location":"DOCUMENTATION_STANDARDS/#quarterly-review","title":"Quarterly Review","text":"<ul> <li>Update success metrics</li> <li>Fix broken examples</li> <li>Remove outdated patterns</li> <li>Add new proven patterns</li> </ul>"},{"location":"DOCUMENTATION_STANDARDS/#continuous","title":"Continuous","text":"<ul> <li>Fix errors when found</li> <li>Add clarifications from support questions</li> <li>Update for API changes</li> </ul> <p>Remember: If you wouldn't copy-paste it into your own project, don't put it in the docs.</p>"},{"location":"code-of-conduct/","title":"Code of Conduct","text":""},{"location":"code-of-conduct/#our-pledge","title":"Our Pledge","text":"<p>We are committed to providing a welcoming and inclusive environment for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, nationality, personal appearance, race, religion, or sexual identity and orientation.</p>"},{"location":"code-of-conduct/#our-standards","title":"Our Standards","text":"<p>Positive behaviors include:</p> <ul> <li>Using welcoming and inclusive language</li> <li>Being respectful of differing viewpoints and experiences</li> <li>Gracefully accepting constructive criticism</li> <li>Focusing on what is best for the community</li> <li>Showing empathy towards other community members</li> </ul> <p>Unacceptable behaviors include:</p> <ul> <li>Harassment of any kind</li> <li>Discriminatory language or actions</li> <li>Personal attacks or insults</li> <li>Trolling, spamming, or disruptive behavior</li> <li>Publishing others' private information without permission</li> </ul>"},{"location":"code-of-conduct/#our-responsibilities","title":"Our Responsibilities","text":"<p>Community leaders are responsible for clarifying standards of acceptable behavior and will take appropriate action in response to unacceptable behavior.</p>"},{"location":"code-of-conduct/#scope","title":"Scope","text":"<p>This Code of Conduct applies to all community spaces, including:</p> <ul> <li>GitHub repositories and issues</li> <li>Discord channels</li> <li>Community forums</li> <li>Social media interactions</li> <li>In-person events</li> </ul>"},{"location":"code-of-conduct/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement. All complaints will be reviewed and investigated promptly and fairly.</p> <p>Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned with this Code of Conduct.</p>"},{"location":"code-of-conduct/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 2.1.</p> <p>For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq.</p>"},{"location":"contributing/","title":"Contributing to LionAGI","text":"<p>Thank you for your interest in contributing to LionAGI!</p>"},{"location":"contributing/#ways-to-contribute","title":"Ways to Contribute","text":"<ul> <li>Bug Reports: Found an issue? Create a GitHub issue</li> <li>Feature Requests: Have ideas for improvements? Let us know</li> <li>Documentation: Help improve our docs and examples</li> <li>Code: Submit pull requests for bug fixes and features</li> <li>Community: Help others in discussions and forums</li> </ul>"},{"location":"contributing/#development-setup","title":"Development Setup","text":"<ol> <li>Fork the repository</li> <li>Clone your fork</li> <li>Install dependencies: <code>uv sync</code></li> <li>Create a branch: <code>git checkout -b feature/your-feature</code></li> <li>Make changes and add tests</li> <li>Run tests: <code>uv run pytest</code></li> <li>Submit a pull request</li> </ol>"},{"location":"contributing/#code-standards","title":"Code Standards","text":"<ul> <li>Follow Python PEP 8 style guidelines</li> <li>Add type hints to all functions</li> <li>Include docstrings for public APIs</li> <li>Write tests for new functionality</li> <li>Keep pull requests focused and atomic</li> </ul>"},{"location":"contributing/#documentation","title":"Documentation","text":"<ul> <li>Update docs when adding features</li> <li>Include examples in docstrings</li> <li>Test code examples to ensure they work</li> <li>Follow our documentation standards</li> </ul>"},{"location":"contributing/#community-guidelines","title":"Community Guidelines","text":"<ul> <li>Be respectful and inclusive</li> <li>Help newcomers learn</li> <li>Focus on constructive feedback</li> <li>Follow our Code of Conduct</li> </ul>"},{"location":"contributing/#questions","title":"Questions?","text":"<ul> <li>GitHub Discussions for questions</li> <li>GitHub Issues for bugs and features</li> <li>Discord community for real-time chat</li> </ul> <p>We appreciate all contributions, no matter how small!</p>"},{"location":"problem-statement/","title":"The LionAGI Problem Statement","text":""},{"location":"problem-statement/#ai-reasoning-is-a-black-box-but-ai-workflows-dont-have-to-be","title":"AI reasoning is a black box, but AI workflows don't have to be.","text":"<p>Everyone's racing to build agents, but we're solving the wrong problem. The industry is obsessed with making models \"explain their reasoning\" - but those reasoning traces are just generated text, not actual thought processes. They're probabilistic outputs, not deterministic explanations.</p> <p>Meanwhile, enterprises need AI automation but can't get it past security audits. They need to trust AI systems with critical decisions, but they can't see or verify what's happening inside.</p>"},{"location":"problem-statement/#the-reality-check","title":"The Reality Check","text":"<ol> <li>No single model will achieve AGI - Complex intelligence requires multiple specialized models working together</li> <li>\"Reasoning\" models are theater - Those traces don't show actual thinking, just prolonged inference time</li> <li>The biggest model isn't the best model - Different tasks need different capabilities at different costs</li> <li>Agent demos \u2260 Production systems - What works in demos breaks when facing real complexity</li> </ol>"},{"location":"problem-statement/#the-lionagi-insight","title":"The LionAGI Insight","text":"<p>\"Even if we can't explain LLM reasoning, the workflow itself is explainable.\"</p> <p>Trust doesn't come from models explaining themselves (they can't). Trust comes from observing structured workflows where you can: - See every decision point - Verify every data transformation - Audit every agent interaction - Reproduce every outcome</p>"},{"location":"problem-statement/#what-everyone-wants-vs-what-they-need","title":"What Everyone Wants vs What They Need","text":"<p>Want: AI automation that handles complex tasks Need: AI systems they can trust in production</p> <p>Current \"solutions\": - LangChain/LlamaIndex: Kitchen sink frameworks with everything but clarity - AutoGen/CrewAI: Agents chatting in unpredictable conversations - \"Reasoning\" models: Self-reported thinking that's just more generated text</p> <p>What's missing: Observable, deterministic, production-ready orchestration</p>"},{"location":"problem-statement/#the-market-timing","title":"The Market Timing","text":"<ul> <li>2022-2023: \"What's an agent?\" (too early)</li> <li>2024-2025: \"Agents are cool but how do we trust them in production?\" (perfect timing)</li> <li>Enterprise pain: Need AI automation but can't pass security audits with current solutions</li> </ul>"},{"location":"problem-statement/#lionagis-answer","title":"LionAGI's Answer","text":"<p>Instead of trying to make AI explain itself (impossible), make AI workflows observable (achievable).</p> <p>Simple patterns that work: - Parallel specialists with different perspectives - Mandatory critics to catch errors - Artifact coordination for transparency - Cognitive limits that prevent chaos</p> <p>Not \"complexity theater\": - No Byzantine fault tolerance when you don't need it - No category theory abstractions for their own sake - No formal verification overkill - Just observable, reliable workflows</p>"},{"location":"problem-statement/#the-philosophical-shift","title":"The Philosophical Shift","text":""},{"location":"problem-statement/#old-way-trust-the-model","title":"Old way: Trust the model","text":"<p>\"This model says it considered X, Y, and Z in its reasoning\"</p>"},{"location":"problem-statement/#lionagi-way-trust-the-workflow","title":"LionAGI way: Trust the workflow","text":"<p>\"We asked three specialists, had a critic review, and here's exactly what each one did\"</p>"},{"location":"problem-statement/#who-this-is-for","title":"Who This Is For","text":"<ul> <li>Enterprises who need AI automation but can't deploy black boxes</li> <li>Developers tired of agent chaos and unpredictable outcomes</li> <li>Teams who know one model isn't enough for complex problems</li> <li>Organizations who need to explain AI decisions to auditors and regulators</li> </ul>"},{"location":"problem-statement/#the-bottom-line","title":"The Bottom Line","text":"<p>LionAGI doesn't make AI models explainable (nobody can). LionAGI makes AI workflows observable (everybody needs).</p> <p>In a world racing toward AGI with black box models, we're building the glass box that lets you see - and trust - what's actually happening.</p> <p>The best orchestration is embarrassingly simple: parallel specialists + mandatory critics + artifact coordination + cognitive limits. Everything else is complexity theater.</p>"},{"location":"advanced/","title":"Advanced Topics","text":"<p>This section covers production-oriented features of lionagi: concurrency primitives, resilience patterns, workflow composition, and runtime observability.</p>"},{"location":"advanced/#whats-here","title":"What's Here","text":"<ul> <li> <p>Custom Operations -- Register custom async   functions as Branch operations, compose them into graphs with   <code>OperationGraphBuilder</code>, and use <code>Session.operation()</code> as a decorator.</p> </li> <li> <p>Flow Composition -- Build multi-step graphs with   <code>OperationGraphBuilder</code>, control dependencies, fan-out with <code>expand_from_result</code>,   and aggregate results.</p> </li> <li> <p>Performance -- Use lionagi's structured concurrency   utilities (<code>gather</code>, <code>race</code>, <code>bounded_map</code>, <code>CompletionStream</code>, <code>retry</code>)   and iModel rate limiting to maximize throughput.</p> </li> <li> <p>Error Handling -- Built-in retry logic, circuit   breakers, rate limiting, and structured error propagation in iModel and   operation flows.</p> </li> <li> <p>Observability -- DataLogger and Log objects,   HookRegistry for pre/post-invoke hooks on iModel, message inspection,   and flow-level verbose mode.</p> </li> </ul>"},{"location":"advanced/#prerequisites","title":"Prerequisites","text":"<p>Before reading these pages you should be comfortable with:</p> <ul> <li>Branch operations (<code>chat</code>, <code>communicate</code>, <code>operate</code>, <code>ReAct</code>)</li> <li>iModel configuration and provider setup</li> <li>Python <code>async</code>/<code>await</code> patterns</li> </ul>"},{"location":"advanced/custom-operations/","title":"Custom Operations","text":"<p>Branch ships with built-in operations -- <code>chat</code>, <code>communicate</code>, <code>operate</code>, <code>parse</code>, <code>interpret</code>, <code>ReAct</code>, and <code>act</code>. You can extend this set by registering your own async functions and composing them into graphs.</p>"},{"location":"advanced/custom-operations/#built-in-operations","title":"Built-in Operations","text":"<p>Every Branch instance exposes these methods directly:</p> Operation Adds messages? Tool calling? Structured output? <code>chat</code> No No No <code>communicate</code> Yes No Optional <code>operate</code> Yes Yes Yes <code>parse</code> No No Yes <code>interpret</code> No No No (returns string) <code>ReAct</code> Yes Yes Optional <code>act</code> Yes Yes (only) N/A <p>These are the values accepted by <code>OperationGraphBuilder.add_operation()</code> in its <code>operation</code> parameter (see the <code>BranchOperations</code> literal type in <code>lionagi.operations.node</code>).</p>"},{"location":"advanced/custom-operations/#registering-custom-operations","title":"Registering Custom Operations","text":""},{"location":"advanced/custom-operations/#sessionoperation-decorator","title":"Session.operation() Decorator","text":"<p>The cleanest way to add a custom operation is with the <code>Session.operation()</code> decorator. Registered operations become available to every Branch in the session, and they can be referenced by name in <code>OperationGraphBuilder</code>.</p> <pre><code>from lionagi import Session, Branch\n\nsession = Session()\n\n@session.operation()\nasync def summarize(branch: Branch, instruction: str, **kwargs):\n    \"\"\"Summarize with a structured two-part prompt.\"\"\"\n    guidance = \"Respond with: 1) Key points  2) One-sentence summary\"\n    return await branch.communicate(\n        instruction=instruction,\n        guidance=guidance,\n        **kwargs,\n    )\n</code></pre> <p>The function name (<code>summarize</code>) becomes the operation name. To use a different name, pass it as an argument:</p> <pre><code>@session.operation(\"custom_summary\")\nasync def my_summary_func(branch: Branch, instruction: str, **kwargs):\n    ...\n</code></pre>"},{"location":"advanced/custom-operations/#sessionregister_operation","title":"Session.register_operation()","text":"<p>If you prefer not to use decorators:</p> <pre><code>async def extract_entities(branch: Branch, text: str, **kwargs):\n    from pydantic import BaseModel\n\n    class Entities(BaseModel):\n        people: list[str]\n        organizations: list[str]\n\n    return await branch.operate(\n        instruction=f\"Extract entities from: {text}\",\n        response_format=Entities,\n        **kwargs,\n    )\n\nsession.register_operation(\"extract_entities\", extract_entities)\n</code></pre>"},{"location":"advanced/custom-operations/#requirements","title":"Requirements","text":"<p>Custom operations must be async functions. Synchronous functions will raise <code>ValueError</code> at registration time.</p> <p>The first positional argument receives the <code>Branch</code> instance. Any remaining keyword arguments come from the <code>parameters</code> dict you pass to <code>OperationGraphBuilder.add_operation()</code>.</p>"},{"location":"advanced/custom-operations/#using-custom-operations-in-graphs","title":"Using Custom Operations in Graphs","text":"<p>Once registered, custom operations work exactly like built-in ones in <code>OperationGraphBuilder</code>:</p> <pre><code>from lionagi import Builder, Session, Branch, iModel\n\nsession = Session()\nbranch = Branch(chat_model=iModel(provider=\"openai\", model=\"gpt-4.1-mini\"))\nsession.include_branches(branch)\n\n@session.operation()\nasync def research(branch: Branch, topic: str, **kwargs):\n    return await branch.communicate(\n        instruction=f\"Research: {topic}\",\n        guidance=\"Be thorough and cite sources\",\n    )\n\n@session.operation()\nasync def critique(branch: Branch, instruction: str, **kwargs):\n    return await branch.communicate(\n        instruction=instruction,\n        guidance=\"Identify weaknesses and gaps\",\n    )\n\nbuilder = Builder(\"research_pipeline\")\n\nresearch_id = builder.add_operation(\n    \"research\",\n    branch=branch,\n    topic=\"Transformer architecture efficiency\",\n)\n\ncritique_id = builder.add_operation(\n    \"critique\",\n    branch=branch,\n    instruction=\"Critique the research above\",\n    depends_on=[research_id],\n)\n\nresult = await session.flow(builder.get_graph(), max_concurrent=2)\n</code></pre>"},{"location":"advanced/custom-operations/#wrapping-external-apis","title":"Wrapping External APIs","text":"<p>A common pattern is wrapping external services as custom operations so they participate in graph-based workflows:</p> <pre><code>import httpx\n\n@session.operation()\nasync def fetch_data(branch: Branch, url: str, **kwargs):\n    async with httpx.AsyncClient() as client:\n        response = await client.get(url)\n        data = response.json()\n\n    # Feed external data into the LLM as context\n    return await branch.communicate(\n        instruction=\"Summarize this data\",\n        context=data,\n    )\n</code></pre>"},{"location":"advanced/custom-operations/#composing-built-in-and-custom-operations","title":"Composing Built-in and Custom Operations","text":"<p>Custom operations mix freely with built-in ones in the same graph:</p> <pre><code>builder = Builder(\"mixed_pipeline\")\n\n# Built-in operation\nstep1 = builder.add_operation(\n    \"communicate\",\n    branch=branch,\n    instruction=\"List 5 research questions about climate change\",\n)\n\n# Custom operation depending on step1\nstep2 = builder.add_operation(\n    \"research\",\n    branch=branch,\n    topic=\"Use the questions above\",\n    depends_on=[step1],\n)\n\n# Built-in aggregation\nstep3 = builder.add_aggregation(\n    \"communicate\",\n    branch=branch,\n    source_node_ids=[step2],\n    instruction=\"Write an executive summary\",\n)\n\nresult = await session.flow(builder.get_graph())\n</code></pre>"},{"location":"advanced/custom-operations/#guidelines","title":"Guidelines","text":"<ul> <li>Prefer composing built-in operations (<code>communicate</code>, <code>operate</code>) inside   custom operations rather than reimplementing prompt construction.</li> <li>Keep custom operations focused on a single responsibility.</li> <li>Test operations independently with a Branch before wiring them into graphs.</li> <li>Remember that <code>chat</code> does not add messages to history -- use   <code>communicate</code> or <code>operate</code> when you need conversational context to   accumulate.</li> </ul>"},{"location":"advanced/error-handling/","title":"Error Handling","text":"<p>lionagi provides multiple layers of resilience: retry with backoff in the concurrency layer, circuit breakers and rate limiting in the service layer, and structured error propagation in operation flows.</p>"},{"location":"advanced/error-handling/#imodel-built-in-resilience","title":"iModel Built-in Resilience","text":""},{"location":"advanced/error-handling/#rate-limiting","title":"Rate Limiting","text":"<p>Every <code>iModel</code> wraps a <code>RateLimitedAPIExecutor</code> that automatically queues and throttles requests. You configure limits at construction time:</p> <pre><code>from lionagi import iModel\n\nmodel = iModel(\n    provider=\"openai\",\n    model=\"gpt-4.1-mini\",\n    limit_requests=60,          # Requests per cycle\n    limit_tokens=100_000,       # Tokens per cycle\n    capacity_refresh_time=60,   # Cycle length in seconds\n    queue_capacity=100,         # Max queued requests\n)\n</code></pre> <p>When the request or token budget is exhausted, the executor holds incoming requests until the next replenishment cycle. This happens transparently -- you do not need to add manual delays.</p>"},{"location":"advanced/error-handling/#circuit-breaker","title":"Circuit Breaker","text":"<p>Endpoints can be configured with a <code>CircuitBreaker</code> that prevents repeated calls to a failing service:</p> <pre><code>from lionagi.service.resilience import CircuitBreaker, RetryConfig\n\nbreaker = CircuitBreaker(\n    failure_threshold=5,     # Open after 5 consecutive failures\n    recovery_time=30.0,      # Wait 30s before testing recovery\n    half_open_max_calls=1,   # Allow 1 test call in half-open state\n    name=\"openai_chat\",\n)\n</code></pre> <p>Circuit states:</p> <ul> <li>CLOSED -- normal operation, requests pass through.</li> <li>OPEN -- too many failures, requests are rejected immediately with   <code>CircuitBreakerOpenError</code>.</li> <li>HALF_OPEN -- recovery time elapsed, allowing a limited number of   test calls. Success closes the circuit; failure reopens it.</li> </ul>"},{"location":"advanced/error-handling/#retry-with-backoff-service-layer","title":"Retry with Backoff (Service Layer)","text":"<p>The service layer provides <code>retry_with_backoff</code> for retrying API calls:</p> <pre><code>from lionagi.service.resilience import retry_with_backoff\n\nresult = await retry_with_backoff(\n    some_api_call,\n    arg1, arg2,\n    max_retries=3,\n    base_delay=1.0,\n    max_delay=60.0,\n    backoff_factor=2.0,\n    jitter=True,\n    retry_exceptions=(ConnectionError, TimeoutError),\n    exclude_exceptions=(AuthenticationError,),\n)\n</code></pre> <p>There is also a <code>@with_retry</code> decorator for applying retry logic to async functions declaratively:</p> <pre><code>from lionagi.service.resilience import with_retry\n\n@with_retry(max_retries=3, base_delay=1.0)\nasync def call_external_api():\n    ...\n</code></pre>"},{"location":"advanced/error-handling/#concurrency-layer-retry","title":"Concurrency-Layer Retry","text":"<p>The <code>retry</code> function in <code>lionagi.ln.concurrency</code> provides structured- concurrency-aware retry with deadline support:</p> <pre><code>from lionagi.ln.concurrency import retry, fail_after\n\n# Retry with ambient deadline awareness\nwith fail_after(30):\n    result = await retry(\n        lambda: branch.communicate(\"Analyze this\"),\n        attempts=3,\n        base_delay=0.5,\n        max_delay=5.0,\n        retry_on=(ValueError,),\n    )\n</code></pre> <p>Key differences from the service-layer retry:</p> <ul> <li>Uses AnyIO structured concurrency (cancellation is never retried).</li> <li>Respects parent <code>CancelScope</code> deadlines -- delays are capped so they   do not exceed the ambient deadline.</li> <li>Takes a zero-argument async callable (use <code>lambda</code> or <code>functools.partial</code>).</li> </ul>"},{"location":"advanced/error-handling/#flow-level-error-handling","title":"Flow-Level Error Handling","text":""},{"location":"advanced/error-handling/#operation-status-tracking","title":"Operation Status Tracking","text":"<p>Each <code>Operation</code> node in a flow graph tracks its execution status:</p> <pre><code>from lionagi.protocols.generic import EventStatus\n\n# After session.flow(), check individual operation status\nfor node in builder.get_graph().internal_nodes.values():\n    print(f\"{node.operation}: {node.execution.status}\")\n    if node.execution.status == EventStatus.FAILED:\n        print(f\"  Error: {node.execution.error}\")\n</code></pre> <p>Possible statuses: <code>PENDING</code>, <code>PROCESSING</code>, <code>COMPLETED</code>, <code>FAILED</code>, <code>CANCELLED</code>, <code>SKIPPED</code>.</p>"},{"location":"advanced/error-handling/#partial-success-in-flows","title":"Partial Success in Flows","text":"<p>When an operation in a graph fails, the error is captured and dependent operations may still proceed (they receive the error as context). The flow result separates completed and skipped operations:</p> <pre><code>result = await session.flow(builder.get_graph(), verbose=True)\n\ncompleted = result[\"completed_operations\"]\nskipped = result[\"skipped_operations\"]\nerrors = {\n    op_id: res\n    for op_id, res in result[\"operation_results\"].items()\n    if isinstance(res, dict) and \"error\" in res\n}\n\nprint(f\"Completed: {len(completed)}, Skipped: {len(skipped)}\")\nfor op_id, err in errors.items():\n    print(f\"  {str(op_id)[:8]}: {err['error']}\")\n</code></pre>"},{"location":"advanced/error-handling/#edge-conditions","title":"Edge Conditions","text":"<p>Graph edges can have conditions that control whether dependent operations execute. When all incoming edges fail their conditions, the operation is skipped rather than failed:</p> <pre><code>from lionagi.protocols.graph.edge import Edge, EdgeCondition\n</code></pre>"},{"location":"advanced/error-handling/#provider-fallback-pattern","title":"Provider Fallback Pattern","text":"<p>Try multiple providers in sequence:</p> <pre><code>from lionagi import Branch, iModel\n\nconfigs = [\n    {\"provider\": \"openai\", \"model\": \"gpt-4.1-mini\"},\n    {\"provider\": \"anthropic\", \"model\": \"claude-sonnet-4-20250514\"},\n]\n\nasync def resilient_call(prompt: str) -&gt; str:\n    for i, config in enumerate(configs):\n        try:\n            branch = Branch(chat_model=iModel(**config))\n            return await branch.communicate(prompt)\n        except Exception as e:\n            if i == len(configs) - 1:\n                raise\n            print(f\"Provider {config['provider']} failed: {e}, trying next\")\n</code></pre>"},{"location":"advanced/error-handling/#structured-output-validation","title":"Structured Output Validation","text":"<p><code>branch.operate()</code> and <code>branch.parse()</code> have built-in retry logic for structured output parsing. When the LLM returns malformed JSON, lionagi retries the parse (not the API call) using fuzzy matching:</p> <pre><code>from pydantic import BaseModel\n\nclass Analysis(BaseModel):\n    sentiment: str\n    confidence: float\n    key_points: list[str]\n\nresult = await branch.operate(\n    instruction=\"Analyze this review\",\n    response_format=Analysis,\n    handle_validation=\"return_value\",  # Return best-effort on failure\n    # Other options: \"raise\" (raise on failure), \"return_none\"\n)\n</code></pre> <p>The <code>handle_validation</code> parameter controls behavior when parsing fails after all retries:</p> <ul> <li><code>\"raise\"</code> -- raise an exception.</li> <li><code>\"return_value\"</code> -- return whatever was parsed (may be partial).</li> <li><code>\"return_none\"</code> -- return <code>None</code>.</li> </ul>"},{"location":"advanced/error-handling/#gather-with-return_exceptions","title":"gather with return_exceptions","text":"<p>For batch operations where partial failure is acceptable:</p> <pre><code>from lionagi.ln.concurrency import gather\n\nresults = await gather(\n    branch.communicate(\"Task 1\"),\n    branch.communicate(\"Task 2\"),\n    branch.communicate(\"Task 3\"),\n    return_exceptions=True,\n)\n\nsuccesses = [r for r in results if not isinstance(r, BaseException)]\nfailures = [r for r in results if isinstance(r, BaseException)]\nprint(f\"{len(successes)} succeeded, {len(failures)} failed\")\n</code></pre>"},{"location":"advanced/error-handling/#guidelines","title":"Guidelines","text":"<ul> <li>Let iModel's built-in rate limiting handle API throttling -- do not   add manual <code>sleep()</code> calls.</li> <li>Use <code>CircuitBreaker</code> for endpoints that may go down entirely, not for   transient rate limit errors (rate limiting handles those).</li> <li>Use <code>handle_validation=\"return_value\"</code> in <code>operate()</code> for best-effort   structured output rather than failing the entire pipeline.</li> <li>In flows, use <code>verbose=True</code> to diagnose which operations failed and   why, then add targeted error handling.</li> <li>Prefer <code>gather(return_exceptions=True)</code> over try/except loops for   batch operations.</li> </ul>"},{"location":"advanced/flow-composition/","title":"Flow Composition","text":"<p><code>OperationGraphBuilder</code> (imported as <code>Builder</code>) lets you define directed acyclic graphs of Branch operations. <code>Session.flow()</code> executes them with dependency-aware scheduling and configurable concurrency.</p>"},{"location":"advanced/flow-composition/#core-concepts","title":"Core Concepts","text":"<p>A flow is a DAG where each node is an Operation (a Branch method call with parameters) and edges encode execution order. The executor:</p> <ol> <li>Topologically sorts the graph.</li> <li>Runs independent operations concurrently (up to <code>max_concurrent</code>).</li> <li>Passes predecessor results as <code>context</code> to dependent operations.</li> <li>Returns a dict with <code>completed_operations</code>, <code>operation_results</code>,    <code>skipped_operations</code>, and <code>final_context</code>.</li> </ol>"},{"location":"advanced/flow-composition/#sequential-flows","title":"Sequential Flows","text":"<p>Chain operations with <code>depends_on</code>:</p> <pre><code>from lionagi import Session, Builder, Branch, iModel\n\nsession = Session()\nbranch = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4.1-mini\"),\n    system=\"You are a research analyst.\",\n)\nsession.include_branches(branch)\n\nbuilder = Builder(\"sequential\")\n\nresearch = builder.add_operation(\n    \"communicate\",\n    branch=branch,\n    instruction=\"Research AI market trends for 2025\",\n)\n\nanalysis = builder.add_operation(\n    \"communicate\",\n    branch=branch,\n    instruction=\"Analyze the research findings and identify opportunities\",\n    depends_on=[research],\n)\n\nreport = builder.add_operation(\n    \"communicate\",\n    branch=branch,\n    instruction=\"Write an executive summary with recommendations\",\n    depends_on=[analysis],\n)\n\nresult = await session.flow(builder.get_graph())\n</code></pre> <p>When <code>depends_on</code> is omitted, <code>add_operation</code> automatically links the new node to the previous one (sequential by default).</p>"},{"location":"advanced/flow-composition/#parallel-flows","title":"Parallel Flows","text":"<p>Operations without dependency relationships run concurrently:</p> <pre><code>builder = Builder(\"parallel_analysis\")\n\n# These three operations have no dependencies on each other\nmarket = builder.add_operation(\n    \"communicate\",\n    branch=branch,\n    instruction=\"Analyze market conditions\",\n)\n\ncompetitor = builder.add_operation(\n    \"communicate\",\n    branch=branch,\n    instruction=\"Analyze top 3 competitors\",\n    depends_on=[],  # Explicitly no dependencies\n)\n\ntech = builder.add_operation(\n    \"communicate\",\n    branch=branch,\n    instruction=\"Analyze technology trends\",\n    depends_on=[],\n)\n\n# Aggregate all three into a synthesis\nsynthesis = builder.add_aggregation(\n    \"communicate\",\n    branch=branch,\n    source_node_ids=[market, competitor, tech],\n    instruction=\"Synthesize all analyses into a strategic brief\",\n)\n\nresult = await session.flow(builder.get_graph(), max_concurrent=3)\n</code></pre>"},{"location":"advanced/flow-composition/#fan-out-with-expand_from_result","title":"Fan-Out with expand_from_result","text":"<p>After executing a graph, you can expand it dynamically based on results and continue execution:</p> <pre><code>from lionagi.operations.builder import ExpansionStrategy\n\nbuilder = Builder(\"dynamic_expansion\")\n\n# Step 1: generate sub-tasks\ngenerate = builder.add_operation(\n    \"operate\",\n    branch=branch,\n    instruction=\"List 3 research questions about renewable energy\",\n    response_format=ResearchQuestions,  # a Pydantic model\n)\n\n# Execute step 1\nresult = await session.flow(builder.get_graph())\n\n# Step 2: expand -- create one operation per question\nquestions = result[\"operation_results\"][generate]\nif hasattr(questions, \"questions\"):\n    builder.expand_from_result(\n        items=questions.questions,\n        source_node_id=generate,\n        operation=\"communicate\",\n        strategy=ExpansionStrategy.CONCURRENT,\n        instruction=\"Answer this research question in detail\",\n    )\n\n# Step 3: aggregate expanded results\nbuilder.add_aggregation(\n    \"communicate\",\n    branch=branch,\n    instruction=\"Combine all answers into a report\",\n)\n\n# Execute the expanded graph\nfinal = await session.flow(builder.get_graph())\n</code></pre> <p><code>ExpansionStrategy</code> options:</p> <ul> <li><code>CONCURRENT</code> -- all expanded operations run in parallel (default).</li> <li><code>SEQUENTIAL</code> -- expanded operations run one after another.</li> <li><code>SEQUENTIAL_CONCURRENT_CHUNK</code> -- sequential groups of concurrent ops.</li> <li><code>CONCURRENT_SEQUENTIAL_CHUNK</code> -- concurrent groups of sequential ops.</li> </ul>"},{"location":"advanced/flow-composition/#multi-branch-flows","title":"Multi-Branch Flows","text":"<p>Assign different branches (with different models or system prompts) to different operations:</p> <pre><code>researcher = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4.1-mini\"),\n    system=\"You are a thorough researcher.\",\n)\nanalyst = Branch(\n    chat_model=iModel(provider=\"anthropic\", model=\"claude-sonnet-4-20250514\"),\n    system=\"You are a critical data analyst.\",\n)\nwriter = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4.1-mini\"),\n    system=\"You are a concise report writer.\",\n)\nsession.include_branches([researcher, analyst, writer])\n\nbuilder = Builder(\"multi_branch\")\n\nstep1 = builder.add_operation(\n    \"communicate\",\n    branch=researcher,\n    instruction=\"Research quantum computing advances\",\n)\n\nstep2 = builder.add_operation(\n    \"communicate\",\n    branch=analyst,\n    instruction=\"Critically evaluate the research\",\n    depends_on=[step1],\n)\n\nstep3 = builder.add_operation(\n    \"communicate\",\n    branch=writer,\n    instruction=\"Write a two-paragraph summary\",\n    depends_on=[step2],\n)\n\nresult = await session.flow(builder.get_graph())\n</code></pre>"},{"location":"advanced/flow-composition/#context-inheritance","title":"Context Inheritance","text":"<p>When <code>inherit_context=True</code>, a dependent operation clones the conversation history from its primary dependency (the first ID in <code>depends_on</code>). This means the downstream Branch sees all the messages from the upstream Branch:</p> <pre><code>parent = builder.add_operation(\n    \"communicate\",\n    branch=branch,\n    instruction=\"Analyze business requirements\",\n)\n\nchild = builder.add_operation(\n    \"communicate\",\n    branch=branch,\n    instruction=\"Based on the analysis, suggest an architecture\",\n    depends_on=[parent],\n    inherit_context=True,\n)\n</code></pre> <p>Without <code>inherit_context</code>, each operation gets a fresh branch clone. Predecessor results are still passed as <code>context</code> data, but the conversation history does not carry over.</p>"},{"location":"advanced/flow-composition/#conditional-branching","title":"Conditional Branching","text":"<p><code>add_conditional_branch</code> creates a condition-check node with true/false paths:</p> <pre><code>nodes = builder.add_conditional_branch(\n    condition_check_op=\"communicate\",\n    true_op=\"communicate\",\n    false_op=\"communicate\",\n    instruction=\"Is the dataset large enough for ML? Answer yes or no.\",\n)\n# nodes = {\"check\": id, \"true\": id, \"false\": id}\n</code></pre> <p>Edge conditions on the graph control which path executes at runtime.</p>"},{"location":"advanced/flow-composition/#controlling-concurrency","title":"Controlling Concurrency","text":"<p><code>Session.flow()</code> accepts <code>max_concurrent</code> to limit how many operations run simultaneously:</p> <pre><code># Conservative: 2 concurrent API calls\nresult = await session.flow(builder.get_graph(), max_concurrent=2)\n\n# Aggressive: 10 concurrent calls (watch your rate limits)\nresult = await session.flow(builder.get_graph(), max_concurrent=10)\n\n# Sequential execution\nresult = await session.flow(builder.get_graph(), parallel=False)\n</code></pre> <p>The default is <code>max_concurrent=5</code>. Set <code>parallel=False</code> to force <code>max_concurrent=1</code>.</p>"},{"location":"advanced/flow-composition/#inspecting-results","title":"Inspecting Results","text":"<p><code>Session.flow()</code> returns a dict:</p> <pre><code>result = await session.flow(builder.get_graph(), verbose=True)\n\nprint(result[\"completed_operations\"])   # list of operation IDs\nprint(result[\"skipped_operations\"])     # list of skipped operation IDs\nprint(result[\"operation_results\"])      # {op_id: response} mapping\nprint(result[\"final_context\"])          # accumulated context dict\n</code></pre> <p>Use <code>verbose=True</code> during development to see execution order, dependency waits, and timing.</p>"},{"location":"advanced/flow-composition/#guidelines","title":"Guidelines","text":"<ul> <li>Start with linear flows. Add parallelism only when you have independent   operations that benefit from concurrent execution.</li> <li>Use <code>add_aggregation</code> to merge parallel results into a single downstream   operation.</li> <li>Keep <code>max_concurrent</code> at or below your API provider's rate limit.</li> <li>Use multi-branch flows when operations need different models or system   prompts, not just different instructions.</li> </ul>"},{"location":"advanced/observability/","title":"Observability","text":"<p>lionagi provides several mechanisms for inspecting runtime behavior: <code>DataLogger</code> for activity logs, <code>HookRegistry</code> for aspect-oriented hooks on iModel calls, message inspection on Branch, and verbose mode in <code>Session.flow()</code>.</p>"},{"location":"advanced/observability/#datalogger-and-log","title":"DataLogger and Log","text":"<p>Every Branch has a <code>DataLogger</code> that stores <code>Log</code> entries -- immutable snapshots of events (API calls, tool invocations, etc.).</p>"},{"location":"advanced/observability/#accessing-logs","title":"Accessing Logs","text":"<pre><code>from lionagi import Branch, iModel\n\nbranch = Branch(chat_model=iModel(provider=\"openai\", model=\"gpt-4.1-mini\"))\nawait branch.communicate(\"Explain quantum computing\")\n\n# branch.logs is a Pile[Log]\nprint(f\"Log entries: {len(branch.logs)}\")\nfor log in branch.logs:\n    print(log.content.keys())\n</code></pre>"},{"location":"advanced/observability/#configuring-the-logger","title":"Configuring the Logger","text":"<pre><code>from lionagi.protocols.generic import DataLoggerConfig\n\nconfig = DataLoggerConfig(\n    persist_dir=\"./data/logs\",       # Where log files are saved\n    subfolder=\"experiment_01\",       # Subdirectory within persist_dir\n    file_prefix=\"run\",              # Filename prefix\n    capacity=100,                    # Auto-dump after 100 entries\n    extension=\".json\",               # .json or .csv\n    use_timestamp=True,              # Include timestamp in filename\n    hash_digits=5,                   # Random hash in filename\n    auto_save_on_exit=True,          # Dump remaining logs at exit\n    clear_after_dump=True,           # Clear in-memory logs after dump\n)\n\nbranch = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4.1-mini\"),\n    log_config=config,\n)\n</code></pre> <p>When <code>capacity</code> is set, the logger automatically dumps to disk once that many entries accumulate. When <code>auto_save_on_exit=True</code> (the default), remaining logs are saved when the Python process exits.</p>"},{"location":"advanced/observability/#manual-dump","title":"Manual Dump","text":"<pre><code># Synchronous dump\nbranch.dump_logs(clear=True, persist_path=\"./my_logs.json\")\n\n# Asynchronous dump\nawait branch.adump_logs(clear=True)\n</code></pre>"},{"location":"advanced/observability/#branch-as-context-manager","title":"Branch as Context Manager","text":"<p>Using <code>async with</code> on a Branch automatically dumps logs on exit:</p> <pre><code>async with Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4.1-mini\")\n) as branch:\n    await branch.communicate(\"Hello\")\n    # Logs are dumped when exiting the context\n</code></pre>"},{"location":"advanced/observability/#message-inspection","title":"Message Inspection","text":"<p><code>branch.messages</code> is a <code>Pile[RoledMessage]</code> containing the full conversation history. Use it for debugging, analysis, or export.</p>"},{"location":"advanced/observability/#inspecting-messages","title":"Inspecting Messages","text":"<pre><code>branch = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4.1-mini\"),\n    system=\"You are a research assistant.\",\n)\n\nawait branch.communicate(\"List 3 AI trends\")\nawait branch.communicate(\"Elaborate on the first trend\")\n\n# Total message count (system + instruction + response pairs)\nprint(f\"Messages: {len(branch.messages)}\")\n\n# System message\nif branch.system:\n    print(f\"System: {branch.system.content[:80]}...\")\n\n# Iterate messages\nfor msg in branch.messages:\n    role = msg.role.value if hasattr(msg.role, \"value\") else msg.role\n    content = str(msg.content)[:100]\n    print(f\"[{role}] {content}...\")\n</code></pre>"},{"location":"advanced/observability/#exporting-to-dataframe","title":"Exporting to DataFrame","text":"<pre><code>df = branch.to_df()\nprint(df[[\"role\", \"content\"]].head())\n</code></pre>"},{"location":"advanced/observability/#clearing-history","title":"Clearing History","text":"<pre><code># Clear all messages (keeps system message intact via MessageManager)\nbranch.messages.clear()\n</code></pre>"},{"location":"advanced/observability/#hookregistry","title":"HookRegistry","text":"<p><code>HookRegistry</code> provides aspect-oriented hooks on iModel API calls. You can intercept events at three points:</p> <ol> <li>PreEventCreate -- before the API call event is constructed.</li> <li>PreInvocation -- after the event is queued but before the HTTP    request is sent.</li> <li>PostInvocation -- after the HTTP response is received.</li> </ol>"},{"location":"advanced/observability/#registering-hooks","title":"Registering Hooks","text":"<pre><code>from lionagi import iModel, HookRegistry\nfrom lionagi.service.hooks import HookEventTypes\n\nasync def log_before_call(event, **kwargs):\n    \"\"\"Called before each API request.\"\"\"\n    print(f\"About to call API: {type(event).__name__}\")\n    # Return None to proceed normally\n    return None\n\nasync def log_after_call(event, **kwargs):\n    \"\"\"Called after each API response.\"\"\"\n    print(f\"API call completed: {event.execution.status}\")\n    return None\n\nregistry = HookRegistry(\n    hooks={\n        HookEventTypes.PreInvocation: log_before_call,\n        HookEventTypes.PostInvocation: log_after_call,\n    }\n)\n\nmodel = iModel(\n    provider=\"openai\",\n    model=\"gpt-4.1-mini\",\n    hook_registry=registry,\n)\n</code></pre>"},{"location":"advanced/observability/#exit-hooks","title":"Exit Hooks","text":"<p>When <code>exit_hook=True</code> on iModel, a hook can abort the API call by raising an exception. The exception is captured and the event is marked as cancelled:</p> <pre><code>async def permission_check(event, **kwargs):\n    \"\"\"Block calls that exceed a budget.\"\"\"\n    if over_budget():\n        raise RuntimeError(\"API budget exceeded\")\n    return None\n\nmodel = iModel(\n    provider=\"openai\",\n    model=\"gpt-4.1-mini\",\n    hook_registry=HookRegistry(\n        hooks={HookEventTypes.PreInvocation: permission_check}\n    ),\n    exit_hook=True,\n)\n</code></pre>"},{"location":"advanced/observability/#stream-handlers","title":"Stream Handlers","text":"<p>For streaming responses, register handlers by chunk type:</p> <pre><code>async def handle_chunk(event, chunk_type, chunk, **kwargs):\n    print(f\"Received chunk: {chunk}\")\n\nregistry = HookRegistry(\n    stream_handlers={\"text\": handle_chunk}\n)\n</code></pre>"},{"location":"advanced/observability/#flow-verbose-mode","title":"Flow Verbose Mode","text":"<p><code>Session.flow()</code> accepts <code>verbose=True</code> to print execution details:</p> <pre><code>from lionagi import Session, Builder, Branch, iModel\n\nsession = Session()\nbranch = Branch(chat_model=iModel(provider=\"openai\", model=\"gpt-4.1-mini\"))\nsession.include_branches(branch)\n\nbuilder = Builder(\"debug_flow\")\nstep1 = builder.add_operation(\n    \"communicate\", branch=branch,\n    instruction=\"Research topic A\",\n)\nstep2 = builder.add_operation(\n    \"communicate\", branch=branch,\n    instruction=\"Analyze findings\",\n    depends_on=[step1],\n)\n\nresult = await session.flow(builder.get_graph(), verbose=True)\n</code></pre> <p>With <code>verbose=True</code>, the executor prints:</p> <ul> <li>When each operation starts executing.</li> <li>Dependency wait events (which operation is waiting for which).</li> <li>Completion and failure events with operation IDs.</li> <li>Context inheritance actions.</li> <li>Pre-allocation of branches.</li> </ul>"},{"location":"advanced/observability/#result-inspection","title":"Result Inspection","text":"<pre><code>result = await session.flow(builder.get_graph())\n\nprint(f\"Completed: {len(result['completed_operations'])}\")\nprint(f\"Skipped: {len(result['skipped_operations'])}\")\n\n# Check individual operation results\nfor op_id, response in result[\"operation_results\"].items():\n    if isinstance(response, dict) and \"error\" in response:\n        print(f\"FAILED {str(op_id)[:8]}: {response['error']}\")\n    else:\n        print(f\"OK {str(op_id)[:8]}: {str(response)[:60]}...\")\n</code></pre>"},{"location":"advanced/observability/#graph-visualization","title":"Graph Visualization","text":"<p><code>OperationGraphBuilder</code> provides state inspection and visualization:</p> <pre><code># Text-based state summary\nstate = builder.visualize_state()\nprint(f\"Total nodes: {state['total_nodes']}\")\nprint(f\"Executed: {state['executed_nodes']}\")\nprint(f\"Edges: {state['edges']}\")\n\n# Matplotlib visualization (requires matplotlib and networkx)\nbuilder.visualize(title=\"My Workflow\", figsize=(14, 10))\n</code></pre>"},{"location":"advanced/observability/#operation-timing","title":"Operation Timing","text":"<p>Each Operation node records execution duration:</p> <pre><code>for node in builder.get_graph().internal_nodes.values():\n    if hasattr(node, \"execution\") and node.execution.duration:\n        print(\n            f\"{node.operation}: \"\n            f\"{node.execution.duration:.2f}s \"\n            f\"({node.execution.status})\"\n        )\n</code></pre>"},{"location":"advanced/observability/#guidelines","title":"Guidelines","text":"<ul> <li>Enable <code>verbose=True</code> during development, disable in production.</li> <li>Set <code>capacity</code> on <code>DataLoggerConfig</code> to prevent unbounded memory   growth in long-running processes.</li> <li>Use <code>HookRegistry</code> for cross-cutting concerns (logging, metrics,   access control) rather than modifying individual call sites.</li> <li>Export messages with <code>branch.to_df()</code> for post-hoc analysis of   conversation quality and token usage.</li> <li>Check <code>execution.duration</code> on Operation nodes to identify bottlenecks   in graph workflows.</li> </ul>"},{"location":"advanced/performance/","title":"Performance","text":"<p>lionagi provides structured concurrency primitives in <code>lionagi.ln.concurrency</code> and built-in rate limiting in <code>iModel</code> to help you maximize throughput without overwhelming API providers.</p>"},{"location":"advanced/performance/#concurrency-primitives","title":"Concurrency Primitives","text":"<p>All primitives are built on AnyIO and work with both asyncio and trio backends.</p>"},{"location":"advanced/performance/#gather-concurrent-execution","title":"gather -- Concurrent Execution","text":"<p>Run multiple awaitables concurrently and collect results in input order:</p> <pre><code>from lionagi.ln.concurrency import gather\n\nresults = await gather(\n    branch.communicate(\"Analyze market trends\"),\n    branch.communicate(\"Analyze competitor landscape\"),\n    branch.communicate(\"Analyze technology adoption\"),\n)\n# results[0], results[1], results[2] match input order\n</code></pre> <p>With <code>return_exceptions=True</code>, failures are returned as exception objects instead of propagating:</p> <pre><code>results = await gather(\n    branch.communicate(\"Task A\"),\n    branch.communicate(\"Task B\"),\n    return_exceptions=True,\n)\nfor r in results:\n    if isinstance(r, Exception):\n        print(f\"Failed: {r}\")\n    else:\n        print(f\"Success: {r[:50]}...\")\n</code></pre>"},{"location":"advanced/performance/#race-first-to-complete","title":"race -- First-to-Complete","text":"<p>Run multiple awaitables and return the first result. All other tasks are cancelled:</p> <pre><code>from lionagi.ln.concurrency import race\n\n# Try multiple providers, use whichever responds first\nfastest = await race(\n    openai_branch.communicate(\"Summarize this paper\"),\n    anthropic_branch.communicate(\"Summarize this paper\"),\n)\n</code></pre>"},{"location":"advanced/performance/#bounded_map-concurrent-mapping-with-limit","title":"bounded_map -- Concurrent Mapping with Limit","text":"<p>Apply an async function to a sequence of items with a concurrency limit:</p> <pre><code>from lionagi.ln.concurrency import bounded_map\n\ndocuments = [\"doc1.txt\", \"doc2.txt\", \"doc3.txt\", \"doc4.txt\", \"doc5.txt\"]\n\nasync def summarize(doc: str):\n    return await branch.communicate(f\"Summarize: {doc}\")\n\n# Process all documents, at most 3 at a time\nsummaries = await bounded_map(summarize, documents, limit=3)\n</code></pre> <p>Like <code>gather</code>, it supports <code>return_exceptions=True</code> for partial failure tolerance.</p>"},{"location":"advanced/performance/#completionstream-results-as-they-arrive","title":"CompletionStream -- Results As They Arrive","text":"<p>Iterate over results in completion order (first-finished, not input order):</p> <pre><code>from lionagi.ln.concurrency import CompletionStream\n\ntasks = [\n    branch.communicate(f\"Analyze topic {i}\")\n    for i in range(10)\n]\n\nasync with CompletionStream(tasks, limit=5) as stream:\n    async for idx, result in stream:\n        print(f\"Task {idx} finished: {result[:80]}...\")\n        # Process each result as soon as it's available\n</code></pre> <p>The <code>limit</code> parameter controls how many tasks run concurrently. Without it, all tasks start immediately.</p>"},{"location":"advanced/performance/#retry-exponential-backoff","title":"retry -- Exponential Backoff","text":"<p>Retry an async callable with exponential backoff and deadline awareness:</p> <pre><code>from lionagi.ln.concurrency import retry\n\nresult = await retry(\n    lambda: branch.communicate(\"Flaky request\"),\n    attempts=3,\n    base_delay=0.5,\n    max_delay=5.0,\n    retry_on=(ValueError, ConnectionError),\n    jitter=0.1,\n)\n</code></pre> <p><code>retry</code> respects structured concurrency: cancellation exceptions are never retried, and delays are capped to any ambient deadline from a parent <code>CancelScope</code>.</p>"},{"location":"advanced/performance/#imodel-rate-limiting","title":"iModel Rate Limiting","text":"<p>Every <code>iModel</code> instance uses a <code>RateLimitedAPIExecutor</code> that queues requests and enforces rate limits automatically.</p>"},{"location":"advanced/performance/#configuration","title":"Configuration","text":"<pre><code>from lionagi import iModel\n\nmodel = iModel(\n    provider=\"openai\",\n    model=\"gpt-4.1-mini\",\n    # Rate limiting\n    limit_requests=60,          # Max requests per cycle\n    limit_tokens=100_000,       # Max tokens per cycle\n    capacity_refresh_time=60,   # Cycle duration in seconds\n    # Queue\n    queue_capacity=100,         # Max queued requests\n    # Streaming concurrency\n    concurrency_limit=5,        # Max concurrent streaming requests\n)\n</code></pre> <p>The executor maintains token and request budgets that replenish every <code>capacity_refresh_time</code> seconds. When limits are exhausted, requests queue until capacity is available.</p>"},{"location":"advanced/performance/#per-task-model-selection","title":"Per-Task Model Selection","text":"<p>Use lighter models for simple tasks and heavier models for complex ones:</p> <pre><code>from lionagi import Branch, iModel\n\nfast = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4.1-mini\"),\n    system=\"Classify briefly.\",\n)\npowerful = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4.1\"),\n    system=\"Provide detailed analysis.\",\n)\n\n# Quick classification\ncategory = await fast.communicate(\"Classify: complex or simple?\")\n\n# Route to appropriate model\nif \"complex\" in str(category).lower():\n    analysis = await powerful.communicate(\"Detailed analysis of...\")\nelse:\n    analysis = await fast.communicate(\"Brief analysis of...\")\n</code></pre>"},{"location":"advanced/performance/#flow-level-concurrency","title":"Flow-Level Concurrency","text":"<p><code>Session.flow()</code> controls how many operations in a graph run simultaneously:</p> <pre><code>from lionagi import Session, Builder\n\n# Run at most 3 operations concurrently\nresult = await session.flow(builder.get_graph(), max_concurrent=3)\n\n# Sequential execution (useful for debugging)\nresult = await session.flow(builder.get_graph(), parallel=False)\n</code></pre> <p>The <code>max_concurrent</code> parameter maps directly to a <code>CapacityLimiter</code> in the <code>DependencyAwareExecutor</code>. The default is 5.</p>"},{"location":"advanced/performance/#memory-management","title":"Memory Management","text":""},{"location":"advanced/performance/#clearing-message-history","title":"Clearing Message History","text":"<p>Long-running branches accumulate messages. Clear them when context is no longer needed:</p> <pre><code>branch = Branch(chat_model=iModel(provider=\"openai\", model=\"gpt-4.1-mini\"))\n\nfor chunk in data_chunks:\n    result = await branch.communicate(f\"Process: {chunk}\")\n    results.append(result)\n    branch.messages.clear()  # Free memory, reset context\n</code></pre>"},{"location":"advanced/performance/#branch-as-context-manager","title":"Branch as Context Manager","text":"<p>Branch supports <code>async with</code> for automatic log cleanup:</p> <pre><code>async with Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4.1-mini\")\n) as branch:\n    result = await branch.communicate(\"Analyze this data\")\n    # Logs are automatically dumped on exit\n</code></pre>"},{"location":"advanced/performance/#flow-cleanup","title":"Flow Cleanup","text":"<p>For large graphs, use <code>flow_with_cleanup</code> to free operation results after execution:</p> <pre><code>from lionagi.operations.flow import flow_with_cleanup\n\nresult = await flow_with_cleanup(\n    session=session,\n    graph=builder.get_graph(),\n    cleanup_results=True,\n    keep_only=[final_op_id],  # Only keep the final result\n)\n</code></pre>"},{"location":"advanced/performance/#structured-concurrency-patterns","title":"Structured Concurrency Patterns","text":""},{"location":"advanced/performance/#task-groups","title":"Task Groups","text":"<p>For fine-grained control, use <code>TaskGroup</code> directly:</p> <pre><code>from lionagi.ln.concurrency import create_task_group\n\nresults = {}\n\nasync with create_task_group() as tg:\n    async def worker(name, prompt):\n        results[name] = await branch.communicate(prompt)\n\n    tg.start_soon(worker, \"market\", \"Analyze market\")\n    tg.start_soon(worker, \"tech\", \"Analyze technology\")\n    # All tasks complete before exiting the context\n</code></pre>"},{"location":"advanced/performance/#cancel-scopes","title":"Cancel Scopes","text":"<p>Set timeouts on operations:</p> <pre><code>from lionagi.ln.concurrency import fail_after, move_on_after\n\n# Hard timeout: raises TimeoutError after 30 seconds\nwith fail_after(30):\n    result = await branch.communicate(\"Complex analysis...\")\n\n# Soft timeout: continues execution, result may be None\nwith move_on_after(10) as scope:\n    result = await branch.communicate(\"Quick check...\")\nif scope.cancelled_caught:\n    result = \"Timed out, using fallback\"\n</code></pre>"},{"location":"advanced/performance/#guidelines","title":"Guidelines","text":"<ul> <li>Use <code>bounded_map</code> instead of manual batching loops -- it handles   concurrency limiting and error propagation correctly.</li> <li>Set <code>limit_requests</code> and <code>limit_tokens</code> on <code>iModel</code> to match your   API provider's rate limits.</li> <li>Use <code>CompletionStream</code> when you need to process results as they arrive   rather than waiting for all to complete.</li> <li>Prefer <code>gather</code> over <code>asyncio.gather</code> -- lionagi's version uses   structured concurrency (AnyIO TaskGroups) which provides proper   cancellation semantics.</li> </ul>"},{"location":"cookbook/","title":"Cookbook","text":"<p>You're in Step 5 of the Learning Path</p> <p>You've learned the patterns. Now grab these complete, working examples that you can copy, modify, and use in production.</p> <p>These recipes are production-ready implementations that demonstrate LionAGI patterns in real-world scenarios. Each includes full code, expected outputs, performance metrics, and customization guidance.</p>"},{"location":"cookbook/#available-recipes","title":"Available Recipes","text":""},{"location":"cookbook/#analysis-research","title":"Analysis &amp; Research","text":"<ul> <li>Claim Extraction - Extract and validate claims from   documents</li> <li>Research Synthesis - Aggregate multiple sources into   insights</li> </ul>"},{"location":"cookbook/#business-applications","title":"Business Applications","text":"<ul> <li>HR Automation - Multi-agent HR workflow system</li> <li>Code Review Crew - Parallel code analysis with quality   gates</li> </ul>"},{"location":"cookbook/#creative-work","title":"Creative Work","text":"<ul> <li>Brainstorming - Generate and refine ideas collaboratively</li> </ul>"},{"location":"cookbook/#technical","title":"Technical","text":"<ul> <li>Data Persistence - Save agent state to databases</li> </ul>"},{"location":"cookbook/#quick-templates","title":"Quick Templates","text":""},{"location":"cookbook/#basic-multi-agent-analysis","title":"Basic Multi-Agent Analysis","text":"<pre><code>from lionagi import Branch, iModel\nimport asyncio\n\nagents = {\n    \"analyst\": Branch(system=\"Analyze data\", chat_model=iModel(provider=\"openai\", model=\"gpt-4.1-mini\")),\n    \"critic\": Branch(system=\"Find issues\", chat_model=iModel(provider=\"openai\", model=\"gpt-4.1-mini\")),\n    \"advisor\": Branch(system=\"Give recommendations\", chat_model=iModel(provider=\"openai\", model=\"gpt-4.1-mini\"))\n}\n\nasync def analyze(topic):\n    results = await asyncio.gather(*[\n        agent.chat(f\"Analyze: {topic}\") \n        for agent in agents.values()\n    ])\n    return dict(zip(agents.keys(), results))\n</code></pre>"},{"location":"cookbook/#sequential-pipeline","title":"Sequential Pipeline","text":"<pre><code>from lionagi import Session, Branch, Builder\n\nasync def pipeline(input_data):\n    session = Session()\n    builder = Builder(\"pipeline\")\n\n    extract = builder.add_operation(\"chat\", instruction=f\"Extract key points from: {input_data}\")\n    analyze = builder.add_operation(\"chat\", depends_on=[extract], instruction=\"Analyze the extracted points\")\n    summarize = builder.add_operation(\"chat\", depends_on=[analyze], instruction=\"Create executive summary\")\n\n    return await session.flow(builder.get_graph())\n</code></pre>"},{"location":"cookbook/#contributing-examples","title":"Contributing Examples","text":"<p>Have a useful pattern? Submit a PR with:</p> <ol> <li>Working code</li> <li>Clear use case</li> <li>Expected output</li> <li>Performance metrics</li> </ol> <p>Congratulations! \ud83c\udf89</p> <p>You've completed the LionAGI learning path! You now have:</p> <ul> <li>\u2705 Understanding of LionAGI's paradigm and advantages</li> <li>\u2705 Knowledge of core concepts and architecture  </li> <li>\u2705 Proven patterns for common workflows</li> <li>\u2705 Production-ready examples to build upon</li> </ul> <p>What's next? - Advanced Topics - Custom operations, performance tuning, observability - Integrations - Connect with databases, tools, and services - API Reference - Full API documentation</p>"},{"location":"cookbook/brainstorming/","title":"Brainstorming Workflows","text":"<p>Creative ideation using parallel agents with divergent \u2192 convergent thinking patterns.</p>"},{"location":"cookbook/brainstorming/#basic-brainstorming-pattern","title":"Basic Brainstorming Pattern","text":"<pre><code>from lionagi import Branch, Session, Builder, iModel\n\nsession = Session()\nbuilder = Builder(\"brainstorming\")\n\n# Create diverse creative agents\ninnovator = Branch(\n    chat_model=iModel(provider=\"anthropic\", model=\"claude-sonnet-4-20250514\"),\n    system=\"Innovative thinker generating bold, unconventional ideas.\"\n)\n\npragmatist = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4.1-mini\"),\n    system=\"Focus on practical, implementable solutions.\"\n)\n\ncontrarian = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4.1-mini\"),\n    system=\"Challenge assumptions and think from opposite perspectives.\"\n)\n\nsynthesizer = Branch(\n    chat_model=iModel(provider=\"anthropic\", model=\"claude-sonnet-4-20250514\"),\n    system=\"Combine and refine ideas into coherent solutions.\"\n)\n\nsession.include_branches([innovator, pragmatist, contrarian, synthesizer])\n\nchallenge = \"How can we reduce plastic waste in urban environments?\"\n\n# Parallel ideation phase\ninnovator_ideas = builder.add_operation(\n    \"communicate\",\n    branch=innovator,\n    instruction=f\"Generate 3 innovative solutions: {challenge}\"\n)\n\npragmatist_ideas = builder.add_operation(\n    \"communicate\", \n    branch=pragmatist,\n    instruction=f\"Generate 3 practical solutions: {challenge}\"\n)\n\ncontrarian_ideas = builder.add_operation(\n    \"communicate\",\n    branch=contrarian,\n    instruction=f\"3 unconventional approaches: {challenge}\"\n)\n\n# Synthesis phase\nsynthesis = builder.add_aggregation(\n    \"communicate\",\n    branch=synthesizer,\n    source_node_ids=[innovator_ideas, pragmatist_ideas, contrarian_ideas],\n    instruction=\"Create 3 refined solutions combining best aspects\"\n)\n\nresult = await session.flow(builder.get_graph())\n</code></pre>"},{"location":"cookbook/brainstorming/#sequential-brainstorming-process","title":"Sequential Brainstorming Process","text":"<pre><code># Sequential stages: Explore \u2192 Generate \u2192 Evaluate \u2192 Refine\nexplorer = Branch(system=\"Explore problems deeply, identify root causes\")\ngenerator = Branch(system=\"Generate many diverse ideas quickly\")  \nevaluator = Branch(system=\"Evaluate ideas for feasibility and impact\")\nrefiner = Branch(system=\"Refine and improve promising ideas\")\n\nproblem = \"Remote team creative collaboration challenges\"\n\n# Chain dependent operations\nexplore = builder.add_operation(\"communicate\", branch=explorer, \n                                instruction=f\"Analyze problem: {problem}\")\n\ngenerate = builder.add_operation(\"communicate\", branch=generator,\n                                 instruction=\"Generate 10 solution approaches\",\n                                 depends_on=[explore])\n\nevaluate = builder.add_operation(\"communicate\", branch=evaluator,\n                                 instruction=\"Evaluate and rank top 5 ideas\",\n                                 depends_on=[generate])\n\nrefine = builder.add_operation(\"communicate\", branch=refiner,\n                               instruction=\"Develop top ideas into solutions\",\n                               depends_on=[evaluate])\n\nresult = await session.flow(builder.get_graph())\n</code></pre>"},{"location":"cookbook/brainstorming/#multiple-perspectives","title":"Multiple Perspectives","text":"<pre><code># Different viewpoint agents\nuser_advocate = Branch(system=\"Represent end-user needs and experiences\")\ntech_expert = Branch(system=\"Focus on technical feasibility\")\nbusiness_analyst = Branch(system=\"Consider business impact and ROI\") \ncreative_director = Branch(system=\"Focus on innovative user experiences\")\n\nchallenge = \"Design mobile app for sustainable living\"\n\n# Parallel perspective generation\nimport asyncio\n\nperspectives = {\n    \"user_advocate\": user_advocate,\n    \"tech_expert\": tech_expert,\n    \"business_analyst\": business_analyst,\n    \"creative_director\": creative_director,\n}\n\nasync def get_perspective(name, agent):\n    prompt = f\"From {name} perspective, 3 key ideas for: {challenge}\"\n    return name, await agent.communicate(prompt)\n\npairs = await asyncio.gather(*[\n    get_perspective(name, agent) for name, agent in perspectives.items()\n])\nresults = dict(pairs)\n\n# Synthesize perspectives\nsynthesizer = Branch(system=\"Synthesize diverse perspectives into solutions\")\nall_perspectives = \"\\n\\n\".join([f\"{k}: {v}\" for k, v in results.items()])\nsynthesis = await synthesizer.communicate(f\"Synthesize: {all_perspectives}\")\n</code></pre>"},{"location":"cookbook/brainstorming/#rapid-ideation-sprint","title":"Rapid Ideation Sprint","text":"<pre><code># Quick parallel idea generation\ngenerators = [\n    Branch(system=\"Generate wild, unconventional ideas\"),\n    Branch(system=\"Focus on simple, elegant solutions\"),\n    Branch(system=\"Think scalable, systematic approaches\"),\n    Branch(system=\"Consider user-centered solutions\")\n]\n\ntopic = \"Make coding accessible to beginners\"\n\n# Parallel rapid generation\nimport asyncio\n\nall_ideas = await asyncio.gather(*[\n    gen.communicate(f\"5 quick ideas: {topic}\") for gen in generators\n])\n\n# Curate best ideas\ncurator = Branch(system=\"Identify and combine best ideas\")\ncuration = await curator.communicate(f\"Top 7 from: {all_ideas}\")\n</code></pre>"},{"location":"cookbook/brainstorming/#best-practices","title":"Best Practices","text":""},{"location":"cookbook/brainstorming/#diverse-agent-personalities","title":"Diverse Agent Personalities","text":"<pre><code># Different thinking styles\nagents = [\n    Branch(system=\"Think analytically and systematically\"),\n    Branch(system=\"Think creatively and associatively\"), \n    Branch(system=\"Think practically and implementally\"),\n    Branch(system=\"Think critically and skeptically\")\n]\n</code></pre>"},{"location":"cookbook/brainstorming/#clear-ideation-prompts","title":"Clear Ideation Prompts","text":"<pre><code># Good: Specific, actionable\n\"Generate 5 solutions for X under $Y budget in Z time\"\n\n# Avoid: Vague\n\"Think of some ideas\"\n</code></pre>"},{"location":"cookbook/brainstorming/#structured-synthesis","title":"Structured Synthesis","text":"<pre><code>synthesis_prompt = f\"\"\"\nReview ideas: {all_ideas}\nCreate 3 refined concepts that:\n1. Combine best aspects\n2. Address concerns  \n3. Are actionable\n\"\"\"\n</code></pre>"},{"location":"cookbook/brainstorming/#balance-divergence-and-convergence","title":"Balance Divergence and Convergence","text":"<p>Pattern: Divergent (generate many) \u2192 Convergent (refine/combine) \u2192 Select (develop)</p>"},{"location":"cookbook/brainstorming/#when-to-use","title":"When to Use","text":"<p>Perfect for: Product development, problem solving, strategic planning, content creation, process improvement</p> <p>AI brainstorming leverages parallel processing and diverse perspectives for faster, higher-quality ideation through structured synthesis phases.</p>"},{"location":"cookbook/claim-extraction/","title":"Academic Claim Extraction","text":"<p>ReAct-based claim extraction with sequential document analysis and structured outputs.</p>"},{"location":"cookbook/claim-extraction/#basic-claim-extraction","title":"Basic Claim Extraction","text":"<pre><code>from typing import Literal\nfrom pathlib import Path\nfrom pydantic import BaseModel, Field\nfrom lionagi import Branch, Session, Builder, types, iModel\nfrom lionagi.tools.types import ReaderTool\n\n# Structured claim models\nclass Claim(BaseModel):\n    claim: str\n    type: Literal[\"citation\", \"performance\", \"technical\", \"other\"]\n    location: str = Field(..., description=\"Section/paragraph reference\")\n    verifiability: Literal[\"high\", \"medium\", \"low\"]\n    search_strategy: str = Field(..., description=\"How to verify this claim\")\n\nclass ClaimExtraction(BaseModel):\n    claims: list[Claim]\n\nasync def extract_claims_from_document(document_path: str):\n    \"\"\"Extract verifiable claims using ReAct pattern\"\"\"\n\n    # Create ReAct-enabled branch with ReaderTool\n    extractor = Branch(\n        tools=[ReaderTool],\n        chat_model=iModel(provider=\"openai\", model=\"gpt-4.1-mini\"),\n        name=\"claim_extractor\"\n    )\n\n    # Use ReAct for systematic claim extraction\n    result = await extractor.ReAct(\n        instruct=types.Instruct(\n            instruction=(\n                f\"Use ReaderTool to analyze document at {document_path}. \"\n                \"Extract 5-7 specific, verifiable claims. Focus on citations, \"\n                \"performance metrics, and technical assertions.\"\n            ),\n            context={\"document_path\": document_path}\n        ),\n        response_format=ClaimExtraction,\n        tools=[\"reader_tool\"],\n        max_extensions=4,\n        verbose=True\n    )\n\n    return result\n\n# Usage\ndocument_path = \"data/research_paper.pdf\"\nclaims = await extract_claims_from_document(document_path)\n\nfor claim in claims.claims:\n    print(f\"[{claim.type.upper()}] {claim.claim}\")\n    print(f\"Location: {claim.location}\")\n    print(f\"Verifiability: {claim.verifiability}\\n\")\n</code></pre>"},{"location":"cookbook/claim-extraction/#sequential-document-analysis","title":"Sequential Document Analysis","text":"<pre><code>async def sequential_claim_analysis(document_path: str):\n    \"\"\"Progressive document analysis: open \u2192 analyze \u2192 extract claims\"\"\"\n\n    # Create branch with ReaderTool\n    analyzer = Branch(\n        tools=[ReaderTool],\n        chat_model=iModel(provider=\"openai\", model=\"gpt-4.1-mini\")\n    )\n    session = Session(default_branch=analyzer)\n    builder = Builder(\"ClaimAnalysis\")\n\n    # Step 1: Document exploration\n    doc_reader = builder.add_operation(\n        \"ReAct\",\n        node_id=\"explore_document\",\n        instruct=types.Instruct(\n            instruction=(\n                \"Open and explore the document structure. \"\n                \"Identify sections containing verifiable claims.\"\n            ),\n            context={\"document_path\": document_path}\n        ),\n        tools=[\"reader_tool\"],\n        max_extensions=2,\n        verbose=True\n    )\n\n    # Step 2: Content analysis\n    content_analyzer = builder.add_operation(\n        \"ReAct\",\n        node_id=\"analyze_content\",\n        depends_on=[doc_reader],\n        instruct=types.Instruct(\n            instruction=(\n                \"Analyze key sections for citations, technical claims, \"\n                \"and performance metrics that can be verified.\"\n            )\n        ),\n        response_format=types.Outline,\n        tools=[\"reader_tool\"],\n        max_extensions=3,\n        verbose=True\n    )\n\n    # Step 3: Claim extraction\n    claim_extractor = builder.add_operation(\n        \"ReAct\",\n        node_id=\"extract_claims\",\n        depends_on=[content_analyzer],\n        instruct=types.Instruct(\n            instruction=(\n                \"Extract specific verifiable claims based on analysis. \"\n                \"Prioritize citations, performance data, and technical assertions.\"\n            )\n        ),\n        response_format=ClaimExtraction,\n        tools=[\"reader_tool\"],\n        max_extensions=3,\n        verbose=True\n    )\n\n    # Execute sequential workflow\n    graph = builder.get_graph()\n    result = await session.flow(graph, parallel=False, verbose=True)\n\n    return result[\"operation_results\"][claim_extractor]\n\n# Usage\nclaims = await sequential_claim_analysis(\"data/ai_safety_paper.pdf\")\n</code></pre>"},{"location":"cookbook/claim-extraction/#multi-document-claim-extraction","title":"Multi-Document Claim Extraction","text":"<pre><code>from typing import Dict\nimport asyncio\n\nclass DocumentClaims(BaseModel):\n    document: str\n    claims: list[Claim]\n    summary: str\n\nasync def extract_from_multiple_documents(document_paths: list[str]):\n    \"\"\"Parallel claim extraction from multiple documents\"\"\"\n\n    async def process_document(doc_path: str) -&gt; DocumentClaims:\n        \"\"\"Process single document\"\"\"\n        extractor = Branch(\n            tools=[ReaderTool],\n            chat_model=iModel(provider=\"openai\", model=\"gpt-4.1-mini\"),\n            name=f\"extractor_{Path(doc_path).stem}\"\n        )\n\n        # Extract claims using ReAct\n        result = await extractor.ReAct(\n            instruct=types.Instruct(\n                instruction=(\n                    f\"Analyze {doc_path} and extract verifiable claims. \"\n                    \"Focus on novel findings and key assertions.\"\n                ),\n                context={\"document\": doc_path}\n            ),\n            response_format=ClaimExtraction,\n            tools=[\"reader_tool\"],\n            max_extensions=3\n        )\n\n        # Generate summary\n        summary = await extractor.communicate(\n            \"Provide brief summary of the document's main contributions\"\n        )\n\n        return DocumentClaims(\n            document=doc_path,\n            claims=result.claims,\n            summary=summary\n        )\n\n    # Process documents in parallel\n    tasks = [process_document(doc) for doc in document_paths]\n    results = await asyncio.gather(*tasks)\n\n    return results\n\n# Usage\npapers = [\n    \"data/transformer_paper.pdf\",\n    \"data/bert_paper.pdf\", \n    \"data/gpt_paper.pdf\"\n]\nall_claims = await extract_from_multiple_documents(papers)\n</code></pre>"},{"location":"cookbook/claim-extraction/#claim-validation-pipeline","title":"Claim Validation Pipeline","text":"<pre><code>class ValidationResult(BaseModel):\n    claim: str\n    validation_status: Literal[\"verified\", \"disputed\", \"unclear\", \"unverifiable\"]\n    evidence: list[str]\n    confidence: float\n\nclass ClaimValidator(BaseModel):\n    validations: list[ValidationResult]\n\ndef search_evidence(claim: str) -&gt; str:\n    \"\"\"Mock search function - replace with actual search API\"\"\"\n    return f\"Search results for: {claim}\"\n\ndef cross_reference(claim: str, reference_docs: list[str]) -&gt; str:\n    \"\"\"Cross-reference claim against known sources\"\"\"\n    return f\"Cross-reference results for: {claim}\"\n\nasync def validate_claims(claims: list[Claim], reference_docs: list[str] = None):\n    \"\"\"Validate extracted claims using ReAct reasoning\"\"\"\n\n    validator = Branch(\n        tools=[search_evidence, cross_reference],\n        chat_model=iModel(provider=\"openai\", model=\"gpt-4.1-mini\"),\n        name=\"claim_validator\"\n    )\n\n    validation_tasks = []\n\n    for claim in claims:\n        # Use ReAct to validate each claim\n        task = validator.ReAct(\n            instruct=types.Instruct(\n                instruction=(\n                    f\"Validate this claim: '{claim.claim}' \"\n                    \"Use available tools to search for evidence and cross-reference.\"\n                ),\n                context={\n                    \"claim\": claim.model_dump(),\n                    \"reference_docs\": reference_docs or []\n                }\n            ),\n            response_format=ValidationResult,\n            max_extensions=4,\n            verbose=True\n        )\n        validation_tasks.append(task)\n\n    # Execute validations in parallel\n    validations = await asyncio.gather(*validation_tasks)\n\n    return ClaimValidator(validations=validations)\n\n# Usage\nextracted_claims = claims.claims  # From previous extraction\nvalidation_results = await validate_claims(extracted_claims)\n\nfor validation in validation_results.validations:\n    print(f\"Claim: {validation.claim}\")\n    print(f\"Status: {validation.validation_status}\")\n    print(f\"Confidence: {validation.confidence}\\n\")\n</code></pre>"},{"location":"cookbook/claim-extraction/#citation-specific-extraction","title":"Citation-Specific Extraction","text":"<pre><code>class Citation(BaseModel):\n    text: str\n    authors: list[str]\n    year: int\n    title: str\n    venue: str\n    context: str = Field(..., description=\"Context where citation appears\")\n\nclass CitationExtraction(BaseModel):\n    citations: list[Citation]\n\nasync def extract_citations(document_path: str):\n    \"\"\"Extract and structure citations from academic papers\"\"\"\n\n    citation_extractor = Branch(\n        tools=[ReaderTool],\n        chat_model=iModel(provider=\"openai\", model=\"gpt-4.1-mini\"),\n        system=\"You specialize in extracting citations from academic papers\",\n        name=\"citation_extractor\"\n    )\n\n    # Step 1: Identify citation patterns\n    result = await citation_extractor.ReAct(\n        instruct=types.Instruct(\n            instruction=(\n                \"Scan document for citations and references. \"\n                \"Extract complete citation information including context.\"\n            ),\n            context={\"document_path\": document_path}\n        ),\n        response_format=CitationExtraction,\n        tools=[\"reader_tool\"],\n        max_extensions=5,\n        verbose=True\n    )\n\n    return result\n\n# Usage\ncitations = await extract_citations(\"data/survey_paper.pdf\")\nprint(f\"Found {len(citations.citations)} citations\")\n</code></pre>"},{"location":"cookbook/claim-extraction/#performance-claims-analysis","title":"Performance Claims Analysis","text":"<pre><code>class PerformanceClaim(BaseModel):\n    metric: str\n    value: str\n    baseline: str = None\n    improvement: str = None\n    dataset: str\n    methodology: str\n    location: str\n\nclass PerformanceExtraction(BaseModel):\n    performance_claims: list[PerformanceClaim]\n\nasync def extract_performance_claims(document_path: str):\n    \"\"\"Extract performance metrics and benchmarks\"\"\"\n\n    performance_extractor = Branch(\n        tools=[ReaderTool],\n        chat_model=iModel(provider=\"openai\", model=\"gpt-4.1-mini\"),\n        system=\"You extract performance metrics and experimental results from research papers\",\n        name=\"performance_extractor\"\n    )\n\n    result = await performance_extractor.ReAct(\n        instruct=types.Instruct(\n            instruction=(\n                \"Extract performance claims including metrics, values, \"\n                \"baselines, datasets, and methodology details.\"\n            ),\n            context={\"document_path\": document_path}\n        ),\n        response_format=PerformanceExtraction,\n        tools=[\"reader_tool\"],\n        max_extensions=4,\n        verbose=True\n    )\n\n    return result\n\n# Usage\nperformance_data = await extract_performance_claims(\"data/benchmark_paper.pdf\")\n</code></pre>"},{"location":"cookbook/claim-extraction/#production-pipeline","title":"Production Pipeline","text":"<pre><code>async def production_claim_extraction_pipeline(\n    document_paths: list[str],\n    validate_claims: bool = True,\n    extract_citations: bool = True\n):\n    \"\"\"Complete production pipeline for claim extraction\"\"\"\n\n    try:\n        results = {}\n\n        for doc_path in document_paths:\n            print(f\"Processing: {doc_path}\")\n\n            # Sequential analysis\n            claims = await sequential_claim_analysis(doc_path)\n            results[doc_path] = {\"claims\": claims}\n\n            # Optional citation extraction\n            if extract_citations:\n                citations = await extract_citations(doc_path)\n                results[doc_path][\"citations\"] = citations\n\n            # Optional claim validation\n            if validate_claims:\n                validations = await validate_claims(claims.claims)\n                results[doc_path][\"validations\"] = validations\n\n            print(f\"Completed: {doc_path}\")\n\n        return results\n\n    except Exception as e:\n        print(f\"Pipeline failed: {e}\")\n        import traceback\n        traceback.print_exc()\n        return None\n\n# Usage\ndocuments = [\n    \"data/paper1.pdf\",\n    \"data/paper2.pdf\",\n    \"data/paper3.pdf\"\n]\n\npipeline_results = await production_claim_extraction_pipeline(\n    documents,\n    validate_claims=True,\n    extract_citations=True\n)\n</code></pre>"},{"location":"cookbook/claim-extraction/#key-features","title":"Key Features","text":"<p>ReAct Integration:</p> <ul> <li>Systematic reasoning before extraction</li> <li>Tool-assisted document analysis</li> <li>Progressive understanding building</li> </ul> <p>Structured Outputs:</p> <ul> <li>Pydantic models for reliable parsing</li> <li>Type-safe claim categorization</li> <li>Consistent data formats</li> </ul> <p>Sequential Processing:</p> <ul> <li>Document exploration \u2192 analysis \u2192 extraction</li> <li>Context-aware claim identification</li> <li>Dependency-based operation flow</li> </ul> <p>Validation Pipeline:</p> <ul> <li>Evidence searching and cross-referencing</li> <li>Confidence scoring for claims</li> <li>Multi-source verification</li> </ul> <p>Production Ready:</p> <ul> <li>Error handling and recovery</li> <li>Parallel processing support</li> <li>Comprehensive logging and monitoring</li> </ul>"},{"location":"cookbook/code-review-crew/","title":"Multi-Agent Code Review","text":"<p>Use specialized agents to review code from multiple perspectives.</p>"},{"location":"cookbook/code-review-crew/#basic-multi-agent-review","title":"Basic Multi-Agent Review","text":"<pre><code>from lionagi import Session, Branch, iModel\n\nsession = Session()\n\n# Create specialized reviewers\nsecurity = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4.1-mini\"),\n    system=\"Security expert. Focus only on security issues.\"\n)\n\nperformance = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4.1-mini\"),\n    system=\"Performance expert. Focus only on performance issues.\"\n)\n\nmaintainability = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4.1-mini\"),\n    system=\"Code quality expert. Focus on maintainability and readability.\"\n)\n\nsession.include_branches([security, performance, maintainability])\n\n# Code to review\ncode = '''\ndef login(user, pwd):\n    query = f\"SELECT * FROM users WHERE name='{user}' AND pass='{pwd}'\"\n    return db.execute(query).fetchone()\n'''\n\n# Parallel reviews\nimport asyncio\n\nsecurity_result, performance_result, maintainability_result = await asyncio.gather(\n    security.chat(f\"Security review: {code}\"),\n    performance.chat(f\"Performance review: {code}\"),\n    maintainability.chat(f\"Code quality review: {code}\"),\n)\n\nreview_results = {\n    \"security\": security_result,\n    \"performance\": performance_result,\n    \"maintainability\": maintainability_result,\n}\n</code></pre>"},{"location":"cookbook/code-review-crew/#builder-pattern-review","title":"Builder Pattern Review","text":"<pre><code>from lionagi import Session, Builder, Branch, iModel\n\nsession = Session()\nbuilder = Builder(\"code_review\")\n\n# Reviewers\nsecurity_branch = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4.1-mini\"),\n    system=\"Security code reviewer\"\n)\nquality_branch = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4.1-mini\"),\n    system=\"Code quality reviewer\"\n)\n\nsession.include_branches([security_branch, quality_branch])\n\n# Code snippet to review (example with SQL injection vulnerability)\nuser_input = \"1 OR 1=1\"  # Example malicious input\ncode_snippet = \"SELECT * FROM users WHERE id=\" + user_input\n\n# Parallel review operations\nsecurity_review = builder.add_operation(\n    \"communicate\",\n    branch=security_branch,\n    instruction=f\"Review for security issues: {code_snippet}\"\n)\n\nquality_review = builder.add_operation(\n    \"communicate\", \n    branch=quality_branch,\n    instruction=f\"Review for code quality: {code_snippet}\"\n)\n\n# Synthesis\nsynthesis = builder.add_aggregation(\n    \"communicate\",\n    branch=security_branch,\n    source_node_ids=[security_review, quality_review],\n    instruction=\"Summarize all review findings\"\n)\n\nresult = await session.flow(builder.get_graph())\n</code></pre>"},{"location":"cookbook/code-review-crew/#review-with-final-decision","title":"Review with Final Decision","text":"<pre><code>session = Session()\nbuilder = Builder(\"comprehensive_review\")\n\n# Create multiple reviewers\nreview_types = [\"security\", \"performance\", \"maintainability\", \"correctness\"]\nreviewers = {}\nreview_ops = []\n\nfor review_type in review_types:\n    branch = Branch(\n        chat_model=iModel(provider=\"openai\", model=\"gpt-4.1-mini\"),\n        system=f\"{review_type.title()} code reviewer\"\n    )\n    reviewers[review_type] = branch\n\n    op_id = builder.add_operation(\n        \"communicate\",\n        branch=branch,\n        instruction=f\"{review_type} review of submitted code\"\n    )\n    review_ops.append(op_id)\n\n# Senior reviewer for final decision\nsenior = Branch(\n    chat_model=iModel(provider=\"anthropic\", model=\"claude-sonnet-4-20250514\"),\n    system=\"Senior code reviewer who makes final approval decisions\"\n)\n\nsession.include_branches([*reviewers.values(), senior])\n\n# Final synthesis\nfinal_decision = builder.add_aggregation(\n    \"communicate\",\n    branch=senior,\n    source_node_ids=review_ops,\n    instruction=\"Based on all reviews, provide final APPROVE/REJECT decision\"\n)\n\nresult = await session.flow(builder.get_graph())\n</code></pre>"},{"location":"cookbook/code-review-crew/#best-practices","title":"Best Practices","text":""},{"location":"cookbook/code-review-crew/#specialized-systems","title":"Specialized Systems","text":"<pre><code># Good: Clear specialization\nsecurity = Branch(system=\"Focus only on security vulnerabilities\")\nperformance = Branch(system=\"Focus only on performance bottlenecks\")\n\n# Avoid: Generic reviewers\ngeneric = Branch(system=\"Review all aspects of code\")\n</code></pre>"},{"location":"cookbook/code-review-crew/#structured-output","title":"Structured Output","text":"<pre><code>instruction = \"\"\"\nReview this code for security issues:\n\nFormat response as:\n- Issues Found: [list]\n- Severity: [high/medium/low]  \n- Recommendations: [list]\n\"\"\"\n</code></pre>"},{"location":"cookbook/code-review-crew/#advanced-parallel-execution","title":"Advanced Parallel Execution","text":"<pre><code>import asyncio\n\n# asyncio.gather() for parallel reviews\nreviews = await asyncio.gather(\n    security.chat(prompt),\n    performance.chat(prompt),\n    quality.chat(prompt),\n)\n</code></pre> <p>Multi-agent code review leverages specialized expertise in parallel, catching issues that single reviewers might miss.</p>"},{"location":"cookbook/data-persistence/","title":"Data Persistence","text":"<p>Direct Node-to-database patterns with automatic schema creation.</p>"},{"location":"cookbook/data-persistence/#basic-postgresql-setup","title":"Basic PostgreSQL Setup","text":"<pre><code># Install lionagi with postgres support\n# uv add \"lionagi[postgres]\"\n\nfrom pydantic import BaseModel\nfrom typing import Literal\nfrom lionagi import types\nfrom lionagi.adapters.async_postgres_adapter import LionAGIAsyncPostgresAdapter\n\n# Connection string format\ndsn = \"postgresql+asyncpg://postgres:postgres@127.0.0.1:54322/postgres\"\n\n# Define your data model\nclass StudentInfo(BaseModel):\n    name: str\n    age: int\n    grade: Literal[\"A\", \"B\", \"C\", \"D\", \"F\"]\n\n# Create Node with content\nclass Student(types.Node):\n    content: StudentInfo\n\n# Register the adapter\nStudent.register_async_adapter(LionAGIAsyncPostgresAdapter)\n</code></pre>"},{"location":"cookbook/data-persistence/#save-operations","title":"Save Operations","text":"<pre><code># Create student objects\nstudents = [\n    Student(content=StudentInfo(name=\"Adam Smith\", grade=\"A\", age=20)),\n    Student(content=StudentInfo(name=\"Bob Johnson\", grade=\"B\", age=22)),\n    Student(content=StudentInfo(name=\"Charlie Brown\", grade=\"C\", age=21)),\n]\n\n# Save to database (table created automatically)\nrecords = []\nfor student in students:\n    result = await student.adapt_to_async(\n        obj_key=\"lionagi_async_pg\",\n        dsn=dsn,\n        table=\"students\",\n    )\n    records.append(result)\n\nprint(f\"Saved {len(records)} records\")\n# Output: Saved 3 records\n</code></pre>"},{"location":"cookbook/data-persistence/#query-operations","title":"Query Operations","text":"<pre><code># Fetch single record\nstudent = await Student.adapt_from_async(\n    {\"dsn\": dsn, \"table\": \"students\"},\n    obj_key=\"lionagi_async_pg\",\n)\nprint(f\"Retrieved: {student.content.name}, Grade: {student.content.grade}\")\n\n# Fetch multiple with limit\nstudents = await Student.adapt_from_async(\n    {\"dsn\": dsn, \"table\": \"students\", \"limit\": 2},\n    obj_key=\"lionagi_async_pg\",\n    many=True,\n)\nprint(f\"Retrieved {len(students)} students\")\n\n# Fetch with conditions\nadam = await Student.adapt_from_async(\n    {\"dsn\": dsn, \"table\": \"students\", \"selectors\": {\"id\": str(student.id)}},\n    obj_key=\"lionagi_async_pg\",\n)\n</code></pre>"},{"location":"cookbook/data-persistence/#update-operations","title":"Update Operations","text":"<pre><code># Modify and update\nadam.content.age = 22\n\n# Update in database\nresult = await adam.adapt_to_async(\n    \"lionagi_async_pg\",\n    dsn=dsn,\n    table=\"students\",\n    operation=\"update\",\n    where={\"id\": str(adam.id)},\n)\n\n# Verify update\nupdated_adam = await Student.adapt_from_async(\n    {\"dsn\": dsn, \"table\": \"students\", \"selectors\": {\"id\": str(adam.id)}},\n    obj_key=\"lionagi_async_pg\",\n)\nprint(f\"Updated age: {updated_adam.content.age}\")\n</code></pre>"},{"location":"cookbook/data-persistence/#conversation-persistence","title":"Conversation Persistence","text":"<pre><code>from lionagi import Branch\n\n# Create conversation data model\nclass ConversationState(BaseModel):\n    branch_id: str\n    conversation_data: dict\n    message_count: int\n    last_updated: float\n\nclass ConversationNode(types.Node):\n    content: ConversationState\n\nConversationNode.register_async_adapter(LionAGIAsyncPostgresAdapter)\n\nasync def save_conversation(branch: Branch):\n    \"\"\"Save branch state to database\"\"\"\n    import time\n\n    conversation = ConversationNode(\n        content=ConversationState(\n            branch_id=str(branch.id),\n            conversation_data=branch.to_dict(),\n            message_count=len(branch.messages),\n            last_updated=time.time()\n        )\n    )\n\n    await conversation.adapt_to_async(\n        obj_key=\"lionagi_async_pg\",\n        dsn=dsn,\n        table=\"conversations\",\n    )\n    return conversation\n\nasync def load_conversation(branch_id: str) -&gt; Branch:\n    \"\"\"Load branch state from database\"\"\"\n    conversation = await ConversationNode.adapt_from_async(\n        {\"dsn\": dsn, \"table\": \"conversations\", \"selectors\": {\"branch_id\": branch_id}},\n        obj_key=\"lionagi_async_pg\",\n    )\n\n    return Branch.from_dict(conversation.content.conversation_data)\n\n# Usage\nbranch = Branch(system=\"You are a helpful assistant\")\nawait branch.communicate(\"Hello, how are you?\")\n\n# Save conversation\nsaved_conv = await save_conversation(branch)\nprint(f\"Saved conversation with {saved_conv.content.message_count} messages\")\n\n# Load conversation\nloaded_branch = await load_conversation(str(branch.id))\nprint(f\"Loaded conversation with {len(loaded_branch.messages)} messages\")\n</code></pre>"},{"location":"cookbook/data-persistence/#sqlite-pattern","title":"SQLite Pattern","text":"<pre><code># For local development with SQLite\nsqlite_dsn = \"sqlite+aiosqlite:///./conversations.db\"\n\n# Same Node patterns work with SQLite\nclass LocalStudent(types.Node):\n    content: StudentInfo\n\n# Use async SQLite adapter (if available) or regular sync operations\nasync def save_local(student: LocalStudent):\n    return await student.adapt_to_async(\n        obj_key=\"lionagi_async_sqlite\",  # Adapter key for SQLite\n        dsn=sqlite_dsn,\n        table=\"local_students\",\n    )\n</code></pre>"},{"location":"cookbook/data-persistence/#supabase-integration","title":"Supabase Integration","text":"<pre><code># For Supabase (managed PostgreSQL)\n# Set up project: supabase init &amp;&amp; supabase start\n\nsupabase_dsn = \"postgresql+asyncpg://postgres:postgres@127.0.0.1:54322/postgres\"\n\nclass SupabaseData(BaseModel):\n    title: str\n    content: str\n    created_by: str\n\nclass SupabaseNode(types.Node):\n    content: SupabaseData\n\nSupabaseNode.register_async_adapter(LionAGIAsyncPostgresAdapter)\n\nasync def supabase_operations():\n    # Create\n    node = SupabaseNode(\n        content=SupabaseData(\n            title=\"Research Paper\",\n            content=\"AI safety considerations...\",\n            created_by=\"researcher_001\"\n        )\n    )\n\n    # Save to Supabase\n    result = await node.adapt_to_async(\n        obj_key=\"lionagi_async_pg\",\n        dsn=supabase_dsn,\n        table=\"research_papers\",\n    )\n\n    # Query by creator\n    papers = await SupabaseNode.adapt_from_async(\n        {\n            \"dsn\": supabase_dsn, \n            \"table\": \"research_papers\",\n            \"selectors\": {\"created_by\": \"researcher_001\"}\n        },\n        obj_key=\"lionagi_async_pg\",\n        many=True,\n    )\n\n    return papers\n\n# Usage\npapers = await supabase_operations()\nprint(f\"Found {len(papers)} research papers\")\n</code></pre>"},{"location":"cookbook/data-persistence/#production-patterns","title":"Production Patterns","text":"<pre><code>import asyncio\nfrom contextlib import asynccontextmanager\n\nclass DatabaseManager:\n    def __init__(self, dsn: str):\n        self.dsn = dsn\n        self.pool = None\n\n    @asynccontextmanager\n    async def get_connection(self):\n        \"\"\"Connection pool management\"\"\"\n        try:\n            # Connection logic here\n            yield self.dsn\n        except Exception as e:\n            print(f\"Database error: {e}\")\n            raise\n        finally:\n            # Cleanup\n            pass\n\n# Error handling wrapper\nasync def safe_db_operation(operation, **kwargs):\n    \"\"\"Wrapper for database operations with error handling\"\"\"\n    try:\n        return await operation(**kwargs)\n    except Exception as e:\n        print(f\"Database operation failed: {e}\")\n        return None\n\n# Batch operations\nasync def batch_save(nodes: list[types.Node], table: str):\n    \"\"\"Save multiple nodes efficiently\"\"\"\n    tasks = []\n    for node in nodes:\n        task = safe_db_operation(\n            node.adapt_to_async,\n            obj_key=\"lionagi_async_pg\",\n            dsn=dsn,\n            table=table,\n        )\n        tasks.append(task)\n\n    results = await asyncio.gather(*tasks, return_exceptions=True)\n    successful = [r for r in results if r is not None]\n\n    print(f\"Saved {len(successful)}/{len(nodes)} records\")\n    return successful\n\n# Usage\nstudents = [\n    Student(content=StudentInfo(name=f\"Student {i}\", grade=\"A\", age=20+i))\n    for i in range(10)\n]\nresults = await batch_save(students, \"batch_students\")\n</code></pre>"},{"location":"cookbook/data-persistence/#key-patterns","title":"Key Patterns","text":"<p>Node Creation:</p> <ol> <li>Define Pydantic model for data structure</li> <li>Create Node class with content field</li> <li>Register appropriate async adapter</li> </ol> <p>Database Operations:</p> <ul> <li><code>adapt_to_async()</code> for save/update operations</li> <li><code>adapt_from_async()</code> for query operations</li> <li>Automatic table creation and schema management</li> <li>Support for PostgreSQL, SQLite, and Supabase</li> </ul> <p>Production Considerations:</p> <ul> <li>Connection pooling for performance</li> <li>Error handling with graceful degradation</li> <li>Batch operations for efficiency</li> <li>Proper async/await patterns throughout</li> </ul>"},{"location":"cookbook/hr-automation/","title":"HR Automation System","text":"<p>Multi-agent workflow with feedback loops for comprehensive HR operations.</p>"},{"location":"cookbook/hr-automation/#basic-hr-workflow","title":"Basic HR Workflow","text":"<pre><code>from lionagi import Branch\nimport json\n\n# Define HR system agents\nrecruiter = Branch(\n    system=\"You are an HR recruiter. Screen candidates and assess fit for roles.\",\n    name=\"recruiter\"\n)\n\ninterviewer = Branch(\n    system=\"You are a technical interviewer. Conduct interviews and evaluate skills.\",\n    name=\"interviewer\"  \n)\n\nmanager = Branch(\n    system=\"You are a hiring manager. Make final hiring decisions based on all feedback.\",\n    name=\"manager\"\n)\n\n# Initial candidate screening\ncandidate_profile = {\n    \"name\": \"Alice Johnson\",\n    \"experience\": \"5 years Python development\",\n    \"skills\": [\"Python\", \"FastAPI\", \"PostgreSQL\", \"Docker\"],\n    \"role\": \"Senior Backend Developer\"\n}\n\nasync def hr_workflow(candidate_profile: dict):\n    \"\"\"Complete HR workflow with feedback loops\"\"\"\n\n    # Step 1: Initial screening\n    screening = await recruiter.communicate(\n        \"Screen this candidate for the role\",\n        context=candidate_profile\n    )\n\n    # Step 2: Technical interview\n    interview_result = await interviewer.communicate(\n        \"Conduct technical evaluation based on recruiter screening\",\n        context={\"screening\": screening, \"candidate\": candidate_profile}\n    )\n\n    # Step 3: Feedback loop - recruiter reviews interview\n    recruiter_feedback = await recruiter.communicate(\n        \"Review the technical interview results and provide additional insights\",\n        context=interview_result\n    )\n\n    # Step 4: Manager decision\n    final_decision = await manager.communicate(\n        \"Make hiring decision based on all feedback\",\n        context={\n            \"candidate\": candidate_profile,\n            \"screening\": screening,\n            \"interview\": interview_result,\n            \"recruiter_feedback\": recruiter_feedback\n        }\n    )\n\n    return {\n        \"screening\": screening,\n        \"interview\": interview_result,\n        \"recruiter_feedback\": recruiter_feedback,\n        \"final_decision\": final_decision\n    }\n\n# Execute workflow\nresult = await hr_workflow(candidate_profile)\n</code></pre>"},{"location":"cookbook/hr-automation/#iterative-improvement-workflow","title":"Iterative Improvement Workflow","text":"<pre><code>from pydantic import BaseModel\nfrom typing import Literal\n\nclass CandidateEvaluation(BaseModel):\n    score: int  # 1-10\n    strengths: list[str]\n    concerns: list[str]\n    recommendation: Literal[\"hire\", \"reject\", \"second_interview\"]\n\nasync def iterative_hiring_process(candidate_profile: dict):\n    \"\"\"Multi-round evaluation with feedback improvements\"\"\"\n\n    # Round 1: Initial assessments\n    recruiter_eval = await recruiter.operate(\n        instruction=\"Evaluate candidate for role fit and cultural alignment\",\n        context=candidate_profile,\n        response_format=CandidateEvaluation\n    )\n\n    interviewer_eval = await interviewer.operate(\n        instruction=\"Technical skills assessment and problem-solving evaluation\", \n        context=candidate_profile,\n        response_format=CandidateEvaluation\n    )\n\n    # Round 2: Cross-feedback and refinement\n    recruiter_refined = await recruiter.communicate(\n        \"Review technical assessment and refine your evaluation\",\n        context={\n            \"your_evaluation\": recruiter_eval.model_dump(),\n            \"technical_assessment\": interviewer_eval.model_dump()\n        }\n    )\n\n    interviewer_refined = await interviewer.communicate(\n        \"Consider cultural fit assessment and adjust technical evaluation\",\n        context={\n            \"your_evaluation\": interviewer_eval.model_dump(),\n            \"cultural_assessment\": recruiter_eval.model_dump()\n        }\n    )\n\n    # Round 3: Final synthesis\n    final_assessment = await manager.operate(\n        instruction=\"Synthesize all evaluations into final hiring decision\",\n        context={\n            \"candidate\": candidate_profile,\n            \"recruiter_initial\": recruiter_eval.model_dump(),\n            \"interviewer_initial\": interviewer_eval.model_dump(),\n            \"recruiter_refined\": recruiter_refined,\n            \"interviewer_refined\": interviewer_refined\n        },\n        response_format=CandidateEvaluation\n    )\n\n    return final_assessment\n\n# Usage\nassessment = await iterative_hiring_process(candidate_profile)\nprint(f\"Final recommendation: {assessment.recommendation}\")\nprint(f\"Score: {assessment.score}/10\")\n</code></pre>"},{"location":"cookbook/hr-automation/#performance-review-system","title":"Performance Review System","text":"<pre><code>class PerformanceReview(BaseModel):\n    employee_id: str\n    overall_rating: int  # 1-5\n    achievements: list[str]\n    areas_for_improvement: list[str]\n    goals: list[str]\n    manager_feedback: str\n\n# Performance review agents\ndirect_manager = Branch(\n    system=\"You are a direct manager conducting performance reviews\",\n    name=\"direct_manager\"\n)\n\npeer_reviewer = Branch(\n    system=\"You provide peer feedback for performance reviews\",\n    name=\"peer_reviewer\"\n)\n\nhr_specialist = Branch(\n    system=\"You synthesize performance data and ensure consistency\",\n    name=\"hr_specialist\"\n)\n\nasync def performance_review_cycle(employee_data: dict):\n    \"\"\"360-degree performance review with multiple perspectives\"\"\"\n\n    # Manager assessment\n    manager_review = await direct_manager.operate(\n        instruction=\"Conduct comprehensive performance review\",\n        context=employee_data,\n        response_format=PerformanceReview\n    )\n\n    # Peer feedback\n    peer_feedback = await peer_reviewer.communicate(\n        \"Provide peer perspective on performance and collaboration\",\n        context=employee_data\n    )\n\n    # HR synthesis with manager input\n    hr_synthesis = await hr_specialist.communicate(\n        \"Review manager assessment and peer feedback for consistency\",\n        context={\n            \"employee\": employee_data,\n            \"manager_review\": manager_review.model_dump(),\n            \"peer_feedback\": peer_feedback\n        }\n    )\n\n    # Manager refinement based on HR feedback\n    final_review = await direct_manager.communicate(\n        \"Refine review based on HR synthesis and peer feedback\",\n        context=hr_synthesis\n    )\n\n    return {\n        \"initial_review\": manager_review,\n        \"peer_feedback\": peer_feedback,\n        \"hr_synthesis\": hr_synthesis,\n        \"final_review\": final_review\n    }\n\n# Employee data\nemployee = {\n    \"id\": \"EMP001\",\n    \"name\": \"Bob Smith\",\n    \"role\": \"Software Engineer\",\n    \"tenure\": \"2 years\",\n    \"recent_projects\": [\"API refactoring\", \"Database optimization\"],\n    \"peer_ratings\": [4, 5, 4, 3]\n}\n\nreview_results = await performance_review_cycle(employee)\n</code></pre>"},{"location":"cookbook/hr-automation/#policy-consultation-system","title":"Policy Consultation System","text":"<pre><code># HR policy consultant with tools\ndef lookup_policy(policy_area: str) -&gt; str:\n    \"\"\"Look up HR policies by area\"\"\"\n    policies = {\n        \"vacation\": \"Employees accrue 2 weeks vacation per year...\",\n        \"remote_work\": \"Remote work approved for up to 3 days per week...\",\n        \"benefits\": \"Health insurance starts after 30 days...\",\n        \"performance\": \"Reviews conducted quarterly with annual ratings...\"\n    }\n    return policies.get(policy_area, \"Policy not found\")\n\ndef check_compliance(situation: str) -&gt; str:\n    \"\"\"Check compliance requirements\"\"\"\n    return f\"Compliance check for: {situation}\"\n\n# Policy-aware HR agent\npolicy_agent = Branch(\n    system=\"You are an HR policy expert. Use tools to look up policies and check compliance.\",\n    tools=[lookup_policy, check_compliance],\n    name=\"policy_agent\"\n)\n\n# Employee inquiry agent  \nemployee_support = Branch(\n    system=\"You help employees with HR questions and concerns.\",\n    name=\"employee_support\"\n)\n\nasync def hr_inquiry_workflow(employee_question: str):\n    \"\"\"Handle employee inquiries with policy consultation\"\"\"\n\n    # Initial response from support agent\n    initial_response = await employee_support.communicate(employee_question)\n\n    # Policy consultation using tools\n    policy_guidance = await policy_agent.ReAct(\n        instruct={\"instruction\": f\"Research policy guidance for: {employee_question}\"},\n        max_extensions=3\n    )\n\n    # Refined response with policy backing\n    final_response = await employee_support.communicate(\n        \"Provide comprehensive response using policy guidance\",\n        context={\n            \"original_question\": employee_question,\n            \"initial_response\": initial_response,\n            \"policy_guidance\": policy_guidance\n        }\n    )\n\n    return final_response\n\n# Usage\nquestion = \"Can I work remotely 4 days per week for family reasons?\"\nresponse = await hr_inquiry_workflow(question)\n</code></pre>"},{"location":"cookbook/hr-automation/#onboarding-workflow","title":"Onboarding Workflow","text":"<pre><code># Onboarding specialist agents\nonboarding_coordinator = Branch(\n    system=\"You coordinate new employee onboarding processes\",\n    name=\"coordinator\"\n)\n\nit_setup = Branch(\n    system=\"You handle IT setup and access provisioning for new employees\",\n    name=\"it_setup\"\n)\n\nbuddy_matcher = Branch(\n    system=\"You match new employees with suitable workplace buddies\",\n    name=\"buddy_matcher\"\n)\n\nasync def onboarding_process(new_employee: dict):\n    \"\"\"Complete onboarding with multi-agent coordination\"\"\"\n\n    # Phase 1: Initial planning\n    onboarding_plan = await onboarding_coordinator.operate(\n        instruction=\"Create onboarding plan for new employee\",\n        context=new_employee,\n        response_format=dict\n    )\n\n    # Phase 2: Parallel setup tasks\n    it_tasks = await it_setup.communicate(\n        \"Generate IT setup checklist and timeline\",\n        context={\"employee\": new_employee, \"plan\": onboarding_plan}\n    )\n\n    buddy_match = await buddy_matcher.communicate(\n        \"Find suitable workplace buddy based on role and interests\",\n        context={\"employee\": new_employee, \"plan\": onboarding_plan}\n    )\n\n    # Phase 3: Coordination feedback\n    coordination_update = await onboarding_coordinator.communicate(\n        \"Review setup progress and adjust plan as needed\",\n        context={\n            \"original_plan\": onboarding_plan,\n            \"it_progress\": it_tasks,\n            \"buddy_assignment\": buddy_match\n        }\n    )\n\n    # Phase 4: Final checklist\n    final_checklist = await onboarding_coordinator.communicate(\n        \"Generate final onboarding checklist with all requirements\"\n    )\n\n    return {\n        \"plan\": onboarding_plan,\n        \"it_setup\": it_tasks,\n        \"buddy_match\": buddy_match,\n        \"updates\": coordination_update,\n        \"checklist\": final_checklist\n    }\n\n# New employee data\nnew_hire = {\n    \"name\": \"Carol Davis\",\n    \"role\": \"Product Manager\",\n    \"start_date\": \"2024-02-01\",\n    \"department\": \"Product\",\n    \"manager\": \"Alice Johnson\",\n    \"location\": \"Remote\",\n    \"experience_level\": \"Senior\"\n}\n\nonboarding_result = await onboarding_process(new_hire)\n</code></pre>"},{"location":"cookbook/hr-automation/#state-persistence","title":"State Persistence","text":"<pre><code>async def save_hr_workflow_state():\n    \"\"\"Save conversation states for audit and continuity\"\"\"\n\n    # Save all agent conversations\n    agent_states = {}\n\n    for agent_name, agent in [\n        (\"recruiter\", recruiter),\n        (\"interviewer\", interviewer), \n        (\"manager\", manager)\n    ]:\n        # Convert to dict for persistence\n        state = agent.to_dict()\n\n        # Save to file or database\n        with open(f\"hr_data/{agent_name}_state.json\", \"w\") as f:\n            json.dump(state, f, indent=2)\n\n        agent_states[agent_name] = state\n\n    return agent_states\n\nasync def load_hr_workflow_state():\n    \"\"\"Restore agent states for continued processing\"\"\"\n\n    restored_agents = {}\n\n    for agent_name in [\"recruiter\", \"interviewer\", \"manager\"]:\n        try:\n            with open(f\"hr_data/{agent_name}_state.json\") as f:\n                state_data = json.load(f)\n\n            # Restore agent from saved state\n            agent = Branch.from_dict(state_data)\n            restored_agents[agent_name] = agent\n\n        except FileNotFoundError:\n            print(f\"No saved state for {agent_name}\")\n\n    return restored_agents\n\n# Usage\nawait save_hr_workflow_state()\nrestored_agents = await load_hr_workflow_state()\n</code></pre>"},{"location":"cookbook/hr-automation/#key-benefits","title":"Key Benefits","text":"<p>Multi-Agent Collaboration:</p> <ul> <li>Specialized roles with domain expertise</li> <li>Natural feedback loops between agents</li> <li>Iterative improvement of decisions</li> </ul> <p>Feedback Integration:</p> <ul> <li>Cross-agent review and refinement</li> <li>Multiple perspectives on each decision</li> <li>Continuous improvement through iteration</li> </ul> <p>State Management:</p> <ul> <li>Complete audit trail of decisions</li> <li>Ability to pause/resume workflows</li> <li>Historical analysis of HR patterns</li> </ul> <p>Production Features:</p> <ul> <li>Policy integration with tool usage</li> <li>Compliance checking and documentation</li> <li>Scalable workflow orchestration</li> </ul>"},{"location":"cookbook/research-synthesis/","title":"Research Synthesis","text":"<p>Parallel research with expert synthesis - the natural pattern for comprehensive analysis.</p>"},{"location":"cookbook/research-synthesis/#basic-research-synthesis","title":"Basic Research Synthesis","text":"<pre><code>from lionagi import Branch, Builder, Session, iModel\nfrom lionagi.fields import LIST_INSTRUCT_FIELD_MODEL, Instruct\nfrom lionagi.protocols.types import AssistantResponse\n\n# Setup orchestrator\norchestrator = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4.1-mini\"),\n    system=\"Break research into parallel assignments and synthesize findings\"\n)\nsession = Session(default_branch=orchestrator)\nbuilder = Builder(\"ResearchSynthesis\")\n\ntopic = \"AI safety in production systems\"\n\n# Decomposition phase\nroot = builder.add_operation(\n    \"operate\",\n    instruct=Instruct(\n        instruction=f\"Create 3-4 research assignments for: {topic}\",\n        context=topic\n    ),\n    reason=True,\n    field_models=[LIST_INSTRUCT_FIELD_MODEL]\n)\n\n# Execute decomposition\nresult = await session.flow(builder.get_graph())\ninstruct_models = result[\"operation_results\"][root].instruct_models\n\n# Fan-out: Create researchers\nresearch_nodes = []\nfor i, instruction in enumerate(instruct_models):\n    researcher = Branch(\n        chat_model=iModel(provider=\"openai\", model=\"gpt-4.1-mini\"),\n        system=f\"Research specialist #{i+1} - focused domain expert\"\n    )\n\n    node = builder.add_operation(\n        \"communicate\",\n        depends_on=[root],\n        branch=researcher,\n        **instruction.to_dict()\n    )\n    research_nodes.append(node)\n\n# Execute research\nawait session.flow(builder.get_graph())\n\n# Extract findings with cost tracking\ncosts = 0\ndef get_context(node_id):\n    global costs\n    graph = builder.get_graph()\n    node = graph.internal_nodes[node_id]\n    branch = session.get_branch(node.branch_id, None)\n    if (branch and len(branch.messages) &gt; 0 and \n        isinstance(msg := branch.messages[-1], AssistantResponse)):\n        costs += msg.model_response.get(\"total_cost_usd\") or 0\n        return f\"\"\"\nResponse: {msg.model_response.get(\"result\") or \"Not available\"}\nSummary: {msg.model_response.get(\"summary\") or \"Not available\"}\n        \"\"\".strip()\n\nctx = [get_context(i) for i in research_nodes]\n\n# Fan-in: Synthesize\nsynthesis = builder.add_operation(\n    \"communicate\",\n    depends_on=research_nodes,\n    branch=orchestrator,\n    instruction=\"Synthesize research findings into comprehensive analysis\",\n    context=[i for i in ctx if i is not None]\n)\n\nfinal_result = await session.flow(builder.get_graph())\nprint(f\"Research complete. Total cost: ${costs:.4f}\")\n</code></pre>"},{"location":"cookbook/research-synthesis/#literature-review","title":"Literature Review","text":"<pre><code># Literature review orchestrator\norchestrator = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4.1-mini\"),\n    system=\"Literature review coordinator and synthesizer\"\n)\n\npapers = [\"Attention is All You Need\", \"BERT\", \"GPT-3\"]\nfocus = \"transformer architecture evolution\"\n\n# Generate review plan\nplanning = builder.add_operation(\n    \"operate\",\n    branch=orchestrator,\n    instruct=Instruct(\n        instruction=f\"Create analysis framework for: {focus}\",\n        context={\"papers\": papers, \"focus\": focus}\n    ),\n    field_models=[LIST_INSTRUCT_FIELD_MODEL]\n)\n\nresult = await session.flow(builder.get_graph())\nreview_tasks = result[\"operation_results\"][planning].instruct_models\n\n# Parallel paper analysis\nreview_nodes = []\nfor task in review_tasks:\n    reviewer = Branch(\n        chat_model=iModel(provider=\"openai\", model=\"gpt-4.1-mini\"),\n        system=\"Academic paper analysis specialist\"\n    )\n\n    node = builder.add_operation(\n        \"communicate\",\n        depends_on=[planning],\n        branch=reviewer,\n        **task.to_dict()\n    )\n    review_nodes.append(node)\n\nawait session.flow(builder.get_graph())\n\n# Synthesis\nsynthesis = builder.add_operation(\n    \"communicate\",\n    depends_on=review_nodes,\n    branch=orchestrator,\n    instruction=\"Create comprehensive literature review synthesis\"\n)\n\nfinal_result = await session.flow(builder.get_graph())\n</code></pre>"},{"location":"cookbook/research-synthesis/#cost-efficient-research","title":"Cost-Efficient Research","text":"<pre><code># Budget-aware research\ntotal_cost = 0\nmax_cost = 1.0\n\ncoordinator = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4.1-mini\"),\n    system=\"Efficient research coordinator\"\n)\n\n# Budget check before synthesis\ndef track_costs(node_id):\n    global total_cost\n    graph = builder.get_graph()\n    node = graph.internal_nodes[node_id]\n    branch = session.get_branch(node.branch_id, None)\n    if (branch and len(branch.messages) &gt; 0 and \n        isinstance(msg := branch.messages[-1], AssistantResponse)):\n        cost = msg.model_response.get(\"total_cost_usd\", 0)\n        total_cost += cost\n        return cost &gt; 0\n    return False\n\nawait session.flow(builder.get_graph())\n\nif total_cost &lt; max_cost:\n    synthesis = builder.add_operation(\n        \"communicate\",\n        depends_on=research_nodes,\n        branch=coordinator,\n        instruction=\"Synthesize research findings efficiently\"\n    )\n    final_result = await session.flow(builder.get_graph())\n    print(f\"Research completed. Cost: ${total_cost:.4f}\")\nelse:\n    print(f\"Budget exceeded: ${total_cost:.4f}\")\n</code></pre>"},{"location":"cookbook/research-synthesis/#production-error-handling","title":"Production Error Handling","text":"<pre><code>try:\n    # Research execution with error handling\n    final_result = await session.flow(builder.get_graph())\n    print(f\"Research complete. Total cost: ${costs:.4f}\")\n\nexcept Exception as e:\n    print(f\"Research failed: {e}\")\n    import traceback\n    traceback.print_exc()\n</code></pre>"},{"location":"cookbook/research-synthesis/#when-to-use","title":"When to Use","text":"<p>Perfect for: Complex topics, literature reviews, market research, technical analysis requiring domain expertise</p> <p>Execution Flow: Task Decomposition \u2192 Parallel Researchers \u2192 Context Extraction \u2192 Synthesis</p> <p>Key Benefits:</p> <ul> <li>3-4x faster than sequential research</li> <li>Higher quality through diverse perspectives</li> <li>Systematic evidence gathering</li> <li>Cost-efficient parallel execution</li> </ul> <p>Research synthesis leverages the fan-out/fan-in pattern for comprehensive analysis through specialized parallel research with intelligent synthesis.</p>"},{"location":"core-concepts/","title":"Core Concepts","text":"<p>Core Concepts</p> <p>Understanding these abstractions is the foundation for building effective multi-agent workflows with LionAGI.</p> <p>Understanding LionAGI's core abstractions is essential for building effective multi-agent workflows. These concepts work together to provide the parallel execution, memory isolation, and flexible orchestration that make LionAGI powerful.</p>"},{"location":"core-concepts/#key-abstractions","title":"Key Abstractions","text":"<p>Learning Order</p> <p>Read these in order for best understanding: 1. Sessions &amp; Branches (the foundation) 2. Operations (how work gets done) 3. Messages &amp; Memory (how context works)</p>"},{"location":"core-concepts/#sessions-and-branches","title":"Sessions and Branches","text":"<ul> <li>Session: Workspace that coordinates multiple agents</li> <li>Branch: Individual agent with memory and tools</li> </ul>"},{"location":"core-concepts/#operations","title":"Operations","text":"<ul> <li>Building blocks of workflows</li> <li>Types: chat, communicate, operate, ReAct</li> </ul>"},{"location":"core-concepts/#messages-and-memory","title":"Messages and Memory","text":"<ul> <li>How conversation state is managed</li> <li>Memory isolation between branches</li> </ul>"},{"location":"core-concepts/#tools-and-functions","title":"Tools and Functions","text":"<ul> <li>Extending agents with capabilities</li> <li>Built-in and custom tools</li> </ul>"},{"location":"core-concepts/#models-and-providers","title":"Models and Providers","text":"<ul> <li>iModel abstraction for LLM providers</li> <li>Supporting OpenAI, Anthropic, Ollama, etc.</li> </ul>"},{"location":"core-concepts/#the-mental-model","title":"The Mental Model","text":"<pre><code># Traditional: Agents talk to each other\nagent1 \u2192 agent2 \u2192 agent3 \u2192 result\n\n# LionAGI: Agents work in parallel, results synthesized\nagent1 \u2198\nagent2 \u2192 synthesis \u2192 result\nagent3 \u2197\n</code></pre>"},{"location":"core-concepts/#architecture","title":"Architecture","text":"<pre><code>Session (Workspace)\n\u251c\u2500\u2500 Branch (Agent 1)\n\u2502   \u251c\u2500\u2500 Messages (Memory)\n\u2502   \u251c\u2500\u2500 Tools\n\u2502   \u2514\u2500\u2500 Model Config\n\u251c\u2500\u2500 Branch (Agent 2)\n\u2502   \u251c\u2500\u2500 Messages (Memory)\n\u2502   \u251c\u2500\u2500 Tools\n\u2502   \u2514\u2500\u2500 Model Config\n\u2514\u2500\u2500 Graph (Workflow)\n    \u251c\u2500\u2500 Operations\n    \u2514\u2500\u2500 Dependencies\n</code></pre> <p>Ready to Build Workflows?</p> <p>Now that you understand the core concepts, it's time to see them in action:</p> <p>Next: Patterns - Learn proven multi-agent workflow patterns Or: Cookbook - Jump to complete working examples</p>"},{"location":"core-concepts/lionagi-philosophy/","title":"LionAGI Philosophy: Observable Workflows","text":""},{"location":"core-concepts/lionagi-philosophy/#the-core-idea","title":"The Core Idea","text":"<p>LionAGI puts you in control. Instead of hiding LLM interactions behind opaque \"agent\" abstractions, LionAGI gives you observable, composable building blocks. You define the workflow. You control the flow. Every step is visible, testable, and reproducible.</p> <pre><code>from lionagi import Branch, iModel\n\nbranch = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4.1-mini\"),\n    system=\"You are a financial analyst.\"\n)\n\n# Every operation is explicit and observable\nresponse = await branch.communicate(\"Summarize Q4 earnings for NVDA\")\n</code></pre>"},{"location":"core-concepts/lionagi-philosophy/#three-layers-of-abstraction","title":"Three Layers of Abstraction","text":"<p>LionAGI is built on three layers, each adding structure on top of the previous one:</p> <pre><code>Layer 1: iModel           -- Provider-agnostic LLM interface\nLayer 2: Branch            -- Single conversation thread with tools\nLayer 3: Session + Builder -- Multi-branch coordination via DAG workflows\n</code></pre> <p>You can use any layer independently. Most applications only need Layer 2.</p>"},{"location":"core-concepts/lionagi-philosophy/#branch-the-primary-cognitive-unit","title":"Branch: The Primary Cognitive Unit","text":"<p>A <code>Branch</code> is a single conversation thread with its own message history, tools, and model configuration. It is the primary surface for all LLM operations.</p>"},{"location":"core-concepts/lionagi-philosophy/#available-operations","title":"Available Operations","text":"<p>Every Branch supports these operations, each with a specific purpose:</p> Operation Purpose Messages added? <code>chat()</code> Raw LLM call; returns response without adding to history No <code>communicate()</code> Conversational exchange with optional structured output Yes <code>operate()</code> Tool calling with structured response validation Yes <code>parse()</code> Parse raw text into a Pydantic model No <code>ReAct()</code> Multi-step reasoning with tool use (think-act-observe) Yes <code>select()</code> Choose from a set of options Yes <code>interpret()</code> Rewrite user input into a clearer prompt No <code>act()</code> Execute tool calls directly Yes"},{"location":"core-concepts/lionagi-philosophy/#chat-raw-llm-interaction","title":"chat() -- Raw LLM Interaction","text":"<p>The simplest operation. Sends a message and gets a response. Does not add messages to the conversation history, making it suitable for one-off queries or when you want manual control.</p> <pre><code>from lionagi import Branch\n\nbranch = Branch(system=\"Expert physicist\")\n\n# Returns the response content directly\nresponse = await branch.chat(\"Explain dark matter in one paragraph\")\nprint(response)\n</code></pre>"},{"location":"core-concepts/lionagi-philosophy/#communicate-conversational-exchange","title":"communicate() -- Conversational Exchange","text":"<p>Like <code>chat()</code>, but automatically adds messages to the branch's conversation history. Supports structured output via <code>response_format</code>.</p> <pre><code>from pydantic import BaseModel\nfrom lionagi import Branch\n\nclass Summary(BaseModel):\n    key_points: list[str]\n    sentiment: str\n\nbranch = Branch(system=\"News analyst\")\n\n# Messages are added to history automatically\nresult = await branch.communicate(\n    \"Summarize the latest AI regulation developments\",\n    response_format=Summary\n)\nprint(result.key_points)\n\n# Follow-up questions reference conversation history\nfollow_up = await branch.communicate(\"Which point is most impactful?\")\n</code></pre>"},{"location":"core-concepts/lionagi-philosophy/#operate-tool-calling-with-structured-output","title":"operate() -- Tool Calling with Structured Output","text":"<p>Combines LLM interaction with automatic tool invocation and response validation. This is the primary operation for building agents that use tools.</p> <pre><code>from pydantic import BaseModel\nfrom lionagi import Branch\n\ndef search_database(query: str, limit: int = 10) -&gt; list[dict]:\n    \"\"\"Search the product database.\"\"\"\n    return [{\"name\": \"Widget A\", \"price\": 9.99}]\n\ndef calculate_discount(price: float, percent: float) -&gt; float:\n    \"\"\"Calculate discounted price.\"\"\"\n    return price * (1 - percent / 100)\n\nclass PricingResult(BaseModel):\n    product: str\n    original_price: float\n    discounted_price: float\n    recommendation: str\n\nbranch = Branch(\n    system=\"Pricing analyst with database access\",\n    tools=[search_database, calculate_discount]\n)\n\nresult = await branch.operate(\n    instruction=\"Find Widget A and calculate a 15% discount\",\n    response_format=PricingResult,\n    reason=True  # Include chain-of-thought reasoning\n)\nprint(result.recommendation)\n</code></pre>"},{"location":"core-concepts/lionagi-philosophy/#parse-structured-text-extraction","title":"parse() -- Structured Text Extraction","text":"<p>Parses raw text into a Pydantic model. Does not add messages to the conversation. Useful for processing LLM output or external text into structured data.</p> <pre><code>from pydantic import BaseModel\nfrom lionagi import Branch\n\nclass ContactInfo(BaseModel):\n    name: str\n    email: str\n    phone: str | None = None\n\nbranch = Branch()\n\nraw_text = \"John Smith, email: john@example.com, phone: 555-0123\"\ncontact = await branch.parse(raw_text, response_format=ContactInfo)\nprint(contact.name)  # \"John Smith\"\n</code></pre>"},{"location":"core-concepts/lionagi-philosophy/#react-multi-step-reasoning","title":"ReAct() -- Multi-Step Reasoning","text":"<p>Implements the ReAct (Reason + Act) paradigm. The model thinks about what to do, takes an action (tool call), observes the result, and repeats until it reaches a conclusion. Best for complex tasks that require multiple steps.</p> <pre><code>from lionagi import Branch\n\nbranch = Branch(\n    system=\"Research assistant\",\n    tools=[web_search, summarize_page, extract_data]\n)\n\nresult = await branch.ReAct(\n    instruct={\n        \"instruction\": \"Research the top 3 renewable energy trends in 2025\",\n        \"guidance\": \"Use web search, then summarize findings\"\n    },\n    extension_allowed=True,\n    max_extensions=3,\n    verbose=True\n)\n</code></pre>"},{"location":"core-concepts/lionagi-philosophy/#imodel-provider-agnostic-interface","title":"iModel: Provider-Agnostic Interface","text":"<p><code>iModel</code> wraps any LLM provider behind a uniform interface. You configure it once and use it everywhere.</p> <pre><code>from lionagi import iModel\n\n# API-based providers\nopenai_model = iModel(provider=\"openai\", model=\"gpt-4.1-mini\")\nclaude_model = iModel(provider=\"anthropic\", model=\"claude-sonnet-4-5-20250929\")\ngemini_model = iModel(provider=\"gemini\", model=\"gemini-2.5-flash\")\n\n# CLI-based providers (agentic coding tools)\nclaude_code = iModel(provider=\"claude_code\")\ngemini_cli = iModel(provider=\"gemini_code\")\ncodex_cli = iModel(provider=\"codex\")\n\n# Use as async context manager for automatic cleanup\nasync with iModel(provider=\"openai\", model=\"gpt-4.1\") as model:\n    branch = Branch(chat_model=model)\n    result = await branch.communicate(\"Hello\")\n</code></pre>"},{"location":"core-concepts/lionagi-philosophy/#session-multi-branch-coordination","title":"Session: Multi-Branch Coordination","text":"<p>A <code>Session</code> manages multiple branches and provides <code>flow()</code> for executing DAG-based workflows.</p> <pre><code>from lionagi import Session\n\nsession = Session()\n\n# Create specialized branches\nresearcher = session.new_branch(name=\"researcher\", system=\"Research expert\")\nwriter = session.new_branch(name=\"writer\", system=\"Technical writer\")\n\n# Coordinate between branches\nfindings = await researcher.communicate(\"Research quantum error correction\")\nreport = await writer.communicate(f\"Write a summary based on: {findings}\")\n</code></pre>"},{"location":"core-concepts/lionagi-philosophy/#builder-flow-dag-based-workflows","title":"Builder + flow(): DAG-Based Workflows","text":"<p>For workflows with dependencies, parallel steps, and aggregation, use <code>Builder</code> to construct a directed acyclic graph (DAG) of operations, then execute it with <code>session.flow()</code>.</p> <pre><code>from lionagi import Session, Builder\n\nsession = Session()\n\n# Create branches for different roles\nanalyzer = session.new_branch(name=\"analyzer\", system=\"Data analyst\")\nreviewer = session.new_branch(name=\"reviewer\", system=\"Code reviewer\")\n\n# Build a workflow graph\nbuilder = Builder(\"analysis_pipeline\")\n\n# Step 1: Two parallel analyses\nstep_a = builder.add_operation(\n    \"communicate\",\n    instruction=\"Analyze performance metrics\",\n    branch=analyzer\n)\n\nstep_b = builder.add_operation(\n    \"communicate\",\n    instruction=\"Review code quality\",\n    branch=reviewer\n)\n\n# Step 2: Aggregate results (depends on both step_a and step_b)\nbuilder.add_aggregation(\n    \"communicate\",\n    instruction=\"Synthesize analysis and review findings\",\n    source_node_ids=[step_a, step_b],\n    branch=analyzer\n)\n\n# Execute the workflow\nresult = await session.flow(\n    graph=builder.get_graph(),\n    parallel=True,\n    max_concurrent=5\n)\n</code></pre>"},{"location":"core-concepts/lionagi-philosophy/#dynamic-graph-expansion","title":"Dynamic Graph Expansion","text":"<p>The Builder supports incremental workflows where you expand the graph based on intermediate results:</p> <pre><code>from lionagi import Session, Builder\n\nsession = Session()\nbuilder = Builder(\"dynamic_workflow\")\n\n# Initial operation\ngen_id = builder.add_operation(\n    \"operate\",\n    instruction=\"Generate 3 research questions about climate change\",\n    response_format=ResearchQuestions\n)\n\n# Execute initial graph\nresult = await session.flow(graph=builder.get_graph())\n\n# Expand based on results\nif hasattr(result, \"questions\"):\n    builder.expand_from_result(\n        items=result.questions,\n        source_node_id=gen_id,\n        operation=\"communicate\",\n    )\n\n# Execute expanded graph\nfinal = await session.flow(graph=builder.get_graph())\n</code></pre>"},{"location":"core-concepts/lionagi-philosophy/#design-principles","title":"Design Principles","text":""},{"location":"core-concepts/lionagi-philosophy/#1-you-control-the-flow","title":"1. You Control the Flow","text":"<p>LionAGI does not impose an agent loop or conversation pattern. You write the control flow. You decide when to call the LLM, which tools to invoke, and how to combine results.</p> <pre><code># You write the loop, not the framework\nfor document in documents:\n    summary = await branch.communicate(f\"Summarize: {document}\")\n    if \"urgent\" in summary.lower():\n        alert = await branch.operate(\n            instruction=f\"Draft alert for: {summary}\",\n            response_format=AlertMessage\n        )\n        send_alert(alert)\n</code></pre>"},{"location":"core-concepts/lionagi-philosophy/#2-observable-state","title":"2. Observable State","text":"<p>Every Branch exposes its state -- messages, logs, tools, models -- as inspectable properties.</p> <pre><code># Inspect conversation history\nfor msg in branch.messages:\n    print(f\"[{msg.role}] {msg.content[:80]}...\")\n\n# Check registered tools\nprint(branch.tools.keys())\n\n# Access logs\nfor log in branch.logs:\n    print(log.content)\n</code></pre>"},{"location":"core-concepts/lionagi-philosophy/#3-composition-over-configuration","title":"3. Composition Over Configuration","text":"<p>Build complex systems by composing simple parts. Branches are independent units that can be combined through explicit coordination.</p> <pre><code># Clone a branch to fork a conversation\nclone = branch.clone()\n\n# Original and clone have independent histories\nawait branch.communicate(\"Explore option A\")\nawait clone.communicate(\"Explore option B\")\n\n# Compare results\n</code></pre>"},{"location":"core-concepts/lionagi-philosophy/#4-async-context-managers-for-resource-safety","title":"4. Async Context Managers for Resource Safety","text":"<p>Both <code>Branch</code> and <code>iModel</code> support async context managers that handle cleanup automatically.</p> <pre><code>async with Branch(system=\"Temporary assistant\") as branch:\n    result = await branch.communicate(\"Help me with this task\")\n    # Logs are automatically flushed on exit\n</code></pre>"},{"location":"core-concepts/lionagi-philosophy/#summary","title":"Summary","text":"Concept What It Does <code>iModel</code> Wraps any LLM provider behind a uniform async interface <code>Branch</code> Single conversation thread with messages, tools, and models <code>Session</code> Manages multiple branches for coordination <code>Builder</code> Constructs DAG workflows for <code>session.flow()</code> execution <p>LionAGI gives you the building blocks. You build the workflow.</p>"},{"location":"core-concepts/messages-and-memory/","title":"Messages and Memory","text":"<p>LionAGI manages conversation state through a structured message system. Understanding which operations add to history and how messages are organized is essential for building reliable workflows.</p>"},{"location":"core-concepts/messages-and-memory/#the-key-distinction-chat-vs-communicate","title":"The Key Distinction: chat() vs communicate()","text":"<p>The most important thing to understand about LionAGI's memory model:</p> <ul> <li><code>chat()</code> does not add messages to conversation history. It reads the existing history for context but leaves it unchanged.</li> <li><code>communicate()</code> does add both the instruction and the response to history.</li> </ul> <pre><code>from lionagi import Branch\n\nbranch = Branch(system=\"You are a helpful assistant\")\n\n# chat() -- stateless, no history changes\nresponse = await branch.chat(\"What is 2 + 2?\")\nprint(len(branch.messages))  # 1 (only the system message)\n\n# communicate() -- adds to history\nresponse = await branch.communicate(\"What is 2 + 2?\")\nprint(len(branch.messages))  # 3 (system + instruction + response)\n\n# Second communicate() sees the full conversation\nresponse = await branch.communicate(\"What about 3 + 3?\")\nprint(len(branch.messages))  # 5 (system + 2 instructions + 2 responses)\n</code></pre> <p>Common Mistake</p> <p>If you use <code>chat()</code> expecting the branch to remember previous exchanges, it will not work. The model receives existing history as context, but the new instruction and response are never stored. Use <code>communicate()</code> for stateful conversations.</p> <p>Other operations that add to history: <code>communicate()</code>, <code>operate()</code>, <code>ReAct()</code>, <code>act()</code></p> <p>Operations that do not add to history: <code>chat()</code>, <code>parse()</code>, <code>interpret()</code></p>"},{"location":"core-concepts/messages-and-memory/#message-types","title":"Message Types","text":"<p>All messages inherit from <code>RoledMessage</code>, which extends <code>Node</code> (and therefore <code>Element</code>). Each message has a UUID, timestamp, role, structured content, sender, and recipient.</p>"},{"location":"core-concepts/messages-and-memory/#roledmessage-hierarchy","title":"RoledMessage Hierarchy","text":"<pre><code>RoledMessage (base)\n  |-- System          -- Sets conversation context and behavior\n  |-- Instruction     -- User input (instructions, context, images)\n  |-- AssistantResponse -- LLM replies\n  |-- ActionRequest   -- Tool call from the LLM\n  |-- ActionResponse  -- Tool execution result\n</code></pre>"},{"location":"core-concepts/messages-and-memory/#system","title":"System","text":"<p>Sets the overall behavior and context for the conversation. Created when you pass <code>system=</code> to <code>Branch()</code>.</p> <pre><code>branch = Branch(system=\"You are a financial analyst. Be precise with numbers.\")\n\n# Access the system message\nprint(branch.system.content.system_message)\nprint(branch.system.role)  # MessageRole.SYSTEM\n</code></pre> <p>System messages support optional datetime stamps:</p> <pre><code>branch = Branch(\n    system=\"You are a helpful assistant\",\n    system_datetime=True,  # Adds current timestamp\n)\n</code></pre>"},{"location":"core-concepts/messages-and-memory/#instruction","title":"Instruction","text":"<p>Represents user input. Contains structured fields for instruction text, guidance, context, tool schemas, response format, and images.</p> <pre><code>from lionagi.protocols.messages import Instruction\n\n# Accessing instruction content\nfor msg in branch.messages:\n    if isinstance(msg, Instruction):\n        print(msg.content.instruction)\n        print(msg.content.guidance)\n        print(msg.content.prompt_context)\n</code></pre>"},{"location":"core-concepts/messages-and-memory/#assistantresponse","title":"AssistantResponse","text":"<p>Wraps the LLM's reply. The extracted text is in <code>content.assistant_response</code>, while the raw provider response is stored in <code>metadata[\"model_response\"]</code>.</p> <pre><code>from lionagi.protocols.messages import AssistantResponse\n\n# Get the last response\nlast = branch.msgs.last_response\nif last:\n    print(last.response)          # Extracted text\n    print(last.model_response)    # Raw API response dict\n</code></pre>"},{"location":"core-concepts/messages-and-memory/#actionrequest-and-actionresponse","title":"ActionRequest and ActionResponse","text":"<p>These represent tool calls. An <code>ActionRequest</code> contains the function name and arguments the LLM wants to invoke. An <code>ActionResponse</code> contains the result.</p> <pre><code>from lionagi.protocols.messages import ActionRequest, ActionResponse\n\n# ActionRequest -- created when the LLM requests a tool call\n# request.function -&gt; \"search_database\"\n# request.arguments -&gt; {\"query\": \"revenue\"}\n\n# ActionResponse -- created after tool execution\n# response.function -&gt; \"search_database\"\n# response.output -&gt; [{\"id\": 1, \"revenue\": 50000}]\n</code></pre> <p>Action requests and responses are linked: <code>ActionRequest.content.action_response_id</code> points to the response, and <code>ActionResponse.content.action_request_id</code> points back to the request.</p>"},{"location":"core-concepts/messages-and-memory/#the-messagemanager","title":"The MessageManager","text":"<p>The <code>MessageManager</code> (accessible via <code>branch.msgs</code>) stores messages in a <code>Pile</code> (an O(1) dict-keyed collection) with a <code>Progression</code> that tracks ordering.</p> <pre><code># Access the manager\nmanager = branch.msgs\n\n# Convenience properties\nmanager.last_response        # Most recent AssistantResponse\nmanager.last_instruction     # Most recent Instruction\nmanager.assistant_responses  # Pile of all AssistantResponses\nmanager.instructions         # Pile of all Instructions\nmanager.action_requests      # Pile of all ActionRequests\nmanager.action_responses     # Pile of all ActionResponses\n</code></pre>"},{"location":"core-concepts/messages-and-memory/#adding-messages-manually","title":"Adding Messages Manually","text":"<p>While operations handle this automatically, you can add messages directly:</p> <pre><code>branch.msgs.add_message(\n    instruction=\"Manual instruction\",\n    context=[\"some context\"],\n    sender=\"user\",\n    recipient=branch.id,\n)\n\nbranch.msgs.add_message(\n    assistant_response=\"Manual response\",\n    sender=branch.id,\n)\n</code></pre>"},{"location":"core-concepts/messages-and-memory/#clearing-history","title":"Clearing History","text":"<pre><code># Remove all messages except the system message\nbranch.msgs.clear_messages()\n</code></pre>"},{"location":"core-concepts/messages-and-memory/#converting-to-chat-format","title":"Converting to Chat Format","text":"<p>The <code>to_chat_msgs()</code> method converts messages to the standard <code>[{\"role\": ..., \"content\": ...}]</code> format used by LLM APIs:</p> <pre><code>chat_msgs = branch.msgs.to_chat_msgs()\n# [{\"role\": \"system\", \"content\": \"...\"}, {\"role\": \"user\", \"content\": \"...\"}, ...]\n</code></pre>"},{"location":"core-concepts/messages-and-memory/#serialization","title":"Serialization","text":""},{"location":"core-concepts/messages-and-memory/#branch-serialization","title":"Branch Serialization","text":"<p>Serialize an entire branch (messages, models, logs, config) to a dictionary:</p> <pre><code># Save\ndata = branch.to_dict()\n\n# Restore\nrestored = Branch.from_dict(data)\n</code></pre>"},{"location":"core-concepts/messages-and-memory/#message-serialization","title":"Message Serialization","text":"<p>Individual messages support <code>to_dict()</code> and <code>from_dict()</code>:</p> <pre><code>msg_dict = message.to_dict()\nrestored_msg = RoledMessage.from_dict(msg_dict)\n</code></pre>"},{"location":"core-concepts/messages-and-memory/#dataframe-export","title":"DataFrame Export","text":"<p>Export messages as a pandas DataFrame:</p> <pre><code>df = branch.to_df()\n# Columns: created_at, role, content, id, sender, recipient, metadata\n</code></pre>"},{"location":"core-concepts/messages-and-memory/#clone-and-content-properties","title":"Clone and Content Properties","text":"<p>Messages support cloning (new ID, reference to original) and a <code>chat_msg</code> property for API-ready format:</p> <pre><code>cloned = message.clone()\nprint(cloned.metadata[\"clone_from\"])  # Original message ID\n\napi_format = message.chat_msg  # {\"role\": \"user\", \"content\": \"...\"}\n</code></pre>"},{"location":"core-concepts/messages-and-memory/#the-datalogger","title":"The DataLogger","text":"<p>Every branch has a <code>DataLogger</code> (accessible via <code>branch.logs</code>) that records API calls and tool invocations. This is separate from conversation messages.</p> <pre><code># Access logs\nprint(len(branch.logs))  # Number of logged events\n\n# Dump logs to file\nbranch.dump_logs(clear=True, persist_path=\"./logs/session.json\")\n\n# Async variant\nawait branch.adump_logs(clear=True)\n</code></pre> <p>The DataLogger is configured through <code>DataLoggerConfig</code>:</p> <pre><code>from lionagi.protocols.generic import DataLoggerConfig\n\nconfig = DataLoggerConfig(\n    persist_dir=\"./data/logs\",\n    capacity=100,        # Auto-dump after 100 entries\n    extension=\".json\",   # .json, .csv, or .jsonl\n    auto_save_on_exit=True,\n)\n\nbranch = Branch(log_config=config)\n</code></pre>"},{"location":"core-concepts/messages-and-memory/#async-context-manager","title":"Async Context Manager","text":"<p>Branch supports async context manager usage. On exit, logs are automatically dumped:</p> <pre><code>async with Branch(system=\"Assistant\") as branch:\n    await branch.communicate(\"Hello\")\n    await branch.communicate(\"How are you?\")\n# Logs are auto-dumped when exiting the context\n</code></pre>"},{"location":"core-concepts/messages-and-memory/#multi-branch-memory","title":"Multi-Branch Memory","text":"<p>Each branch maintains independent memory. Branches within a Session do not share conversation history:</p> <pre><code>from lionagi import Session, Branch\n\nsession = Session()\n\nresearcher = Branch(system=\"Research specialist\", name=\"researcher\")\ncritic = Branch(system=\"Critical analyst\", name=\"critic\")\nsession.include_branches([researcher, critic])\n\nawait researcher.communicate(\"Research AI safety\")\nawait critic.communicate(\"Analyze risks of AI\")\n\n# Each branch has its own memory\nprint(len(researcher.messages))  # Independent count\nprint(len(critic.messages))      # Independent count\n</code></pre>"},{"location":"core-concepts/messages-and-memory/#best-practices","title":"Best Practices","text":"<p>Use <code>communicate()</code> for conversations where context matters across exchanges. Use <code>chat()</code> for isolated queries or internal orchestration logic.</p> <p>Use descriptive system prompts to set consistent behavior:</p> <pre><code>branch = Branch(\n    system=\"You are a senior data analyst. Always include statistical significance when reporting findings.\"\n)\n</code></pre> <p>Monitor message count in long-running conversations. Large message histories increase token usage and cost:</p> <pre><code>if len(branch.messages) &gt; 50:\n    # Consider summarizing or starting a new branch\n    pass\n</code></pre> <p>Use <code>clear_messages</code> in <code>communicate()</code> when you need a fresh start without creating a new branch:</p> <pre><code>await branch.communicate(\"Start a new topic\", clear_messages=True)\n</code></pre>"},{"location":"core-concepts/messages-and-memory/#next-steps","title":"Next Steps","text":"<ul> <li>Operations -- which operations add to history</li> <li>Tools and Functions -- how ActionRequest/ActionResponse flow works</li> <li>Sessions and Branches -- managing multiple conversation branches</li> </ul>"},{"location":"core-concepts/models-and-providers/","title":"Models and Providers","text":"<p>LionAGI's <code>iModel</code> provides a unified interface for working with different LLM providers. Configure once, swap providers without changing application code.</p>"},{"location":"core-concepts/models-and-providers/#basic-usage","title":"Basic Usage","text":"<pre><code>from lionagi import Branch, iModel\n\nbranch = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4.1-mini\")\n)\n\nresponse = await branch.communicate(\"Explain quantum computing\")\n</code></pre> <p>If you do not specify a model, Branch uses the default from environment configuration: <code>LIONAGI_CHAT_PROVIDER</code> (default: <code>openai</code>) and <code>LIONAGI_CHAT_MODEL</code> (default: <code>gpt-4.1-mini</code>).</p>"},{"location":"core-concepts/models-and-providers/#supported-providers","title":"Supported Providers","text":""},{"location":"core-concepts/models-and-providers/#api-based-providers","title":"API-Based Providers","text":"<p>These providers communicate with hosted APIs over HTTP.</p> <p>OpenAI -- API key: <code>OPENAI_API_KEY</code></p> <pre><code># Default model (gpt-4.1-mini)\nbranch = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4.1-mini\")\n)\n\n# Other OpenAI models\ngpt4 = iModel(provider=\"openai\", model=\"gpt-4.1\")\ngpt4o = iModel(provider=\"openai\", model=\"gpt-4o\")\ngpt4o_mini = iModel(provider=\"openai\", model=\"gpt-4o-mini\")\n</code></pre> <p>OpenAI also supports <code>endpoint=\"response\"</code> for the Responses API:</p> <pre><code>response_model = iModel(provider=\"openai\", endpoint=\"response\", model=\"gpt-4.1\")\n</code></pre> <p>Anthropic -- API key: <code>ANTHROPIC_API_KEY</code></p> <pre><code>branch = Branch(\n    chat_model=iModel(provider=\"anthropic\", model=\"claude-sonnet-4-5-20250929\")\n)\n\n# Other Anthropic models\nhaiku = iModel(provider=\"anthropic\", model=\"claude-haiku-4-5-20251001\")\nopus = iModel(provider=\"anthropic\", model=\"claude-opus-4-6\")\n</code></pre> <p>Anthropic supports prompt caching via <code>cache_control=True</code> on individual calls.</p> <p>Gemini (Native API) -- API key: <code>GEMINI_API_KEY</code></p> <p>Gemini uses Google's OpenAI-compatible endpoint:</p> <pre><code>branch = Branch(\n    chat_model=iModel(provider=\"gemini\", model=\"gemini-2.5-flash\")\n)\n\n# Other Gemini models\ngemini_pro = iModel(provider=\"gemini\", model=\"gemini-2.0-flash\")\n</code></pre> <p>Groq -- API key: <code>GROQ_API_KEY</code></p> <p>Fast inference for open models:</p> <pre><code>branch = Branch(\n    chat_model=iModel(provider=\"groq\", model=\"llama-3.3-70b-versatile\")\n)\n</code></pre> <p>OpenRouter -- API key: <code>OPENROUTER_API_KEY</code></p> <p>Access many models through a single API:</p> <pre><code>branch = Branch(\n    chat_model=iModel(provider=\"openrouter\", model=\"google/gemini-2.5-flash\")\n)\n</code></pre> <p>Perplexity -- API key: <code>PERPLEXITY_API_KEY</code></p> <p>Real-time web search and Q&amp;A via the Sonar API:</p> <pre><code>branch = Branch(\n    chat_model=iModel(provider=\"perplexity\", model=\"sonar\")\n)\n</code></pre> <p>NVIDIA NIM -- API key: <code>NVIDIA_NIM_API_KEY</code></p> <p>Cloud-hosted models on NVIDIA infrastructure:</p> <pre><code># Chat models\nbranch = Branch(\n    chat_model=iModel(provider=\"nvidia_nim\", model=\"meta/llama3-8b-instruct\")\n)\n\n# Embedding models\nembed_model = iModel(provider=\"nvidia_nim\", endpoint=\"embed\", model=\"nvidia/nv-embed-v1\")\n</code></pre> <p>Exa -- API key: <code>EXA_API_KEY</code></p> <p>Semantic search (not a chat provider):</p> <pre><code>exa = iModel(provider=\"exa\", endpoint=\"search\")\n</code></pre> <p>Ollama -- Local models, no API key required</p> <pre><code>branch = Branch(\n    chat_model=iModel(\n        provider=\"ollama\",\n        model=\"llama3\",\n        base_url=\"http://localhost:11434\"\n    )\n)\n</code></pre>"},{"location":"core-concepts/models-and-providers/#cli-based-providers-coding-agents","title":"CLI-Based Providers (Coding Agents)","text":"<p>CLI providers wrap agentic coding tools that run as subprocesses rather than HTTP requests. This enables agent-to-agent orchestration -- your outer agent uses lionagi to spawn and coordinate inner coding agents.</p> <p>Key differences from API providers:</p> <ul> <li>Subprocess execution -- each call spawns the CLI binary and streams NDJSON   from stdout with incremental UTF-8 decoding</li> <li>Session persistence -- the endpoint stores <code>session_id</code> and automatically   passes <code>--resume</code> on subsequent calls</li> <li>Conservative concurrency -- 3 concurrent, queue capacity of 10</li> <li>No API key needed -- the CLI tool handles its own authentication</li> <li>Event handlers -- optional callbacks for streaming output   (<code>on_text</code>, <code>on_tool_use</code>, <code>on_final</code>, etc.)</li> </ul> <p>Claude Code -- Uses installed <code>claude</code> CLI (<code>npm i -g @anthropic-ai/claude-code</code>)</p> <pre><code>claude_code = iModel(\n    provider=\"claude_code\",\n    model=\"sonnet\",                          # \"sonnet\" or \"opus\"\n    permission_mode=\"bypassPermissions\",      # skip approval prompts\n    allowed_tools=[\"Read\", \"Grep\", \"Glob\"],   # restrict tool access\n    max_turns=10,                            # conversation turn limit\n)\n\nbranch = Branch(chat_model=claude_code)\nresult = await branch.communicate(\"Refactor the auth module\")\n</code></pre> <p>Gemini CLI -- Uses installed <code>gemini</code> CLI</p> <pre><code>gemini_cli = iModel(\n    provider=\"gemini_code\",\n    model=\"gemini-2.5-pro\",\n    sandbox=True,                      # safety sandboxing (default)\n    approval_mode=\"auto_edit\",         # \"suggest\", \"auto_edit\", \"full_auto\"\n)\n\nbranch = Branch(chat_model=gemini_cli)\nresult = await branch.communicate(\"Review this codebase\")\n</code></pre> <p>Codex CLI -- Uses installed <code>codex</code> CLI (<code>npm i -g codex</code>)</p> <pre><code>codex = iModel(\n    provider=\"codex\",\n    model=\"gpt-5.3-codex\",\n    full_auto=True,                    # auto-approve with sandbox\n    sandbox=\"workspace-write\",         # \"read-only\", \"workspace-write\", \"danger-full-access\"\n)\n\nbranch = Branch(chat_model=codex)\nresult = await branch.communicate(\"Write tests for the parser module\")\n</code></pre> <p>For full parameter references and orchestration patterns, see LLM Provider Integration and CLI Agent Providers.</p>"},{"location":"core-concepts/models-and-providers/#openai-compatible-providers","title":"OpenAI-Compatible Providers","text":"<p>Any provider with an OpenAI-compatible API can be used by specifying <code>base_url</code>:</p> <pre><code>custom = iModel(\n    provider=\"custom\",\n    model=\"my-model\",\n    base_url=\"https://my-provider.example.com/v1\",\n    api_key=\"my-api-key\"\n)\n</code></pre>"},{"location":"core-concepts/models-and-providers/#imodel-constructor","title":"iModel Constructor","text":"<p>The full constructor signature with all parameters:</p> <pre><code>model = iModel(\n    # Provider and endpoint\n    provider=\"openai\",                  # Provider name\n    model=\"gpt-4.1-mini\",              # Model name (passed via **kwargs)\n    endpoint=\"chat\",                    # Endpoint type (default: \"chat\")\n    base_url=None,                      # Custom base URL\n    api_key=None,                       # API key (defaults to env var)\n\n    # Rate limiting\n    queue_capacity=100,                 # Max queued requests (10 for CLI)\n    capacity_refresh_time=60,           # Queue refresh interval (seconds)\n    interval=None,                      # Processing interval\n    limit_requests=None,               # Max requests per cycle\n    limit_tokens=None,                 # Max tokens per cycle\n    concurrency_limit=None,            # Max concurrent requests (3 for CLI)\n\n    # Streaming\n    streaming_process_func=None,       # Custom chunk processor\n\n    # Hooks\n    hook_registry=None,                # HookRegistry for pre/post hooks\n    exit_hook=False,                   # Enable exit hooks\n\n    # Model-specific parameters (passed to the endpoint)\n    temperature=0.7,\n    max_tokens=2000,\n)\n</code></pre>"},{"location":"core-concepts/models-and-providers/#async-context-manager","title":"Async Context Manager","text":"<p>Use <code>iModel</code> as an async context manager for automatic resource cleanup:</p> <pre><code>async with iModel(provider=\"openai\", model=\"gpt-4.1\") as model:\n    branch = Branch(chat_model=model)\n    result = await branch.communicate(\"Hello\")\n    # Executor is stopped and resources released on exit\n</code></pre>"},{"location":"core-concepts/models-and-providers/#copying-models","title":"Copying Models","text":"<p>Use <code>copy()</code> to create an independent <code>iModel</code> instance with the same configuration but a fresh ID and executor:</p> <pre><code>original = iModel(provider=\"openai\", model=\"gpt-4.1-mini\")\n\n# Fresh instance, independent executor\nclone = original.copy()\n\n# For CLI endpoints, optionally share the session for resume\ncli_clone = cli_model.copy(share_session=True)\n</code></pre> <p>This is particularly useful when creating multiple branches that need independent rate limiting.</p>"},{"location":"core-concepts/models-and-providers/#branch-with-separate-chat-and-parse-models","title":"Branch with Separate Chat and Parse Models","text":"<p>Branch supports separate models for chat and structured parsing:</p> <pre><code>branch = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4.1\"),       # For conversations\n    parse_model=iModel(provider=\"openai\", model=\"gpt-4.1-mini\"), # For parsing\n    system=\"Technical assistant\"\n)\n\n# communicate() uses chat_model\nawait branch.communicate(\"Explain this concept\")\n\n# parse() uses parse_model\nresult = await branch.parse(some_text, response_format=MyModel)\n</code></pre> <p>If <code>parse_model</code> is not specified, it defaults to the same model as <code>chat_model</code>.</p>"},{"location":"core-concepts/models-and-providers/#multiple-providers-in-one-session","title":"Multiple Providers in One Session","text":"<p>Mix different providers for different tasks:</p> <pre><code>from lionagi import Session, Branch, iModel\n\nsession = Session()\n\nfast_branch = session.new_branch(\n    name=\"fast\",\n    system=\"Quick answers\",\n    imodel=iModel(provider=\"openai\", model=\"gpt-4.1-mini\")\n)\n\ndeep_branch = session.new_branch(\n    name=\"deep\",\n    system=\"Detailed analysis\",\n    imodel=iModel(provider=\"anthropic\", model=\"claude-sonnet-4-5-20250929\")\n)\n\n# Route tasks to the appropriate model\nquick = await fast_branch.communicate(\"What is 2+2?\")\nanalysis = await deep_branch.communicate(\"Analyze the implications of quantum computing on cryptography\")\n</code></pre>"},{"location":"core-concepts/models-and-providers/#model-configuration","title":"Model Configuration","text":"<p>Configure model parameters at construction time:</p> <pre><code>configured = iModel(\n    provider=\"openai\",\n    model=\"gpt-4.1\",\n    temperature=0.7,\n    max_tokens=2000,\n    limit_requests=100,\n    limit_tokens=50000\n)\n\nbranch = Branch(chat_model=configured, system=\"Creative writer\")\n</code></pre>"},{"location":"core-concepts/models-and-providers/#environment-configuration","title":"Environment Configuration","text":"<p>LionAGI loads API keys and defaults from environment variables:</p> Variable Purpose Default <code>OPENAI_API_KEY</code> OpenAI authentication -- <code>ANTHROPIC_API_KEY</code> Anthropic authentication -- <code>GEMINI_API_KEY</code> Gemini authentication -- <code>GROQ_API_KEY</code> Groq authentication -- <code>OPENROUTER_API_KEY</code> OpenRouter authentication -- <code>PERPLEXITY_API_KEY</code> Perplexity authentication -- <code>NVIDIA_NIM_API_KEY</code> NVIDIA NIM authentication -- <code>EXA_API_KEY</code> Exa authentication -- <code>LIONAGI_CHAT_PROVIDER</code> Default chat provider <code>openai</code> <code>LIONAGI_CHAT_MODEL</code> Default chat model <code>gpt-4.1-mini</code> <p>Settings are loaded from <code>.env</code>, <code>.env.local</code>, or <code>.secrets.env</code> files automatically via pydantic-settings.</p>"},{"location":"core-concepts/models-and-providers/#provider-comparison","title":"Provider Comparison","text":"Provider Type Default Model Auth Key Env Var <code>openai</code> API <code>gpt-4.1-mini</code> <code>OPENAI_API_KEY</code> <code>anthropic</code> API -- <code>ANTHROPIC_API_KEY</code> <code>gemini</code> API <code>gemini-2.5-flash</code> <code>GEMINI_API_KEY</code> <code>groq</code> API <code>llama-3.3-70b-versatile</code> <code>GROQ_API_KEY</code> <code>openrouter</code> API <code>google/gemini-2.5-flash</code> <code>OPENROUTER_API_KEY</code> <code>perplexity</code> API <code>sonar</code> <code>PERPLEXITY_API_KEY</code> <code>nvidia_nim</code> API <code>meta/llama3-8b-instruct</code> <code>NVIDIA_NIM_API_KEY</code> <code>ollama</code> API -- -- <code>exa</code> API -- (search only) <code>EXA_API_KEY</code> <code>claude_code</code> CLI -- -- (uses CLI auth) <code>gemini_code</code> CLI -- -- (uses CLI auth) <code>codex</code> CLI -- -- (uses CLI auth)"},{"location":"core-concepts/operations/","title":"Operations","text":"<p>Operations are the methods on a <code>Branch</code> that interact with LLMs. Each operation has different behavior around conversation history, structured output, and tool invocation.</p>"},{"location":"core-concepts/operations/#operation-overview","title":"Operation Overview","text":"Operation Adds to History Structured Output Tool Calling Use Case <code>chat()</code> No No No Stateless LLM calls, orchestration internals <code>communicate()</code> Yes Optional No Conversational interactions with memory <code>operate()</code> Yes Yes Optional Structured extraction with optional tools <code>parse()</code> No Yes No Parse raw text into Pydantic models <code>ReAct()</code> Yes Optional Yes Multi-step reasoning with tool use <code>select()</code> Yes Yes No Choose from a set of options <code>interpret()</code> No No No Rewrite user input into clearer prompts <code>act()</code> Yes No Yes Execute tool calls directly"},{"location":"core-concepts/operations/#chat","title":"chat()","text":"<p>The lowest-level LLM operation. Sends an instruction to the model using the current conversation history as context, but does not add any messages to the branch.</p> <pre><code>from lionagi import Branch\n\nbranch = Branch(system=\"You are a helpful assistant\")\nresponse = await branch.chat(\"What is 2 + 2?\")\n# response is a string: \"4\"\n\n# Messages are NOT added to history\nprint(len(branch.messages))  # Only the system message\n</code></pre> <p>Key parameters:</p> Parameter Type Description <code>instruction</code> <code>str \\| dict</code> Main instruction text <code>guidance</code> <code>str \\| dict</code> Additional system-level guidance <code>context</code> <code>str \\| dict \\| list</code> Context data for the model <code>response_format</code> <code>type[BaseModel]</code> Request structured JSON response <code>imodel</code> <code>iModel</code> Override the default chat model <code>images</code> <code>list</code> Image URLs or base64 data <code>return_ins_res_message</code> <code>bool</code> If True, returns <code>(Instruction, AssistantResponse)</code> tuple <p>When <code>return_ins_res_message=False</code> (default), returns a plain string. When True, returns the raw message objects for manual processing.</p>"},{"location":"core-concepts/operations/#communicate","title":"communicate()","text":"<p>A higher-level wrapper around <code>chat()</code> that adds messages to conversation history. Use this when you want the branch to remember the exchange.</p> <pre><code>branch = Branch(system=\"You are a tutor\")\n\n# Both instruction and response are saved to history\nawait branch.communicate(\"Explain list comprehensions\")\nawait branch.communicate(\"Show a more advanced example\")\n\n# The second call has full context of the first exchange\nprint(len(branch.messages))  # system + 2 instructions + 2 responses\n</code></pre> <p>Key parameters:</p> Parameter Type Description <code>instruction</code> <code>str \\| dict</code> Main instruction text <code>response_format</code> <code>type[BaseModel]</code> Parse response into a Pydantic model <code>request_fields</code> <code>dict \\| list[str]</code> Extract specific fields from response <code>skip_validation</code> <code>bool</code> Return raw string without parsing <code>num_parse_retries</code> <code>int</code> Retry count for parsing failures (max 5) <code>clear_messages</code> <code>bool</code> Clear history before sending <p>Structured output with <code>communicate()</code>:</p> <pre><code>from pydantic import BaseModel\n\nclass Sentiment(BaseModel):\n    label: str\n    score: float\n\nresult = await branch.communicate(\n    \"The product is excellent and well-made\",\n    response_format=Sentiment,\n)\n# result is a Sentiment instance\nprint(result.label, result.score)\n</code></pre>"},{"location":"core-concepts/operations/#operate","title":"operate()","text":"<p>The most feature-rich operation. Combines structured output with optional tool invocation. Messages are added to conversation history.</p> <pre><code>from pydantic import BaseModel\n\nclass Analysis(BaseModel):\n    summary: str\n    key_points: list[str]\n    confidence: float\n\nresult = await branch.operate(\n    instruction=\"Analyze this quarterly report\",\n    context=report_data,\n    response_format=Analysis,\n)\n# result is an Analysis instance\n</code></pre> <p>Enabling tool calling with <code>actions=True</code>:</p> <pre><code>def search_database(query: str) -&gt; list[dict]:\n    \"\"\"Search the internal database.\"\"\"\n    return [{\"id\": 1, \"title\": \"Result\"}]\n\nbranch = Branch(tools=[search_database])\n\nresult = await branch.operate(\n    instruction=\"Find records about revenue\",\n    response_format=Analysis,\n    actions=True,  # Enable tool invocation\n)\n</code></pre> <p>Key parameters:</p> Parameter Type Description <code>instruction</code> <code>str \\| dict</code> Main instruction text <code>instruct</code> <code>Instruct</code> Structured instruction object (alternative to raw params) <code>response_format</code> <code>type[BaseModel]</code> Expected output model <code>actions</code> <code>bool</code> Enable tool calling <code>reason</code> <code>bool</code> Request chain-of-thought reasoning <code>invoke_actions</code> <code>bool</code> Execute tool calls automatically (default True) <code>action_strategy</code> <code>\"concurrent\" \\| \"sequential\"</code> How to run multiple tools (default \"concurrent\") <code>tools</code> <code>ToolRef</code> Specific tools to make available <code>skip_validation</code> <code>bool</code> Return raw response without parsing <code>handle_validation</code> <code>\"raise\" \\| \"return_value\" \\| \"return_none\"</code> How to handle parse failures"},{"location":"core-concepts/operations/#parse","title":"parse()","text":"<p>Parses raw text into a structured Pydantic model using the parse model. Does not add messages to conversation history. Useful for post-processing LLM output or extracting structure from arbitrary text.</p> <pre><code>from pydantic import BaseModel\n\nclass ContactInfo(BaseModel):\n    name: str\n    email: str\n    phone: str | None = None\n\nraw_text = \"John Smith, john@example.com, phone: 555-1234\"\nresult = await branch.parse(raw_text, response_format=ContactInfo)\n# result is a ContactInfo instance\n</code></pre> <p>Key parameters:</p> Parameter Type Description <code>text</code> <code>str</code> Raw text to parse <code>response_format</code> <code>type[BaseModel]</code> Target Pydantic model <code>max_retries</code> <code>int</code> Number of retry attempts (default 3) <code>fuzzy_match</code> <code>bool</code> Attempt fuzzy key matching (default True) <code>handle_validation</code> <code>\"raise\" \\| \"return_value\" \\| \"return_none\"</code> Failure handling <code>similarity_threshold</code> <code>float</code> Threshold for fuzzy matching (default 0.85)"},{"location":"core-concepts/operations/#react","title":"ReAct()","text":"<p>Implements the Reasoning + Acting paradigm. The model thinks through a problem, optionally uses tools, observes results, and iterates until it reaches a conclusion. Messages are added to history.</p> <pre><code>def web_search(query: str) -&gt; str:\n    \"\"\"Search the web for information.\"\"\"\n    return f\"Results for: {query}\"\n\nbranch = Branch(tools=[web_search])\n\nresult = await branch.ReAct(\n    instruct={\"instruction\": \"Research the latest advances in quantum computing\"},\n    max_extensions=3,\n    verbose=True,\n)\n</code></pre> <p>Key parameters:</p> Parameter Type Description <code>instruct</code> <code>Instruct \\| dict</code> Instruction with optional guidance/context <code>tools</code> <code>ToolRef</code> Tools available for use (defaults to all registered) <code>response_format</code> <code>type[BaseModel]</code> Structure for the final answer <code>max_extensions</code> <code>int</code> Max reasoning-action cycles (default 3, max 5) <code>extension_allowed</code> <code>bool</code> Allow multi-step reasoning (default True) <code>interpret</code> <code>bool</code> Pre-process instruction through <code>interpret()</code> <code>verbose</code> <code>bool</code> Print reasoning steps <code>return_analysis</code> <code>bool</code> Return <code>(result, analyses)</code> tuple <p>ReActStream</p> <p><code>branch.ReActStream()</code> is an async generator variant that yields intermediate analysis results as they are produced, rather than waiting for the final answer.</p>"},{"location":"core-concepts/operations/#select","title":"select()","text":"<p>Presents a set of choices to the model and returns a structured selection. This is a standalone function in <code>lionagi.operations.select</code> that uses <code>operate()</code> internally.</p> <pre><code>from lionagi.operations.select.select import select\n\nresult = await select(\n    branch=branch,\n    instruct={\"instruction\": \"Which database is best for this use case?\"},\n    choices=[\"PostgreSQL\", \"MongoDB\", \"Redis\", \"DynamoDB\"],\n    max_num_selections=2,\n)\n# result.selected contains the chosen items\n</code></pre> <p>Choices can be a list of strings, a dict mapping keys to descriptions, or an Enum type.</p>"},{"location":"core-concepts/operations/#interpret","title":"interpret()","text":"<p>Rewrites a user's raw input into a clearer, more structured prompt. Does not add messages to history. Useful as a preprocessing step before other operations.</p> <pre><code>refined = await branch.interpret(\n    \"how do i do marketing stuff?\",\n    domain=\"marketing\",\n    style=\"detailed\",\n)\n# refined might be: \"Explain step-by-step how to design and execute a\n# marketing strategy, including target audience analysis, channel\n# selection, and campaign performance tracking.\"\n</code></pre> Parameter Type Description <code>text</code> <code>str</code> Raw user input <code>domain</code> <code>str \\| None</code> Domain hint (e.g., \"finance\", \"devops\") <code>style</code> <code>str \\| None</code> Style hint (e.g., \"concise\", \"detailed\")"},{"location":"core-concepts/operations/#act","title":"act()","text":"<p>Executes tool calls directly without going through the LLM. Takes an <code>ActionRequest</code> (or equivalent dict) and invokes the matching registered tool. Messages (ActionRequest and ActionResponse) are added to history.</p> <pre><code>from lionagi.protocols.messages import ActionRequest\n\nresponse = await branch.act(\n    action_request={\"function\": \"search_database\", \"arguments\": {\"query\": \"revenue\"}},\n    strategy=\"concurrent\",\n)\n# response is a list of ActionResponse objects\n</code></pre> Parameter Type Description <code>action_request</code> <code>list \\| dict \\| ActionRequest</code> Tool call(s) to execute <code>strategy</code> <code>\"concurrent\" \\| \"sequential\"</code> Execution strategy (default \"concurrent\") <code>suppress_errors</code> <code>bool</code> Log errors instead of raising (default True)"},{"location":"core-concepts/operations/#using-operations-with-builder","title":"Using Operations with Builder","text":"<p>Operations can be composed into DAGs using <code>Builder</code> for multi-agent workflows:</p> <pre><code>from lionagi import Builder, Session, Branch\n\nsession = Session()\nbuilder = Builder(\"analysis_pipeline\")\n\nanalyst = Branch(system=\"Data analyst\")\nwriter = Branch(system=\"Report writer\")\nsession.include_branches([analyst, writer])\n\n# Operations without dependencies run in parallel\nanalyze = builder.add_operation(\n    \"operate\",\n    branch=analyst,\n    instruction=\"Analyze this dataset\",\n    response_format=Analysis,\n)\n\n# Dependencies control execution order\nreport = builder.add_operation(\n    \"communicate\",\n    branch=writer,\n    instruction=\"Write a report from the analysis\",\n    depends_on=[analyze],\n)\n\nresults = await session.flow(builder.get_graph())\n</code></pre>"},{"location":"core-concepts/operations/#next-steps","title":"Next Steps","text":"<ul> <li>Tools and Functions -- registering tools for <code>operate()</code> and <code>ReAct()</code></li> <li>Messages and Memory -- understanding conversation history</li> <li>Sessions and Branches -- managing multiple conversations</li> </ul>"},{"location":"core-concepts/sessions-and-branches/","title":"Sessions and Branches","text":"<p>Sessions and Branches are the core abstractions that power LionAGI's orchestration engine.</p> <ul> <li>Branch: An individual conversation thread with its own message history, tools, and model configuration</li> <li>Session: A workspace that coordinates multiple branches and executes graph-based workflows</li> </ul>"},{"location":"core-concepts/sessions-and-branches/#quick-start","title":"Quick Start","text":"<pre><code>from lionagi import Branch, iModel\n\nbranch = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4.1-mini\"),\n    system=\"You are a helpful research assistant.\"\n)\n\nresponse = await branch.communicate(\"What are the latest trends in quantum computing?\")\n</code></pre>"},{"location":"core-concepts/sessions-and-branches/#branch","title":"Branch","text":"<p>A <code>Branch</code> is the primary interface for all LLM operations. It manages four internal components:</p> <ul> <li>MessageManager -- Conversation history (messages + ordering)</li> <li>ActionManager -- Tool registry and invocation</li> <li>iModelManager -- Chat and parse model instances</li> <li>DataLogger -- Activity logging</li> </ul>"},{"location":"core-concepts/sessions-and-branches/#creating-a-branch","title":"Creating a Branch","text":"<pre><code>from lionagi import Branch, iModel\n\n# Minimal -- uses default provider and model\nbranch = Branch()\n\n# With explicit configuration\nbranch = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4.1\"),\n    parse_model=iModel(provider=\"openai\", model=\"gpt-4.1-mini\"),\n    system=\"Expert financial analyst\",\n    name=\"finance_analyst\",\n    tools=[calculate_roi, fetch_market_data]\n)\n</code></pre> <p>Constructor parameters:</p> Parameter Type Description <code>chat_model</code> <code>iModel</code> or <code>dict</code> Primary model for conversations <code>parse_model</code> <code>iModel</code> or <code>dict</code> Model for structured parsing (defaults to chat_model) <code>system</code> <code>str</code> or <code>System</code> System prompt <code>name</code> <code>str</code> Human-readable branch name <code>tools</code> <code>list</code> of functions Tools to register <code>messages</code> <code>Pile[RoledMessage]</code> Pre-existing messages <code>system_sender</code> <code>SenderRecipient</code> Sender attributed to system message <code>system_datetime</code> <code>bool</code> or <code>str</code> Include timestamp in system message <code>log_config</code> <code>DataLoggerConfig</code> Logging configuration <code>user</code> <code>SenderRecipient</code> Owner/sender of this branch"},{"location":"core-concepts/sessions-and-branches/#branch-operations","title":"Branch Operations","text":"<p>Every Branch provides these operations:</p>"},{"location":"core-concepts/sessions-and-branches/#chat-raw-llm-call","title":"chat() -- Raw LLM Call","text":"<p>Sends a message to the LLM and returns the response. Does not add messages to conversation history, giving you full control over what gets recorded.</p> <pre><code>response = await branch.chat(\"Explain dark matter briefly\")\n\n# With structured output\nfrom pydantic import BaseModel\n\nclass Answer(BaseModel):\n    explanation: str\n    confidence: float\n\nresponse = await branch.chat(\n    \"Explain dark matter\",\n    response_format=Answer\n)\n</code></pre>"},{"location":"core-concepts/sessions-and-branches/#communicate-conversational-exchange","title":"communicate() -- Conversational Exchange","text":"<p>Like <code>chat()</code>, but automatically adds instruction and response to the conversation history. Supports structured output and follow-up questions.</p> <pre><code># Messages are added to history\nawait branch.communicate(\"What is reinforcement learning?\")\n\n# Follow-up references previous context\nawait branch.communicate(\"How does it compare to supervised learning?\")\n\n# With structured output\nresult = await branch.communicate(\n    \"Summarize our discussion\",\n    response_format=SummaryModel\n)\n</code></pre> <p>Key parameters:</p> <ul> <li><code>instruction</code> -- The user message</li> <li><code>guidance</code> -- Additional system-level guidance</li> <li><code>context</code> -- Extra context data</li> <li><code>response_format</code> -- Pydantic model for structured output</li> <li><code>clear_messages</code> -- Clear history before sending</li> <li><code>skip_validation</code> -- Return raw string without parsing</li> </ul>"},{"location":"core-concepts/sessions-and-branches/#operate-tool-calling-with-validation","title":"operate() -- Tool Calling with Validation","text":"<p>Combines LLM interaction with automatic tool invocation and response validation. Messages are added to conversation history.</p> <pre><code>def search(query: str, limit: int = 10) -&gt; list[dict]:\n    \"\"\"Search the knowledge base.\"\"\"\n    return [{\"title\": \"Result\", \"score\": 0.95}]\n\nbranch = Branch(system=\"Research assistant\", tools=[search])\n\nresult = await branch.operate(\n    instruction=\"Find information about quantum computing\",\n    response_format=ResearchResult,\n    reason=True,          # Include chain-of-thought\n    actions=True,         # Signal tool use is expected\n    invoke_actions=True   # Auto-execute tool calls (default)\n)\n</code></pre> <p>Key parameters:</p> <ul> <li><code>instruction</code> -- The task instruction</li> <li><code>response_format</code> -- Pydantic model for the response</li> <li><code>tools</code> -- Additional tools for this call</li> <li><code>reason</code> -- Include chain-of-thought reasoning</li> <li><code>actions</code> -- Signal that tool use is expected</li> <li><code>invoke_actions</code> -- Automatically execute tool calls (default: <code>True</code>)</li> <li><code>action_strategy</code> -- <code>\"concurrent\"</code> or <code>\"sequential\"</code> (default: <code>\"concurrent\"</code>)</li> </ul>"},{"location":"core-concepts/sessions-and-branches/#parse-structured-text-extraction","title":"parse() -- Structured Text Extraction","text":"<p>Parses raw text into a Pydantic model using the parse model. Does not add messages to conversation history.</p> <pre><code>from pydantic import BaseModel\n\nclass Invoice(BaseModel):\n    vendor: str\n    amount: float\n    date: str\n    line_items: list[str]\n\nraw = \"\"\"\nInvoice from Acme Corp\nDate: 2025-03-15\nAmount: $1,234.56\nItems: Widget A, Widget B, Service C\n\"\"\"\n\ninvoice = await branch.parse(raw, response_format=Invoice)\nprint(invoice.vendor)   # \"Acme Corp\"\nprint(invoice.amount)   # 1234.56\n</code></pre> <p>Key parameters:</p> <ul> <li><code>text</code> -- Raw text to parse</li> <li><code>response_format</code> or <code>request_type</code> -- Target Pydantic model</li> <li><code>max_retries</code> -- Retry count on parse failure (default: 3)</li> <li><code>fuzzy_match</code> -- Attempt fuzzy field matching (default: <code>True</code>)</li> <li><code>handle_validation</code> -- <code>\"raise\"</code>, <code>\"return_value\"</code>, or <code>\"return_none\"</code></li> </ul>"},{"location":"core-concepts/sessions-and-branches/#react-multi-step-reasoning","title":"ReAct() -- Multi-Step Reasoning","text":"<p>Implements the ReAct paradigm: the model reasons about the task, takes actions (tool calls), observes results, and iterates until it reaches a conclusion.</p> <pre><code>result = await branch.ReAct(\n    instruct={\n        \"instruction\": \"Research and compare the top 3 cloud providers\",\n        \"guidance\": \"Use search tools to gather current pricing data\"\n    },\n    extension_allowed=True,  # Allow multiple reasoning steps\n    max_extensions=3,        # Up to 3 additional steps\n    verbose=True             # Log reasoning steps\n)\n</code></pre> <p>Key parameters:</p> <ul> <li><code>instruct</code> -- <code>Instruct</code> object or dict with <code>instruction</code>, <code>guidance</code>, <code>context</code></li> <li><code>tools</code> -- Tools available for the ReAct loop</li> <li><code>response_format</code> -- Final output schema</li> <li><code>extension_allowed</code> -- Allow multi-step reasoning (default: <code>True</code>)</li> <li><code>max_extensions</code> -- Max reasoning steps (default: 3, capped at 5)</li> <li><code>interpret</code> -- Rewrite instructions before starting</li> <li><code>return_analysis</code> -- Return <code>(result, analyses)</code> tuple</li> <li><code>verbose</code> -- Log intermediate reasoning steps</li> </ul>"},{"location":"core-concepts/sessions-and-branches/#select-choice-selection","title":"select() -- Choice Selection","text":"<p>Choose from a list of options using LLM reasoning.</p> <pre><code>from enum import Enum\n\nclass Priority(Enum):\n    HIGH = \"high\"\n    MEDIUM = \"medium\"\n    LOW = \"low\"\n\nresult = await branch.select(\n    instruct={\"instruction\": \"Classify the urgency of this bug report\"},\n    choices=Priority,\n    max_num_selections=1\n)\n</code></pre>"},{"location":"core-concepts/sessions-and-branches/#interpret-prompt-refinement","title":"interpret() -- Prompt Refinement","text":"<p>Rewrites user input into a clearer, more structured prompt. Does not add messages to history.</p> <pre><code>refined = await branch.interpret(\n    text=\"how do I do marketing analytics\",\n    domain=\"marketing\",\n    style=\"detailed\"\n)\n# refined: \"Explain step-by-step how to set up a marketing analytics\n#  pipeline to track campaign performance...\"\n</code></pre>"},{"location":"core-concepts/sessions-and-branches/#act-direct-tool-execution","title":"act() -- Direct Tool Execution","text":"<p>Execute tool calls directly, without going through the LLM.</p> <pre><code>from lionagi.protocols.messages import ActionRequest\n\nresponses = await branch.act(\n    action_request=action_requests,\n    strategy=\"concurrent\"  # or \"sequential\"\n)\n</code></pre>"},{"location":"core-concepts/sessions-and-branches/#async-context-manager","title":"Async Context Manager","text":"<p>Branch supports <code>async with</code> for automatic log flushing on exit:</p> <pre><code>async with Branch(system=\"Temporary assistant\") as branch:\n    result = await branch.communicate(\"Help me analyze this data\")\n    # Logs are automatically dumped on exit\n</code></pre>"},{"location":"core-concepts/sessions-and-branches/#cloning-branches","title":"Cloning Branches","text":"<p><code>clone()</code> creates a new Branch with copied messages and tools but independent state.</p> <pre><code>original = Branch(\n    system=\"Research assistant\",\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4.1\"),\n    tools=[search, analyze]\n)\n\nawait original.communicate(\"Research quantum computing trends\")\n\n# Clone the branch -- independent conversation history\nclone = original.clone()\n\n# For API-based models: clone shares the same iModel instance (efficient)\n# For CLI-based models: clone gets a fresh iModel copy (avoids session conflicts)\n\n# Conversations diverge from here\nawait original.communicate(\"Focus on hardware advances\")\nawait clone.communicate(\"Focus on algorithm improvements\")\n</code></pre> <p>You can also use <code>await branch.aclone()</code> for async-safe cloning that locks the message pile during the copy.</p>"},{"location":"core-concepts/sessions-and-branches/#registering-tools","title":"Registering Tools","text":"<pre><code>def calculate(expression: str) -&gt; float:\n    \"\"\"Evaluate a math expression.\"\"\"\n    return eval(expression)\n\nasync def fetch_data(url: str) -&gt; dict:\n    \"\"\"Fetch data from a URL.\"\"\"\n    async with aiohttp.ClientSession() as session:\n        async with session.get(url) as resp:\n            return await resp.json()\n\n# Register at construction\nbranch = Branch(tools=[calculate, fetch_data])\n\n# Or register after construction\nbranch.register_tools([another_tool], update=True)\n</code></pre>"},{"location":"core-concepts/sessions-and-branches/#inspecting-branch-state","title":"Inspecting Branch State","text":"<pre><code># Messages\nfor msg in branch.messages:\n    print(f\"[{msg.role}] {msg.content[:100]}...\")\n\n# System prompt\nprint(branch.system)\n\n# Registered tools\nprint(list(branch.tools.keys()))\n\n# Models\nprint(branch.chat_model.model_name)\nprint(branch.parse_model.model_name)\n\n# Logs\nprint(len(branch.logs))\n\n# Export to DataFrame\ndf = branch.to_df()\n</code></pre>"},{"location":"core-concepts/sessions-and-branches/#session","title":"Session","text":"<p>A <code>Session</code> manages multiple branches and provides workflow orchestration via <code>flow()</code>.</p>"},{"location":"core-concepts/sessions-and-branches/#creating-a-session","title":"Creating a Session","text":"<pre><code>from lionagi import Session\n\n# Session auto-creates a default branch\nsession = Session(name=\"my_project\")\n\n# Access the default branch\nbranch = session.default_branch\nresponse = await branch.communicate(\"Hello\")\n</code></pre>"},{"location":"core-concepts/sessions-and-branches/#creating-branches-within-a-session","title":"Creating Branches Within a Session","text":"<pre><code>session = Session()\n\n# Create and include a branch\nresearcher = session.new_branch(\n    name=\"researcher\",\n    system=\"Research specialist\",\n    imodel=iModel(provider=\"anthropic\", model=\"claude-sonnet-4-5-20250929\")\n)\n\nwriter = session.new_branch(\n    name=\"writer\",\n    system=\"Technical writer\"\n)\n\n# Or include existing branches\nexternal_branch = Branch(name=\"external\", system=\"External service\")\nsession.include_branches(external_branch)\n</code></pre>"},{"location":"core-concepts/sessions-and-branches/#accessing-branches","title":"Accessing Branches","text":"<pre><code># By name\nresearcher = session.get_branch(\"researcher\")\n\n# By ID\nbranch = session.get_branch(branch_id)\n\n# Change default branch\nsession.change_default_branch(writer)\n</code></pre>"},{"location":"core-concepts/sessions-and-branches/#splitting-branches","title":"Splitting Branches","text":"<p>Split creates a clone of a branch within the session:</p> <pre><code># Clone a branch and add the clone to the session\nclone = session.split(researcher)\n\n# Async-safe version\nclone = await session.asplit(researcher)\n</code></pre>"},{"location":"core-concepts/sessions-and-branches/#registering-custom-operations","title":"Registering Custom Operations","text":"<p>Session supports registering custom operations that branches can execute via the Builder:</p> <pre><code>session = Session()\n\n# Register with decorator\n@session.operation()\nasync def summarize_findings(context: dict) -&gt; str:\n    branch = session.default_branch\n    return await branch.communicate(f\"Summarize: {context}\")\n\n# Register explicitly\nsession.register_operation(\"custom_op\", my_function)\n</code></pre>"},{"location":"core-concepts/sessions-and-branches/#flow-graph-based-workflow-execution","title":"flow() -- Graph-Based Workflow Execution","text":"<p>Execute a DAG of operations across multiple branches:</p> <pre><code>from lionagi import Session, Builder\n\nsession = Session()\nanalyzer = session.new_branch(name=\"analyzer\", system=\"Data analyst\")\nreviewer = session.new_branch(name=\"reviewer\", system=\"Code reviewer\")\n\nbuilder = Builder(\"review_pipeline\")\n\n# Parallel analysis steps\nanalysis_id = builder.add_operation(\n    \"communicate\",\n    instruction=\"Analyze the performance data\",\n    branch=analyzer\n)\n\nreview_id = builder.add_operation(\n    \"communicate\",\n    instruction=\"Review the code changes\",\n    branch=reviewer\n)\n\n# Aggregation step (waits for both)\nbuilder.add_aggregation(\n    \"communicate\",\n    instruction=\"Combine analysis and review into final report\",\n    source_node_ids=[analysis_id, review_id],\n    branch=analyzer\n)\n\n# Execute\nresult = await session.flow(\n    graph=builder.get_graph(),\n    context={\"project\": \"Project Alpha\"},\n    parallel=True,\n    max_concurrent=5,\n    verbose=True\n)\n</code></pre> <p>flow() parameters:</p> Parameter Type Description <code>graph</code> <code>Graph</code> Workflow graph from Builder <code>context</code> <code>dict</code> Initial context for the workflow <code>parallel</code> <code>bool</code> Execute independent operations in parallel <code>max_concurrent</code> <code>int</code> Max concurrent operations (default: 5) <code>verbose</code> <code>bool</code> Enable verbose logging <code>default_branch</code> <code>Branch</code> Branch to use as default (defaults to session's)"},{"location":"core-concepts/sessions-and-branches/#multi-branch-patterns","title":"Multi-Branch Patterns","text":""},{"location":"core-concepts/sessions-and-branches/#parallel-expert-panel","title":"Parallel Expert Panel","text":"<pre><code>import asyncio\nfrom lionagi import Session, iModel\n\nsession = Session()\n\nexperts = {\n    role: session.new_branch(\n        name=f\"{role}_expert\",\n        system=f\"You are an expert in {role}.\"\n    )\n    for role in [\"technical\", \"business\", \"legal\"]\n}\n\n# Gather opinions in parallel\nquestion = \"What are the risks of deploying this AI system?\"\n\nresults = await asyncio.gather(*[\n    branch.communicate(question)\n    for branch in experts.values()\n])\n\n# Synthesize with moderator\nmoderator = session.new_branch(name=\"moderator\", system=\"Consensus builder\")\nsynthesis = await moderator.communicate(\n    f\"Synthesize these expert opinions into a unified assessment:\\n\"\n    + \"\\n\".join(f\"- {role}: {result}\" for role, result in zip(experts.keys(), results))\n)\n</code></pre>"},{"location":"core-concepts/sessions-and-branches/#sequential-pipeline","title":"Sequential Pipeline","text":"<pre><code>session = Session()\n\nparser = session.new_branch(name=\"parser\", system=\"Extract structured data\")\nvalidator = session.new_branch(name=\"validator\", system=\"Validate accuracy\")\nformatter = session.new_branch(name=\"formatter\", system=\"Format for output\")\n\nasync def process_document(document: str):\n    parsed = await parser.communicate(f\"Extract key data from: {document}\")\n    validated = await validator.communicate(f\"Validate this data: {parsed}\")\n    formatted = await formatter.communicate(f\"Format for report: {validated}\")\n    return formatted\n</code></pre>"},{"location":"core-concepts/sessions-and-branches/#fork-and-compare","title":"Fork-and-Compare","text":"<p>Use <code>clone()</code> to explore different approaches from the same starting point:</p> <pre><code>branch = Branch(system=\"Strategy advisor\")\nawait branch.communicate(\"Our company is considering expanding into Asia.\")\n\n# Fork the conversation\noptimistic = branch.clone()\npessimistic = branch.clone()\n\nplan_a = await optimistic.communicate(\"Assume best-case scenario. What's the plan?\")\nplan_b = await pessimistic.communicate(\"Assume worst-case scenario. What's the plan?\")\n\n# Compare\ncomparison = await branch.communicate(\n    f\"Compare these two strategies:\\n\"\n    f\"Optimistic: {plan_a}\\n\"\n    f\"Pessimistic: {plan_b}\"\n)\n</code></pre>"},{"location":"core-concepts/sessions-and-branches/#builder-workflow-graphs","title":"Builder: Workflow Graphs","text":"<p>The <code>Builder</code> (alias for <code>OperationGraphBuilder</code>) constructs directed acyclic graphs of operations for <code>session.flow()</code>.</p>"},{"location":"core-concepts/sessions-and-branches/#basic-usage","title":"Basic Usage","text":"<pre><code>from lionagi import Builder\n\nbuilder = Builder(\"my_workflow\")\n\n# Add sequential operations (auto-linked)\nstep1 = builder.add_operation(\"communicate\", instruction=\"Step 1\")\nstep2 = builder.add_operation(\"communicate\", instruction=\"Step 2\")\n# step2 automatically depends on step1\n\n# Add with explicit dependencies\nstep3 = builder.add_operation(\n    \"communicate\",\n    instruction=\"Step 3\",\n    depends_on=[step1]  # Skip step2, depend directly on step1\n)\n\n# Get the graph for execution\ngraph = builder.get_graph()\n</code></pre>"},{"location":"core-concepts/sessions-and-branches/#parallel-and-aggregation","title":"Parallel and Aggregation","text":"<pre><code>builder = Builder(\"parallel_flow\")\n\n# These run in parallel (no dependencies between them)\na = builder.add_operation(\"communicate\", instruction=\"Task A\")\n\n# Reset heads to allow parallel operations\nbuilder._current_heads = []\nb = builder.add_operation(\"communicate\", instruction=\"Task B\")\nbuilder._current_heads = []\nc = builder.add_operation(\"communicate\", instruction=\"Task C\")\n\n# Aggregate results\nbuilder.add_aggregation(\n    \"communicate\",\n    instruction=\"Combine results from A, B, and C\",\n    source_node_ids=[a, b, c]\n)\n</code></pre>"},{"location":"core-concepts/sessions-and-branches/#dynamic-expansion","title":"Dynamic Expansion","text":"<p>Expand the graph based on execution results:</p> <pre><code>from lionagi.operations.builder import ExpansionStrategy\n\nbuilder = Builder(\"dynamic\")\n\ngen_id = builder.add_operation(\n    \"operate\",\n    instruction=\"Generate 5 sub-tasks\",\n    response_format=TaskList\n)\n\n# Execute first phase\nresult = await session.flow(graph=builder.get_graph())\n\n# Expand based on results\nbuilder.expand_from_result(\n    items=result.tasks,\n    source_node_id=gen_id,\n    operation=\"communicate\",\n    strategy=ExpansionStrategy.CONCURRENT\n)\n\n# Execute expanded graph\nfinal = await session.flow(graph=builder.get_graph())\n</code></pre>"},{"location":"core-concepts/sessions-and-branches/#assigning-branches-to-operations","title":"Assigning Branches to Operations","text":"<p>Direct specific operations to specific branches:</p> <pre><code>builder = Builder(\"multi_branch\")\n\nbuilder.add_operation(\n    \"communicate\",\n    instruction=\"Research the topic\",\n    branch=researcher   # Runs on researcher branch\n)\n\nbuilder.add_operation(\n    \"communicate\",\n    instruction=\"Write the report\",\n    branch=writer,      # Runs on writer branch\n    depends_on=[...]\n)\n</code></pre>"},{"location":"core-concepts/sessions-and-branches/#best-practices","title":"Best Practices","text":""},{"location":"core-concepts/sessions-and-branches/#encapsulate-in-classes","title":"Encapsulate in Classes","text":"<pre><code>from lionagi import Session, iModel\n\nclass DocumentProcessor:\n    def __init__(self):\n        self.session = Session(name=\"doc_processor\")\n        self.parser = self.session.new_branch(\n            name=\"parser\", system=\"Parse documents accurately\"\n        )\n        self.validator = self.session.new_branch(\n            name=\"validator\", system=\"Validate extracted data\"\n        )\n\n    async def process(self, document: str) -&gt; dict:\n        parsed = await self.parser.communicate(f\"Parse: {document}\")\n        validated = await self.validator.communicate(f\"Validate: {parsed}\")\n        return {\"parsed\": parsed, \"validated\": validated}\n</code></pre>"},{"location":"core-concepts/sessions-and-branches/#use-factory-functions","title":"Use Factory Functions","text":"<pre><code>from lionagi import Branch, iModel\n\ndef create_expert(domain: str, provider: str = \"openai\") -&gt; Branch:\n    return Branch(\n        name=f\"{domain}_expert\",\n        chat_model=iModel(provider=provider, model=\"gpt-4.1\"),\n        system=f\"You are an expert in {domain}. Be precise and cite sources.\"\n    )\n\nfinance = create_expert(\"finance\")\ntech = create_expert(\"technology\", provider=\"anthropic\")\n</code></pre>"},{"location":"core-concepts/sessions-and-branches/#per-user-branch-isolation","title":"Per-User Branch Isolation","text":"<pre><code>from lionagi import Session\n\nsession = Session(name=\"app\")\nuser_branches: dict[str, Branch] = {}\n\ndef get_user_branch(user_id: str) -&gt; Branch:\n    if user_id not in user_branches:\n        user_branches[user_id] = session.new_branch(\n            name=f\"user_{user_id}\",\n            system=\"Helpful assistant\"\n        )\n    return user_branches[user_id]\n\n# Each user has isolated conversation history\nbranch = get_user_branch(\"user_123\")\nresponse = await branch.communicate(\"Hello!\")\n</code></pre>"},{"location":"core-concepts/sessions-and-branches/#resource-cleanup-with-context-managers","title":"Resource Cleanup with Context Managers","text":"<pre><code>async with Branch(system=\"Temporary analysis\") as branch:\n    result = await branch.communicate(\"Analyze this data\")\n    # Logs flushed automatically on exit\n</code></pre>"},{"location":"core-concepts/tools-and-functions/","title":"Tools and Functions","text":"<p>Tools give branches the ability to call your Python functions during LLM interactions. LionAGI automatically generates OpenAI-compatible function schemas from your code, handles both sync and async functions, and supports MCP server integration.</p>"},{"location":"core-concepts/tools-and-functions/#registering-tools","title":"Registering Tools","text":"<p>The simplest way to add tools is to pass functions directly when creating a Branch:</p> <pre><code>from lionagi import Branch\n\ndef calculate_sum(a: float, b: float) -&gt; float:\n    \"\"\"Add two numbers together.\"\"\"\n    return a + b\n\ndef search_records(query: str) -&gt; list[dict]:\n    \"\"\"Search the database for matching records.\"\"\"\n    return [{\"id\": 1, \"name\": \"example\"}]\n\nbranch = Branch(tools=[calculate_sum, search_records])\n</code></pre> <p>You can also register tools after creation:</p> <pre><code>branch.register_tools([calculate_sum, search_records])\n\n# Or one at a time\nbranch.register_tools(calculate_sum)\n</code></pre> <p>To update an existing tool registration, pass <code>update=True</code>:</p> <pre><code>branch.register_tools(calculate_sum, update=True)\n</code></pre>"},{"location":"core-concepts/tools-and-functions/#how-schema-generation-works","title":"How Schema Generation Works","text":"<p>When you register a function, LionAGI calls <code>function_to_schema()</code> to auto-generate an OpenAI-compatible tool schema from the function's signature and docstring.</p> <pre><code>from lionagi.libs.schema.function_to_schema import function_to_schema\n\ndef get_weather(city: str, units: str = \"celsius\") -&gt; dict:\n    \"\"\"Get current weather for a city.\n\n    Args:\n        city: The city name to look up.\n        units: Temperature units (celsius or fahrenheit).\n    \"\"\"\n    return {\"city\": city, \"temperature\": 22, \"units\": units}\n\nschema = function_to_schema(get_weather)\n</code></pre> <p>The generated schema follows the OpenAI function calling format:</p> <pre><code>{\n  \"type\": \"function\",\n  \"function\": {\n    \"name\": \"get_weather\",\n    \"description\": \"Get current weather for a city.\",\n    \"parameters\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"city\": {\"type\": \"string\", \"description\": \"The city name to look up.\"},\n        \"units\": {\"type\": \"string\", \"description\": \"Temperature units (celsius or fahrenheit).\"}\n      },\n      \"required\": [\"city\", \"units\"]\n    }\n  }\n}\n</code></pre> <p>Python type hints map to JSON Schema types:</p> Python JSON Schema <code>str</code> <code>string</code> <code>int</code> <code>number</code> <code>float</code> <code>number</code> <code>bool</code> <code>boolean</code> <code>list</code> / <code>tuple</code> <code>array</code> <code>dict</code> <code>object</code> <p>Google-style and reST-style docstrings are both supported for extracting parameter descriptions.</p>"},{"location":"core-concepts/tools-and-functions/#the-tool-class","title":"The Tool Class","text":"<p>For more control, create <code>Tool</code> objects directly:</p> <pre><code>from lionagi.protocols.action.tool import Tool\n\ntool = Tool(func_callable=get_weather)\n\n# Access the auto-generated schema\nprint(tool.function)          # \"get_weather\"\nprint(tool.tool_schema)       # Full OpenAI-format schema\nprint(tool.required_fields)   # {\"city\", \"units\"}\n</code></pre>"},{"location":"core-concepts/tools-and-functions/#input-validation-with-request_options","title":"Input Validation with request_options","text":"<p>You can attach a Pydantic model for strict input validation:</p> <pre><code>from pydantic import BaseModel\n\nclass WeatherRequest(BaseModel):\n    city: str\n    units: str = \"celsius\"\n\ntool = Tool(\n    func_callable=get_weather,\n    request_options=WeatherRequest,\n)\nbranch = Branch(tools=[tool])\n</code></pre> <p>When the LLM calls this tool, arguments are validated against <code>WeatherRequest</code> before the function executes.</p>"},{"location":"core-concepts/tools-and-functions/#prepost-processing","title":"Pre/Post Processing","text":"<p>Tools support optional preprocessing of arguments and postprocessing of results:</p> <pre><code>def clean_args(args: dict) -&gt; dict:\n    args[\"city\"] = args[\"city\"].strip().title()\n    return args\n\ndef format_output(result: dict) -&gt; str:\n    return f\"{result['city']}: {result['temperature']}deg {result['units']}\"\n\ntool = Tool(\n    func_callable=get_weather,\n    preprocessor=clean_args,\n    postprocessor=format_output,\n)\n</code></pre> <p>Both sync and async preprocessors/postprocessors are supported.</p>"},{"location":"core-concepts/tools-and-functions/#strict-mode","title":"Strict Mode","text":"<p>Enable <code>strict_func_call=True</code> to require that the LLM-provided arguments exactly match the function schema's required fields (no extra, no missing):</p> <pre><code>tool = Tool(func_callable=get_weather, strict_func_call=True)\n</code></pre>"},{"location":"core-concepts/tools-and-functions/#async-tool-support","title":"Async Tool Support","text":"<p>Async functions work identically to sync functions. LionAGI detects whether a function is async and handles it automatically:</p> <pre><code>import httpx\n\nasync def fetch_page(url: str) -&gt; str:\n    \"\"\"Fetch the content of a web page.\"\"\"\n    async with httpx.AsyncClient() as client:\n        resp = await client.get(url)\n        return resp.text\n\nbranch = Branch(tools=[fetch_page])\n</code></pre>"},{"location":"core-concepts/tools-and-functions/#using-tools-in-operations","title":"Using Tools in Operations","text":""},{"location":"core-concepts/tools-and-functions/#operate-with-actionstrue","title":"operate() with actions=True","text":"<p>The <code>operate()</code> method supports tool calling when you set <code>actions=True</code>:</p> <pre><code>from pydantic import BaseModel\n\nclass ResearchResult(BaseModel):\n    findings: list[str]\n    sources: list[str]\n\nresult = await branch.operate(\n    instruction=\"Research renewable energy trends\",\n    response_format=ResearchResult,\n    actions=True,\n    action_strategy=\"concurrent\",  # or \"sequential\"\n)\n</code></pre>"},{"location":"core-concepts/tools-and-functions/#react-for-multi-step-tool-use","title":"ReAct() for Multi-Step Tool Use","text":"<p><code>ReAct()</code> is designed for iterative tool use with reasoning:</p> <pre><code>result = await branch.ReAct(\n    instruct={\"instruction\": \"Find and analyze the latest sales data\"},\n    max_extensions=3,\n    verbose=True,\n)\n</code></pre> <p>The model reasons about what tool to call, observes the result, and decides whether to call more tools or produce a final answer.</p>"},{"location":"core-concepts/tools-and-functions/#act-for-direct-execution","title":"act() for Direct Execution","text":"<p><code>act()</code> executes tool calls directly without involving the LLM:</p> <pre><code>responses = await branch.act(\n    action_request=[\n        {\"function\": \"calculate_sum\", \"arguments\": {\"a\": 10, \"b\": 20}},\n        {\"function\": \"search_records\", \"arguments\": {\"query\": \"revenue\"}},\n    ],\n    strategy=\"concurrent\",\n)\n</code></pre>"},{"location":"core-concepts/tools-and-functions/#the-actionmanager","title":"The ActionManager","text":"<p>The <code>ActionManager</code> (accessible via <code>branch.acts</code>) maintains the tool registry and handles invocation:</p> <pre><code># Check registered tools\nprint(branch.tools)                  # dict[str, Tool]\nprint(\"calculate_sum\" in branch.acts)  # True\n\n# Get schemas for all registered tools\nschemas = branch.acts.schema_list    # list[dict]\n</code></pre>"},{"location":"core-concepts/tools-and-functions/#functioncalling","title":"FunctionCalling","text":"<p>When a tool is invoked, the ActionManager creates a <code>FunctionCalling</code> event that tracks execution status, duration, and results:</p> <pre><code>from lionagi.protocols.action.function_calling import FunctionCalling\n\n# Internally, ActionManager does:\nfunc_call = branch.acts.match_tool(action_request)  # Creates FunctionCalling\nawait func_call.invoke()                              # Executes the function\nprint(func_call.execution.status)                     # COMPLETED or FAILED\nprint(func_call.execution.response)                   # Function return value\n</code></pre>"},{"location":"core-concepts/tools-and-functions/#mcp-tool-integration","title":"MCP Tool Integration","text":"<p>LionAGI supports Model Context Protocol (MCP) for integrating external tool servers.</p>"},{"location":"core-concepts/tools-and-functions/#registering-mcp-tools","title":"Registering MCP Tools","text":"<p>Register tools from a single MCP server:</p> <pre><code>tools = await branch.acts.register_mcp_server(\n    server_config={\"server\": \"search\"},\n    tool_names=[\"web_search\", \"news_search\"],\n)\n</code></pre>"},{"location":"core-concepts/tools-and-functions/#auto-discovery","title":"Auto-Discovery","text":"<p>Omit <code>tool_names</code> to auto-discover all tools from a server:</p> <pre><code>tools = await branch.acts.register_mcp_server(\n    server_config={\"server\": \"search\"},\n)\n</code></pre>"},{"location":"core-concepts/tools-and-functions/#loading-from-config-file","title":"Loading from Config File","text":"<p>Load multiple MCP servers from a <code>.mcp.json</code> configuration file:</p> <pre><code>all_tools = await branch.acts.load_mcp_config(\n    config_path=\"/path/to/.mcp.json\",\n    server_names=[\"search\", \"memory\"],  # or None for all servers\n)\n</code></pre>"},{"location":"core-concepts/tools-and-functions/#mcp-config-dict","title":"MCP Config Dict","text":"<p>You can also pass an MCP config dict directly when creating a Tool:</p> <pre><code>tool = Tool(mcp_config={\"web_search\": {\"server\": \"search\"}})\nbranch.register_tools(tool)\n</code></pre> <p>MCP Exclusivity</p> <p>A Tool must have either <code>func_callable</code> or <code>mcp_config</code>, not both. Providing both raises a <code>ValueError</code>.</p>"},{"location":"core-concepts/tools-and-functions/#liontool-base-class","title":"LionTool Base Class","text":"<p>For complex tools with setup/teardown logic, extend the <code>LionTool</code> base class:</p> <pre><code>from lionagi.tools.base import LionTool\n\nclass ReaderTool(LionTool):\n    \"\"\"Tool for reading files.\"\"\"\n    # Implements to_tool() which returns Tool instances\n    pass\n\nbranch = Branch(tools=[ReaderTool])\n</code></pre> <p>Built-in LionTool implementations (like <code>ReaderTool</code>) are automatically converted to <code>Tool</code> instances during registration.</p>"},{"location":"core-concepts/tools-and-functions/#next-steps","title":"Next Steps","text":"<ul> <li>Operations -- using tools with <code>operate()</code>, <code>ReAct()</code>, and <code>act()</code></li> <li>Messages and Memory -- how tool calls appear in conversation history</li> </ul>"},{"location":"for-ai-agents/","title":"For AI Agents","text":"<p>This section is written for AI coding agents -- Claude Code, Codex, Gemini CLI, and similar tools -- that need to use lionagi programmatically. The content prioritizes machine-readable structure, lookup tables, and copy-paste code patterns over narrative explanation.</p>"},{"location":"for-ai-agents/#what-lionagi-is","title":"What lionagi Is","text":"<p>lionagi is a provider-agnostic LLM orchestration SDK. It lets your agent call any LLM provider through a uniform interface, manage conversation state, invoke tools, parse structured output, and coordinate multi-step workflows as DAGs.</p>"},{"location":"for-ai-agents/#core-objects-4-things-to-know","title":"Core Objects (4 things to know)","text":"Object What It Does Import <code>Branch</code> Single conversation thread. Primary API surface. <code>from lionagi import Branch</code> <code>iModel</code> LLM provider wrapper (API or CLI). <code>from lionagi import iModel</code> <code>Session</code> Multi-branch orchestrator. Manages Branch lifecycle. <code>from lionagi import Session</code> <code>Builder</code> DAG builder for multi-step workflows. <code>from lionagi import Builder</code>"},{"location":"for-ai-agents/#branch-methods-the-api-you-will-use-most","title":"Branch Methods (the API you will use most)","text":"Method Adds to History Tool Calling Structured Output Use Case <code>chat()</code> No No Optional Low-level LLM call for orchestration <code>communicate()</code> Yes No Optional Conversational interaction <code>operate()</code> Yes Yes Optional Tool use + structured output <code>ReAct()</code> Yes Yes Optional Multi-step reasoning with tools <code>parse()</code> No No Yes Extract structured data from text <code>interpret()</code> No No No Rewrite/refine a prompt"},{"location":"for-ai-agents/#defaults","title":"Defaults","text":"<pre><code># Default provider and model (from lionagi.config.settings)\nprovider = \"openai\"\nmodel = \"gpt-4.1-mini\"\n\n# Minimal Branch -- uses defaults above\nbranch = Branch()\n\n# Explicit provider\nbranch = Branch(chat_model=iModel(provider=\"anthropic\", model=\"claude-sonnet-4-20250514\"))\n</code></pre>"},{"location":"for-ai-agents/#quick-start","title":"Quick Start","text":"<pre><code>from lionagi import Branch, iModel\n\n# Simple conversation (adds to history)\nasync with Branch() as b:\n    response = await b.communicate(\"Explain dependency injection\")\n\n# Structured output\nfrom pydantic import BaseModel\n\nclass Analysis(BaseModel):\n    summary: str\n    confidence: float\n\nresult = await b.communicate(\n    \"Analyze this code for security issues\",\n    response_format=Analysis,\n)\n# result is an Analysis instance\n\n# Tool use\ndef search_docs(query: str) -&gt; str:\n    \"\"\"Search documentation.\"\"\"\n    return f\"Results for: {query}\"\n\nb.register_tools(search_docs)\nresult = await b.operate(\n    instruction=\"Find documentation about authentication\",\n    actions=True,\n)\n</code></pre>"},{"location":"for-ai-agents/#section-contents","title":"Section Contents","text":"Page When to Read Claude Code Usage Using lionagi with CLI-based agent providers Orchestration Guide Building multi-step workflows Pattern Selection Choosing the right Branch method Self-Improvement Inspecting and debugging conversations"},{"location":"for-ai-agents/#provider-support","title":"Provider Support","text":"<p>API Providers: openai, anthropic, google (Gemini API), ollama, nvidia, perplexity, groq, openrouter</p> <p>CLI Providers (for agent-to-agent): claude_code, gemini_code, codex</p>"},{"location":"for-ai-agents/#key-behaviors","title":"Key Behaviors","text":"<ul> <li><code>Branch</code> supports <code>async with</code> for automatic log cleanup on exit.</li> <li><code>iModel</code> supports <code>async with</code> for automatic executor shutdown.</li> <li><code>chat()</code> does NOT add messages to conversation history. Use it for one-off   orchestration calls.</li> <li><code>communicate()</code> DOES add messages to history. Use it for building conversation   context.</li> <li><code>operate()</code> DOES add messages and can invoke registered tools.</li> <li>Default <code>parse_model</code> is the same as <code>chat_model</code> unless explicitly set.</li> <li>CLI endpoints (claude_code, gemini_code, codex) get fresh copies on   <code>Branch.clone()</code> to avoid session conflicts.</li> </ul>"},{"location":"for-ai-agents/claude-code-usage/","title":"CLI Agent Providers","text":"<p>lionagi integrates with CLI-based AI agents -- Claude Code, Gemini CLI, and OpenAI Codex -- as iModel providers. This enables agent-to-agent orchestration: your outer agent uses lionagi to spawn and coordinate inner agents.</p>"},{"location":"for-ai-agents/claude-code-usage/#supported-cli-providers","title":"Supported CLI Providers","text":"Provider String CLI Tool Endpoint Class <code>claude_code</code> Claude Code <code>ClaudeCodeCLIEndpoint</code> <code>gemini_code</code> Gemini CLI <code>GeminiCLIEndpoint</code> <code>codex</code> OpenAI Codex <code>CodexCLIEndpoint</code> <p>All CLI providers use <code>endpoint=\"query_cli\"</code> and require no real API key (<code>api_key</code> is ignored; the CLI tool handles auth).</p>"},{"location":"for-ai-agents/claude-code-usage/#quick-setup","title":"Quick Setup","text":""},{"location":"for-ai-agents/claude-code-usage/#claude-code","title":"Claude Code","text":"<pre><code>from lionagi import Branch, iModel\n\ncc_model = iModel(\n    provider=\"claude_code\",\n    model=\"sonnet\",                    # model hint for Claude Code\n    cwd=\"/path/to/project\",            # working directory for the agent\n    permission_mode=\"bypassPermissions\",  # or \"default\"\n    allowed_tools=[\"Read\", \"Grep\", \"Glob\"],\n    verbose_output=True,\n)\n\nbranch = Branch(chat_model=cc_model, name=\"claude_agent\")\nresponse = await branch.communicate(\"Analyze the codebase structure\")\n</code></pre>"},{"location":"for-ai-agents/claude-code-usage/#gemini-cli","title":"Gemini CLI","text":"<pre><code>gemini_model = iModel(\n    provider=\"gemini_code\",\n    model=\"gemini-2.5-pro\",\n    cwd=\"/path/to/project\",\n)\n\nbranch = Branch(chat_model=gemini_model, name=\"gemini_agent\")\nresponse = await branch.communicate(\"Review this module for issues\")\n</code></pre>"},{"location":"for-ai-agents/claude-code-usage/#codex","title":"Codex","text":"<pre><code>codex_model = iModel(\n    provider=\"codex\",\n    model=\"codex-mini\",\n    cwd=\"/path/to/project\",\n)\n\nbranch = Branch(chat_model=codex_model, name=\"codex_agent\")\nresponse = await branch.communicate(\"Refactor the authentication module\")\n</code></pre>"},{"location":"for-ai-agents/claude-code-usage/#imodel-configuration-reference","title":"iModel Configuration Reference","text":""},{"location":"for-ai-agents/claude-code-usage/#common-parameters-all-cli-providers","title":"Common Parameters (all CLI providers)","text":"<pre><code>iModel(\n    provider=\"claude_code\",          # Required: provider string\n    model=\"sonnet\",                  # Model selection hint\n    cwd=\"/path/to/project\",          # Working directory for the agent\n    verbose_output=True,             # Show agent output during execution\n    cli_include_summary=False,       # Include cost/usage summary\n    auto_finish=False,               # Auto-complete if agent doesn't finish\n)\n</code></pre>"},{"location":"for-ai-agents/claude-code-usage/#claude-code-specific-parameters","title":"Claude Code-Specific Parameters","text":"<pre><code>iModel(\n    provider=\"claude_code\",\n    model=\"sonnet\",\n    permission_mode=\"bypassPermissions\",  # \"default\" | \"acceptEdits\" | \"bypassPermissions\"\n    allowed_tools=[\"Read\", \"Grep\", \"Glob\", \"Bash\"],\n    disallowed_tools=[\"Write\"],           # Block specific tools\n    cli_display_theme=\"dark\",             # \"dark\" | \"light\"\n    max_turns=10,                         # Max conversation turns\n    max_thinking_tokens=16000,            # Thinking budget\n    continue_conversation=True,           # Continue previous session\n    system_prompt=\"You are a security auditor.\",\n    append_system_prompt=\"Focus on OWASP Top 10.\",  # Appended to existing system\n    mcp_tools=[\"browser\", \"filesystem\"],  # MCP tool names to enable\n    mcp_config=\"/path/to/.mcp.json\",      # MCP config file\n    add_dir=\"/shared/reference\",          # Extra read-only directory\n)\n</code></pre>"},{"location":"for-ai-agents/claude-code-usage/#gemini-cli-specific-parameters","title":"Gemini CLI-Specific Parameters","text":"<pre><code>iModel(\n    provider=\"gemini_code\",\n    model=\"gemini-2.5-pro\",\n    sandbox=True,                         # Safety sandboxing (default True)\n    approval_mode=\"auto_edit\",            # \"suggest\" | \"auto_edit\" | \"full_auto\"\n    # yolo=True,                          # Auto-approve all -- emits safety warning\n    debug=False,\n    include_directories=[\"/extra/src\"],   # Additional directories to include\n    system_prompt=\"You are a code analyst.\",\n)\n</code></pre>"},{"location":"for-ai-agents/claude-code-usage/#codex-cli-specific-parameters","title":"Codex CLI-Specific Parameters","text":"<pre><code>iModel(\n    provider=\"codex\",\n    model=\"gpt-5.3-codex\",\n    full_auto=True,                       # Auto-approve with workspace-write sandbox\n    sandbox=\"workspace-write\",            # \"read-only\" | \"workspace-write\" | \"danger-full-access\"\n    # bypass_approvals=True,              # Skip ALL approvals -- use with caution\n    skip_git_repo_check=False,\n    output_schema=\"/path/to/schema.json\", # JSON Schema for structured output\n    include_plan_tool=True,               # Enable planning tool\n    images=[\"screenshot.png\"],            # Attach images\n    config_overrides={\"key\": \"value\"},    # Custom config -c flags\n    system_prompt=\"You are a test engineer.\",\n)\n</code></pre>"},{"location":"for-ai-agents/claude-code-usage/#context-management","title":"Context Management","text":"<p>CLI providers have two layers of context: the CLI agent's own session (managed via <code>--resume</code>) and the Branch's MessageManager (which records all exchanges in a Pile). Both are useful.</p> <p>Within a session, the CLI agent manages its own context -- lionagi automatically stores the <code>session_id</code> from each response and passes <code>--resume</code> on subsequent calls. You don't need to manually manage what the agent sees per-call.</p> <p>Across sessions, Branch's message history and <code>progression=</code> become essential. CLI sessions can grow too long (context window limits, token costs, accumulated tool output noise). When that happens, start a fresh CLI instance and use <code>progression=</code> to select which prior context to inject as the initial prompt:</p> <pre><code># Session grew too long -- rotate to a fresh CLI instance\nbranch.chat_model = iModel(provider=\"claude_code\", model=\"sonnet\")\n\n# Use progression to carry forward only the relevant context\nrecent = list(branch.msgs.progression)[-30:]\nawait branch.communicate(\n    \"Continue the analysis from where we left off.\",\n    progression=recent,\n)\n</code></pre> <p>This pattern is especially important for long-running branches where you might strip provider-side action input/output from appearing in assistant content, then feed a curated history into a fresh session.</p> Aspect API Providers CLI Providers Per-call context <code>progression=</code> controls every call CLI agent manages via session resume Session rotation N/A (stateless) Start fresh iModel, use <code>progression=</code> for prior context Long-running Slide the progression window Rotate sessions + curate progression Branch records Source of truth Durable log + context source for fresh sessions"},{"location":"for-ai-agents/claude-code-usage/#session-lifecycle","title":"Session Lifecycle","text":"<p>CLI session state is managed automatically via <code>session_id</code>:</p> <ol> <li>First call creates a new session. The CLI returns a <code>session_id</code> in the    <code>system</code> event.</li> <li>lionagi stores the <code>session_id</code> on the endpoint.</li> <li>Subsequent calls on the same iModel pass <code>--resume</code> with that ID.</li> <li>If the resumed session gets a new ID (the CLI may reassign), lionagi    updates to the new ID automatically.</li> <li><code>Branch.clone()</code> creates a fresh copy with no shared session state.</li> <li><code>iModel.copy(share_session=True)</code> carries over the session ID.</li> </ol> <pre><code># Fresh copy, independent session\nnew_model = cc_model.copy()\n\n# Copy that resumes the same CLI session\nresumed_model = cc_model.copy(share_session=True)\n</code></pre>"},{"location":"for-ai-agents/claude-code-usage/#multi-agent-orchestration","title":"Multi-Agent Orchestration","text":""},{"location":"for-ai-agents/claude-code-usage/#workspace-isolation","title":"Workspace Isolation","text":"<p>Give each agent its own working directory to prevent file conflicts:</p> <pre><code>def create_agent(role: str, subdir: str) -&gt; iModel:\n    return iModel(\n        provider=\"claude_code\",\n        model=\"sonnet\",\n        cwd=f\"/project/.agents/{subdir}\",\n        permission_mode=\"bypassPermissions\",\n        allowed_tools=[\"Read\", \"Grep\", \"Glob\", \"Bash\"],\n    )\n\norchestrator = create_agent(\"orchestrator\", \"orc\")\nresearcher_1 = create_agent(\"researcher\", \"res1\")\nresearcher_2 = create_agent(\"researcher\", \"res2\")\n</code></pre>"},{"location":"for-ai-agents/claude-code-usage/#fan-out-pattern","title":"Fan-Out Pattern","text":"<pre><code>from lionagi import Branch, Session, Builder\nfrom lionagi.operations.fields import Instruct\n\nsession = Session()\nbuilder = Builder(\"research\")\n\n# Orchestrator generates sub-tasks\norc_branch = Branch(chat_model=orchestrator, name=\"orchestrator\")\nsession.include_branches([orc_branch])\n\nroot = builder.add_operation(\n    \"operate\",\n    branch=orc_branch,\n    instruction=\"Break this task into 3 research sub-tasks\",\n    response_format=TaskList,  # your Pydantic model\n)\n\nresult = await session.flow(builder.get_graph())\n\n# Fan out to researcher agents\nfor i, task in enumerate(result[\"operation_results\"][root].tasks):\n    agent = create_agent(\"researcher\", f\"res_{i}\")\n    res_branch = Branch(chat_model=agent, name=f\"researcher_{i}\")\n    session.include_branches([res_branch])\n\n    builder.add_operation(\n        \"communicate\",\n        branch=res_branch,\n        instruction=task.description,\n        depends_on=[root],\n    )\n\n# Execute all research in parallel\nfinal = await session.flow(builder.get_graph(), max_concurrent=3)\n</code></pre>"},{"location":"for-ai-agents/claude-code-usage/#cost-tracking","title":"Cost Tracking","text":"<p>CLI providers include cost data in model responses:</p> <pre><code>from lionagi.protocols.messages import AssistantResponse\n\n# After a communicate/operate call, inspect the last message\nlast_msg = branch.messages[-1]\nif isinstance(last_msg, AssistantResponse):\n    cost = last_msg.model_response.get(\"total_cost_usd\", 0)\n    result_text = last_msg.model_response.get(\"result\", \"\")\n    summary = last_msg.model_response.get(\"summary\", \"\")\n</code></pre>"},{"location":"for-ai-agents/claude-code-usage/#event-handlers-claude-code-only","title":"Event Handlers (Claude Code only)","text":"<p>Claude Code supports streaming event handlers for fine-grained control:</p> <pre><code>cc_model = iModel(provider=\"claude_code\", model=\"sonnet\")\n\n# Access the endpoint to set handlers\ncc_model.endpoint.update_handlers(\n    on_text=lambda chunk: print(f\"Text: {chunk.text}\"),\n    on_tool_use=lambda chunk: print(f\"Tool: {chunk}\"),\n    on_thinking=lambda chunk: print(f\"Thinking: {chunk.text}\"),\n    on_final=lambda session: print(f\"Done: {session.result}\"),\n)\n</code></pre> <p>Available handler keys by provider:</p> Provider Handlers <code>claude_code</code> <code>on_thinking</code>, <code>on_text</code>, <code>on_tool_use</code>, <code>on_tool_result</code>, <code>on_system</code>, <code>on_final</code> <code>gemini_code</code> <code>on_text</code>, <code>on_tool_use</code>, <code>on_tool_result</code>, <code>on_final</code> <code>codex</code> <code>on_text</code>, <code>on_tool_use</code>, <code>on_tool_result</code>, <code>on_final</code>"},{"location":"for-ai-agents/claude-code-usage/#async-context-manager","title":"Async Context Manager","text":"<p>Both <code>Branch</code> and <code>iModel</code> support async context managers:</p> <pre><code>async with iModel(provider=\"claude_code\", model=\"sonnet\") as model:\n    async with Branch(chat_model=model) as branch:\n        result = await branch.communicate(\"Analyze this code\")\n    # Branch dumps logs on exit\n\n# iModel executor stops on exit\n</code></pre>"},{"location":"for-ai-agents/claude-code-usage/#error-handling","title":"Error Handling","text":"<pre><code>try:\n    result = await branch.communicate(\"Complex analysis task\")\nexcept ValueError as e:\n    if \"Failed to invoke API call\" in str(e):\n        # CLI tool likely timed out or crashed\n        # Default timeout is 18000 seconds (5 hours)\n        pass\n</code></pre>"},{"location":"for-ai-agents/claude-code-usage/#key-differences-cli-vs-api-providers","title":"Key Differences: CLI vs API Providers","text":"Aspect API Providers CLI Providers Auth API key required CLI tool handles auth Context owner Branch (Progression controls every call) CLI agent within session; Branch across sessions Latency Low (HTTP) Higher (subprocess) Session state Stateless Persistent session_id (auto-updated) Concurrency High (100 queue capacity) Low (default 3) Clone behavior Shared endpoint Fresh endpoint copy Timeout Provider default 18000s (5 hours) Tool calling Via function schemas Via CLI tool's built-in tools <code>progression=</code> Controls LLM context window per call Used when rotating to a fresh session (context injection)"},{"location":"for-ai-agents/orchestration-guide/","title":"Orchestration Guide","text":"<p>How to build single-step and multi-step workflows with lionagi.</p>"},{"location":"for-ai-agents/orchestration-guide/#architecture-overview","title":"Architecture Overview","text":"<pre><code>Branch (single conversation)\n  |- chat_model: iModel      -- LLM for conversation\n  |- parse_model: iModel      -- LLM for structured parsing (defaults to chat_model)\n  |- messages: Pile[RoledMessage]  -- conversation history\n  |- tools: dict[str, Tool]   -- registered callable tools\n  |- logs: Pile[Log]          -- activity logs\n\nSession (multi-branch orchestrator)\n  |- branches: Pile[Branch]   -- all managed branches\n  |- default_branch: Branch   -- fallback branch\n  |- flow(graph) -&gt; dict      -- execute a DAG workflow\n\nBuilder (OperationGraphBuilder)\n  |- add_operation(...)       -- add a node to the DAG\n  |- add_aggregation(...)     -- add a node that collects from multiple sources\n  |- expand_from_result(...)  -- dynamically expand based on results\n  |- get_graph() -&gt; Graph     -- return the graph for execution\n</code></pre>"},{"location":"for-ai-agents/orchestration-guide/#branch-the-primary-api-surface","title":"Branch: The Primary API Surface","text":"<p>Branch is a facade over four managers. All LLM operations are Branch methods.</p>"},{"location":"for-ai-agents/orchestration-guide/#method-signatures","title":"Method Signatures","text":"<pre><code># Low-level call -- does NOT add to history\nawait branch.chat(\n    instruction=\"...\",          # main prompt\n    guidance=\"...\",             # system-level guidance\n    context=\"...\",              # additional context\n    response_format=MyModel,   # optional structured output\n    imodel=alt_model,          # optional model override\n    images=[...],              # optional images\n    **kwargs,                  # passed to LLM (temperature, etc.)\n) -&gt; tuple[Instruction, AssistantResponse]\n# Returns (instruction_msg, response_msg) -- neither added to history\n\n# Conversational -- DOES add to history\nawait branch.communicate(\n    instruction=\"...\",          # main prompt (positional OK)\n    guidance=\"...\",             # system-level guidance\n    context=\"...\",              # additional context\n    response_format=MyModel,   # optional structured output\n    chat_model=alt_model,      # optional model override\n    parse_model=parse_model,   # optional parse model override\n    num_parse_retries=3,       # retries for structured parsing\n    clear_messages=False,      # clear history before this call\n    **kwargs,\n) -&gt; str | BaseModel | dict | None\n\n# Tool use + structured output -- DOES add to history\nawait branch.operate(\n    instruction=\"...\",          # or instruct=Instruct(...)\n    guidance=\"...\",\n    context=\"...\",\n    response_format=MyModel,   # optional structured output\n    actions=True,              # enable tool calling\n    tools=[my_func],           # register tools for this call\n    reason=True,               # request chain-of-thought\n    invoke_actions=True,       # auto-invoke requested tools\n    action_strategy=\"concurrent\",  # or \"sequential\"\n    **kwargs,\n) -&gt; list | BaseModel | None | dict | str\n\n# Multi-step reasoning -- DOES add to history\nawait branch.ReAct(\n    instruct=Instruct(instruction=\"...\", guidance=\"...\"),\n    tools=[my_func],           # available tools\n    response_format=MyModel,   # final output format\n    extension_allowed=True,    # allow multi-step expansion\n    max_extensions=3,          # max reasoning steps (capped at 5)\n    verbose=False,             # print intermediate steps\n    **kwargs,\n) -&gt; Any | tuple[Any, list]   # result, or (result, analyses) if return_analysis=True\n</code></pre>"},{"location":"for-ai-agents/orchestration-guide/#when-to-use-which-method","title":"When to Use Which Method","text":"<pre><code>Need simple LLM response, no history?          -&gt; chat()\nNeed conversational context?                   -&gt; communicate()\nNeed structured output from conversation?      -&gt; communicate(response_format=Model)\nNeed tool calling?                             -&gt; operate(actions=True)\nNeed structured output + tools?               -&gt; operate(response_format=Model, actions=True)\nNeed multi-step reasoning with tools?          -&gt; ReAct()\nNeed to parse existing text into a model?      -&gt; parse(text, response_format=Model)\nNeed to refine/rewrite a prompt?               -&gt; interpret(text)\n</code></pre>"},{"location":"for-ai-agents/orchestration-guide/#concurrency-patterns","title":"Concurrency Patterns","text":"<p>lionagi provides structured concurrency primitives in <code>lionagi.ln.concurrency</code>.</p>"},{"location":"for-ai-agents/orchestration-guide/#gather-run-awaitables-concurrently","title":"gather -- Run awaitables concurrently","text":"<pre><code>from lionagi.ln.concurrency import gather\n\nresults = await gather(\n    branch.communicate(\"Task A\"),\n    branch.communicate(\"Task B\"),\n    branch.communicate(\"Task C\"),\n    return_exceptions=False,  # True to collect errors instead of failing fast\n)\n# results: list in same order as input\n</code></pre>"},{"location":"for-ai-agents/orchestration-guide/#race-first-to-complete-wins","title":"race -- First to complete wins","text":"<pre><code>from lionagi.ln.concurrency import race\n\nresult = await race(\n    branch.communicate(\"Fast approach\"),\n    branch.communicate(\"Thorough approach\"),\n)\n# Returns the first result; cancels the rest\n</code></pre>"},{"location":"for-ai-agents/orchestration-guide/#bounded_map-concurrency-limited-parallel-map","title":"bounded_map -- Concurrency-limited parallel map","text":"<pre><code>from lionagi.ln.concurrency import bounded_map\n\ntasks = [\"Analyze module A\", \"Analyze module B\", \"Analyze module C\"]\nresults = await bounded_map(\n    lambda task: branch.communicate(task),\n    tasks,\n    limit=2,  # max 2 concurrent\n)\n</code></pre>"},{"location":"for-ai-agents/orchestration-guide/#retry-exponential-backoff","title":"retry -- Exponential backoff","text":"<pre><code>from lionagi.ln.concurrency import retry\n\nresult = await retry(\n    lambda: branch.communicate(\"Flaky task\"),\n    attempts=3,\n    base_delay=0.5,\n    retry_on=(ValueError,),\n)\n</code></pre>"},{"location":"for-ai-agents/orchestration-guide/#completionstream-process-results-as-they-arrive","title":"CompletionStream -- Process results as they arrive","text":"<pre><code>from lionagi.ln.concurrency import CompletionStream\n\ntasks = [branch.communicate(f\"Task {i}\") for i in range(10)]\nasync with CompletionStream(tasks, limit=3) as stream:\n    async for idx, result in stream:\n        print(f\"Task {idx} completed: {result[:50]}\")\n</code></pre>"},{"location":"for-ai-agents/orchestration-guide/#alcall-bcall-batch-processing","title":"alcall / bcall -- Batch processing","text":"<pre><code>from lionagi.ln import alcall, bcall\n\n# Apply function to list with concurrency control\nresults = await alcall(\n    [\"item1\", \"item2\", \"item3\"],\n    lambda x: branch.communicate(f\"Process {x}\"),\n    max_concurrent=2,\n    retry_default=3,\n)\n\n# Batch processing with yields per batch\nasync for batch_results in bcall(\n    large_list,\n    process_fn,\n    batch_size=10,\n):\n    handle(batch_results)\n</code></pre>"},{"location":"for-ai-agents/orchestration-guide/#session-builder-dag-workflows","title":"Session + Builder: DAG Workflows","text":"<p>For workflows with dependencies between operations, use Session + Builder.</p>"},{"location":"for-ai-agents/orchestration-guide/#basic-sequential-pipeline","title":"Basic Sequential Pipeline","text":"<pre><code>from lionagi import Branch, Session, Builder, iModel\n\nsession = Session()\nbuilder = Builder(\"pipeline\")\n\nbranch = Branch(chat_model=iModel(provider=\"openai\", model=\"gpt-4.1-mini\"))\nsession.include_branches([branch])\n\nstep1 = builder.add_operation(\n    \"communicate\", branch=branch,\n    instruction=\"Research the topic\",\n)\n\nstep2 = builder.add_operation(\n    \"communicate\", branch=branch,\n    instruction=\"Analyze findings\",\n    depends_on=[step1],\n)\n\nstep3 = builder.add_operation(\n    \"communicate\", branch=branch,\n    instruction=\"Write report\",\n    depends_on=[step2],\n)\n\nresult = await session.flow(builder.get_graph())\n# result[\"operation_results\"][step1]  -- result from step 1\n# result[\"operation_results\"][step2]  -- result from step 2\n# result[\"completed_operations\"]      -- list of completed node IDs\n# result[\"skipped_operations\"]        -- list of skipped node IDs\n</code></pre>"},{"location":"for-ai-agents/orchestration-guide/#fan-out-fan-in","title":"Fan-Out / Fan-In","text":"<pre><code>builder = Builder(\"fan_out\")\n\n# Independent analyses run in parallel\nsecurity = builder.add_operation(\n    \"communicate\", branch=sec_branch,\n    instruction=\"Security analysis\",\n)\nperf = builder.add_operation(\n    \"communicate\", branch=perf_branch,\n    instruction=\"Performance analysis\",\n)\nstyle = builder.add_operation(\n    \"communicate\", branch=style_branch,\n    instruction=\"Code style analysis\",\n)\n\n# Aggregation collects all results\nsynthesis = builder.add_aggregation(\n    \"communicate\", branch=main_branch,\n    source_node_ids=[security, perf, style],\n    instruction=\"Synthesize all analyses into a report\",\n)\n\nresult = await session.flow(builder.get_graph(), max_concurrent=3)\n</code></pre>"},{"location":"for-ai-agents/orchestration-guide/#dynamic-expansion","title":"Dynamic Expansion","text":"<pre><code>from lionagi.operations.fields import LIST_INSTRUCT_FIELD_MODEL\n\n# Step 1: Generate sub-tasks\nroot = builder.add_operation(\n    \"operate\", branch=branch,\n    instruction=\"Generate 5 research questions\",\n    field_models=[LIST_INSTRUCT_FIELD_MODEL],\n)\n\nresult = await session.flow(builder.get_graph())\nsub_tasks = result[\"operation_results\"][root].instruct_models\n\n# Step 2: Expand graph dynamically\nnew_ids = builder.expand_from_result(\n    sub_tasks,\n    source_node_id=root,\n    operation=\"communicate\",\n)\n\n# Step 3: Execute expanded graph\nfinal = await session.flow(builder.get_graph(), max_concurrent=5)\n</code></pre>"},{"location":"for-ai-agents/orchestration-guide/#builder-api-reference","title":"Builder API Reference","text":"<pre><code>builder = Builder(name=\"workflow_name\")\n\n# Add a single operation\nnode_id = builder.add_operation(\n    operation,          # \"communicate\" | \"operate\" | \"chat\" | \"ReAct\"\n    node_id=None,       # optional reference ID\n    depends_on=None,    # list of node IDs this depends on\n    inherit_context=False,  # inherit conversation from dependency\n    branch=None,        # Branch to execute on\n    **parameters,       # passed to the Branch method\n)\n\n# Add aggregation node\nnode_id = builder.add_aggregation(\n    operation,\n    source_node_ids=None,  # defaults to current head nodes\n    inherit_context=False,\n    branch=None,\n    **parameters,\n)\n\n# Expand based on results\nnew_ids = builder.expand_from_result(\n    items,              # list of items (e.g., Instruct models)\n    source_node_id,     # parent node ID\n    operation,          # operation for each item\n    strategy=ExpansionStrategy.CONCURRENT,\n    **shared_params,\n)\n\n# Get the graph for execution\ngraph = builder.get_graph()\n\n# Inspect state\nstate = builder.visualize_state()\n# {\"total_nodes\": N, \"executed_nodes\": M, \"current_heads\": [...]}\n</code></pre>"},{"location":"for-ai-agents/orchestration-guide/#sessionflow-parameters","title":"Session.flow() Parameters","text":"<pre><code>result = await session.flow(\n    graph,                  # Graph from builder.get_graph()\n    context=None,           # dict of initial context\n    parallel=True,          # enable parallel execution\n    max_concurrent=5,       # max concurrent operations\n    verbose=False,          # enable logging\n    default_branch=None,    # override default branch\n)\n</code></pre>"},{"location":"for-ai-agents/orchestration-guide/#error-handling","title":"Error Handling","text":""},{"location":"for-ai-agents/orchestration-guide/#branch-level","title":"Branch-Level","text":"<pre><code>try:\n    result = await branch.communicate(\"Task\")\nexcept ValueError as e:\n    # API call failure, timeout, or parsing error\n    print(f\"Failed: {e}\")\n</code></pre>"},{"location":"for-ai-agents/orchestration-guide/#workflow-level","title":"Workflow-Level","text":"<pre><code>result = await session.flow(builder.get_graph())\n\n# Check for skipped operations\nif result.get(\"skipped_operations\"):\n    for skipped_id in result[\"skipped_operations\"]:\n        print(f\"Skipped: {skipped_id}\")\n\n# Check specific operation results\nfor node_id, op_result in result.get(\"operation_results\", {}).items():\n    if op_result is None:\n        print(f\"Operation {node_id} returned None\")\n</code></pre>"},{"location":"for-ai-agents/orchestration-guide/#structured-output-validation","title":"Structured Output Validation","text":"<pre><code># communicate() with response_format handles parse retries internally\nresult = await branch.communicate(\n    \"Extract entities\",\n    response_format=EntityList,\n    num_parse_retries=3,  # retry parsing up to 3 times\n)\n# result is EntityList or None if all retries fail\n\n# operate() with handle_validation control\nresult = await branch.operate(\n    instruction=\"Extract entities\",\n    response_format=EntityList,\n    handle_validation=\"return_none\",  # \"raise\" | \"return_value\" | \"return_none\"\n)\n</code></pre>"},{"location":"for-ai-agents/orchestration-guide/#registering-tools","title":"Registering Tools","text":"<pre><code># Function tools -- auto-generates schema from signature\ndef multiply(a: int, b: int) -&gt; int:\n    \"\"\"Multiply two numbers.\"\"\"\n    return a * b\n\nasync def fetch_data(url: str) -&gt; str:\n    \"\"\"Fetch data from URL.\"\"\"\n    ...\n\nbranch.register_tools([multiply, fetch_data])\n\n# Then use with operate()\nresult = await branch.operate(\n    instruction=\"What is 6 times 7?\",\n    actions=True,\n)\n</code></pre>"},{"location":"for-ai-agents/orchestration-guide/#system-messages","title":"System Messages","text":"<pre><code># Set at construction\nbranch = Branch(system=\"You are a code reviewer.\")\n\n# With datetime\nbranch = Branch(\n    system=\"You are a code reviewer.\",\n    system_datetime=True,  # includes current timestamp\n)\n\n# With the built-in Lion system message\nbranch = Branch(\n    system=\"Additional instructions here\",\n    use_lion_system_message=True,\n)\n</code></pre>"},{"location":"for-ai-agents/pattern-selection/","title":"Pattern Selection","text":"<p>Decision trees and lookup tables for choosing the right lionagi API for your task.</p>"},{"location":"for-ai-agents/pattern-selection/#primary-decision-tree","title":"Primary Decision Tree","text":"<pre><code>What do you need?\n|\n+-- Single LLM call, no conversation state needed?\n|   +-- chat()\n|\n+-- Conversation with history tracking?\n|   |\n|   +-- Plain text response?\n|   |   +-- communicate(instruction)\n|   |\n|   +-- Structured output (Pydantic model)?\n|   |   +-- communicate(instruction, response_format=Model)\n|   |\n|   +-- Tool calling needed?\n|   |   |\n|   |   +-- Single-step tool use?\n|   |   |   +-- operate(instruction, actions=True)\n|   |   |\n|   |   +-- Multi-step reasoning + tools?\n|   |       +-- ReAct(instruct)\n|   |\n|   +-- Structured output + tool calling?\n|       +-- operate(instruction, response_format=Model, actions=True)\n|\n+-- Parse existing text into a model?\n|   +-- parse(text, response_format=Model)\n|\n+-- Rewrite/improve a prompt?\n|   +-- interpret(text)\n|\n+-- Multiple operations with dependencies?\n|   +-- Session + Builder\n|\n+-- Multiple independent operations in parallel?\n    +-- gather() or asyncio.gather()\n</code></pre>"},{"location":"for-ai-agents/pattern-selection/#method-lookup-table","title":"Method Lookup Table","text":"Scenario Method Key Parameters Ask a question, get a string <code>communicate(instruction)</code> -- Ask a question, get structured data <code>communicate(instruction, response_format=Model)</code> <code>num_parse_retries=3</code> Call tools based on instruction <code>operate(instruction, actions=True)</code> <code>tools=[fn]</code>, <code>action_strategy=\"concurrent\"</code> Tools + structured output <code>operate(instruction, actions=True, response_format=Model)</code> <code>reason=True</code> Multi-step reasoning <code>ReAct(instruct)</code> <code>max_extensions=3</code>, <code>extension_allowed=True</code> Extract data from raw text <code>parse(text, response_format=Model)</code> <code>max_retries=3</code>, <code>fuzzy_match=True</code> Refine a prompt <code>interpret(text)</code> <code>domain=\"...\", style=\"...\"</code> Orchestration call (no history) <code>chat(instruction)</code> <code>response_format=Model</code>"},{"location":"for-ai-agents/pattern-selection/#concurrency-decision-tree","title":"Concurrency Decision Tree","text":"<pre><code>How many operations?\n|\n+-- Just one?\n|   +-- Use the method directly: await branch.communicate(...)\n|\n+-- Multiple, all independent?\n|   |\n|   +-- Small number (2-5)?\n|   |   +-- gather(branch.communicate(\"A\"), branch.communicate(\"B\"))\n|   |\n|   +-- Large number, need concurrency limit?\n|   |   +-- bounded_map(fn, items, limit=N)\n|   |\n|   +-- Want results as they complete?\n|   |   +-- CompletionStream(tasks, limit=N)\n|   |\n|   +-- Want only the fastest result?\n|       +-- race(branch.communicate(\"A\"), branch.communicate(\"B\"))\n|\n+-- Operations have dependencies?\n|   +-- Session + Builder with depends_on\n|\n+-- Need retry on failure?\n    +-- retry(fn, attempts=3, retry_on=(ValueError,))\n</code></pre>"},{"location":"for-ai-agents/pattern-selection/#workflow-complexity-decision","title":"Workflow Complexity Decision","text":"<pre><code>Is the task a single prompt?\n|\n+-- Yes -&gt; Branch method directly\n|\n+-- No -&gt; Are there dependencies between steps?\n    |\n    +-- No -&gt; gather() or bounded_map()\n    |\n    +-- Yes -&gt; Do results of one step determine what steps come next?\n        |\n        +-- No -&gt; Builder with static depends_on\n        |\n        +-- Yes -&gt; Builder + expand_from_result() (dynamic expansion)\n</code></pre>"},{"location":"for-ai-agents/pattern-selection/#structured-output-patterns","title":"Structured Output Patterns","text":""},{"location":"for-ai-agents/pattern-selection/#pattern-1-simple-extraction-from-conversation","title":"Pattern 1: Simple extraction from conversation","text":"<pre><code>from pydantic import BaseModel\n\nclass Entities(BaseModel):\n    people: list[str]\n    organizations: list[str]\n\nresult = await branch.communicate(\n    \"Extract all entities from this text: ...\",\n    response_format=Entities,\n)\n# result: Entities(people=[...], organizations=[...])\n</code></pre>"},{"location":"for-ai-agents/pattern-selection/#pattern-2-tool-use-with-structured-final-output","title":"Pattern 2: Tool use with structured final output","text":"<pre><code>class Report(BaseModel):\n    findings: list[str]\n    recommendation: str\n\nbranch.register_tools([search_docs, check_status])\nresult = await branch.operate(\n    instruction=\"Investigate the issue and write a report\",\n    actions=True,\n    response_format=Report,\n    reason=True,  # include chain-of-thought\n)\n# result: Report(findings=[...], recommendation=\"...\")\n</code></pre>"},{"location":"for-ai-agents/pattern-selection/#pattern-3-multi-step-reasoning-with-structured-output","title":"Pattern 3: Multi-step reasoning with structured output","text":"<pre><code>from lionagi.operations.fields import Instruct\n\nresult = await branch.ReAct(\n    instruct=Instruct(\n        instruction=\"Research and analyze the market opportunity\",\n        guidance=\"Use available tools to gather data before concluding\",\n    ),\n    tools=[search_market_data, get_competitors],\n    response_format=MarketAnalysis,\n    max_extensions=3,\n)\n</code></pre>"},{"location":"for-ai-agents/pattern-selection/#pattern-4-parse-existing-text-no-llm-conversation","title":"Pattern 4: Parse existing text (no LLM conversation)","text":"<pre><code>raw_text = \"\"\"\nName: John Smith\nRole: Engineer\nDepartment: Backend\n\"\"\"\n\nresult = await branch.parse(\n    raw_text,\n    response_format=Employee,\n    fuzzy_match=True,        # handle approximate field names\n    max_retries=3,\n)\n</code></pre>"},{"location":"for-ai-agents/pattern-selection/#branch-configuration-patterns","title":"Branch Configuration Patterns","text":""},{"location":"for-ai-agents/pattern-selection/#single-purpose-branch","title":"Single-purpose branch","text":"<pre><code>reviewer = Branch(\n    system=\"You are a senior code reviewer. Focus on correctness and security.\",\n    chat_model=iModel(provider=\"anthropic\", model=\"claude-sonnet-4-20250514\"),\n)\n</code></pre>"},{"location":"for-ai-agents/pattern-selection/#multi-model-branch-different-models-for-chat-vs-parse","title":"Multi-model branch (different models for chat vs parse)","text":"<pre><code>branch = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4.1\"),\n    parse_model=iModel(provider=\"openai\", model=\"gpt-4.1-mini\"),\n)\n</code></pre>"},{"location":"for-ai-agents/pattern-selection/#branch-with-tools-pre-registered","title":"Branch with tools pre-registered","text":"<pre><code>branch = Branch(\n    system=\"You have access to search and file tools.\",\n    tools=[search_docs, read_file, write_file],\n)\nresult = await branch.operate(\n    instruction=\"Find and fix the bug in auth.py\",\n    actions=True,\n)\n</code></pre>"},{"location":"for-ai-agents/pattern-selection/#common-anti-patterns","title":"Common Anti-Patterns","text":""},{"location":"for-ai-agents/pattern-selection/#using-chat-when-you-need-history","title":"Using chat() when you need history","text":"<pre><code># Wrong: chat() does not add to history, second call lacks context\nawait branch.chat(\"What is X?\")\nawait branch.chat(\"Tell me more about it\")  # \"it\" has no referent\n\n# Right: communicate() maintains conversation\nawait branch.communicate(\"What is X?\")\nawait branch.communicate(\"Tell me more about it\")  # sees previous exchange\n</code></pre>"},{"location":"for-ai-agents/pattern-selection/#using-operate-when-communicate-suffices","title":"Using operate() when communicate() suffices","text":"<pre><code># Wrong: operate() overhead when no tools are needed\nawait branch.operate(instruction=\"Summarize this text\")\n\n# Right: communicate() is simpler when no tools/actions are involved\nawait branch.communicate(\"Summarize this text\")\n</code></pre>"},{"location":"for-ai-agents/pattern-selection/#sequential-when-parallel-is-possible","title":"Sequential when parallel is possible","text":"<pre><code># Wrong: sequential independent tasks\nr1 = await branch.communicate(\"Analyze security\")\nr2 = await branch.communicate(\"Analyze performance\")\n\n# Right: parallel independent tasks\nfrom lionagi.ln.concurrency import gather\nr1, r2 = await gather(\n    branch.communicate(\"Analyze security\"),\n    branch.communicate(\"Analyze performance\"),\n)\n</code></pre>"},{"location":"for-ai-agents/pattern-selection/#over-engineering-with-builder","title":"Over-engineering with Builder","text":"<pre><code># Wrong: Builder for a single operation\nbuilder = Builder(\"simple\")\nop = builder.add_operation(\"communicate\", branch=branch, instruction=\"Hello\")\nresult = await session.flow(builder.get_graph())\n\n# Right: direct call\nresult = await branch.communicate(\"Hello\")\n</code></pre>"},{"location":"for-ai-agents/pattern-selection/#quick-reference-card","title":"Quick Reference Card","text":"<pre><code>from lionagi import Branch, Session, Builder, iModel\nfrom lionagi.ln.concurrency import gather, race, bounded_map, retry, CompletionStream\nfrom lionagi.operations.fields import Instruct, LIST_INSTRUCT_FIELD_MODEL\nfrom pydantic import BaseModel\n\n# -- Setup --\nbranch = Branch(chat_model=iModel(provider=\"openai\", model=\"gpt-4.1-mini\"))\n\n# -- Single operations --\ntext     = await branch.communicate(\"question\")\nmodel    = await branch.communicate(\"question\", response_format=MyModel)\ntool_res = await branch.operate(instruction=\"do X\", actions=True)\nreact_res = await branch.ReAct(Instruct(instruction=\"solve X\"), tools=[fn])\n\n# -- Parallel --\nresults = await gather(branch.communicate(\"A\"), branch.communicate(\"B\"))\nfastest = await race(branch.communicate(\"A\"), branch.communicate(\"B\"))\nmapped  = await bounded_map(lambda x: branch.communicate(x), items, limit=3)\n\n# -- DAG workflow --\nsession = Session()\nbuilder = Builder(\"workflow\")\nsession.include_branches([branch])\ns1 = builder.add_operation(\"communicate\", branch=branch, instruction=\"step 1\")\ns2 = builder.add_operation(\"communicate\", branch=branch, instruction=\"step 2\",\n                           depends_on=[s1])\nresult = await session.flow(builder.get_graph())\n</code></pre>"},{"location":"for-ai-agents/self-improvement/","title":"Self-Improvement","text":"<p>How to inspect, debug, and adapt your lionagi usage by examining conversation state, logs, and serialization.</p>"},{"location":"for-ai-agents/self-improvement/#inspecting-conversation-state","title":"Inspecting Conversation State","text":""},{"location":"for-ai-agents/self-improvement/#branchmessages-the-conversation-history","title":"Branch.messages -- The Conversation History","text":"<p><code>branch.messages</code> is a <code>Pile[RoledMessage]</code> containing all messages in the conversation. Access it to review what has been sent and received.</p> <pre><code># Number of messages\nlen(branch.messages)\n\n# Iterate all messages\nfor msg in branch.messages:\n    print(f\"[{msg.role}] {msg.content[:100]}\")\n\n# Get the last message\nlast = branch.messages[-1]\n\n# Access by UUID\nmsg = branch.messages[some_uuid]\n</code></pre>"},{"location":"for-ai-agents/self-improvement/#message-types","title":"Message Types","text":"<pre><code>from lionagi.protocols.messages import (\n    System,              # system prompt\n    Instruction,         # user instruction\n    AssistantResponse,   # LLM response\n    ActionRequest,       # tool call from LLM\n    ActionResponse,      # tool result back to LLM\n)\n\n# Check message type\nfrom lionagi.protocols.messages import AssistantResponse\nif isinstance(branch.messages[-1], AssistantResponse):\n    response = branch.messages[-1]\n    print(response.content)          # response text\n    print(response.model_response)   # raw provider response dict\n</code></pre>"},{"location":"for-ai-agents/self-improvement/#system-message","title":"System Message","text":"<pre><code># Read the current system message\nif branch.system:\n    print(branch.system.content)\n</code></pre>"},{"location":"for-ai-agents/self-improvement/#convert-to-dataframe","title":"Convert to DataFrame","text":"<pre><code>df = branch.to_df()\n# Columns include: role, sender, recipient, content, created_at, etc.\n</code></pre>"},{"location":"for-ai-agents/self-improvement/#inspecting-logs","title":"Inspecting Logs","text":""},{"location":"for-ai-agents/self-improvement/#branchlogs-activity-logs","title":"Branch.logs -- Activity Logs","text":"<p><code>branch.logs</code> is a <code>Pile[Log]</code> containing API call logs, tool invocations, and other activity records.</p> <pre><code># Number of log entries\nlen(branch.logs)\n\n# Iterate logs\nfor log in branch.logs:\n    print(log.content)  # dict with event details\n\n# Dump logs to file\nbranch.dump_logs(persist_path=\"./debug_logs.json\", clear=False)\n\n# Async version\nawait branch.adump_logs(persist_path=\"./debug_logs.json\", clear=False)\n</code></pre>"},{"location":"for-ai-agents/self-improvement/#using-async-context-manager-for-auto-cleanup","title":"Using Async Context Manager for Auto-Cleanup","text":"<pre><code>async with Branch() as b:\n    await b.communicate(\"First question\")\n    await b.communicate(\"Follow-up question\")\n    # On exit, logs are automatically dumped and cleared\n</code></pre>"},{"location":"for-ai-agents/self-improvement/#serialization-save-and-restore-state","title":"Serialization: Save and Restore State","text":""},{"location":"for-ai-agents/self-improvement/#branch-serialization","title":"Branch Serialization","text":"<pre><code># Save branch state to dict\nstate = branch.to_dict()\n# state contains: messages, logs, chat_model, parse_model, system, log_config, metadata\n\n# Restore from dict\nrestored = Branch.from_dict(state)\n# restored has the same messages, models, and configuration\n\n# Save to JSON file\nimport json\nwith open(\"branch_state.json\", \"w\") as f:\n    json.dump(state, f, default=str)\n\n# Load from JSON file\nwith open(\"branch_state.json\") as f:\n    data = json.load(f)\nrestored = Branch.from_dict(data)\n</code></pre>"},{"location":"for-ai-agents/self-improvement/#what-gets-serialized","title":"What Gets Serialized","text":"Field Included Notes messages Yes Full conversation history logs Yes All activity logs chat_model Yes Provider, model, endpoint config parse_model Yes Provider, model, endpoint config system Yes System message if set log_config Yes Logger configuration metadata Yes Including clone_from info registered tools No Re-register after deserialization"},{"location":"for-ai-agents/self-improvement/#cloning-explore-alternatives","title":"Cloning: Explore Alternatives","text":""},{"location":"for-ai-agents/self-improvement/#branchclone-fork-a-conversation","title":"Branch.clone() -- Fork a Conversation","text":"<p><code>clone()</code> creates a new Branch with the same messages, system prompt, tools, and model configuration. Use it to explore alternative conversation paths without affecting the original.</p> <pre><code># Synchronous clone\nalt_branch = branch.clone()\n\n# Async clone (acquires message lock)\nalt_branch = await branch.aclone()\n\n# Clone with a specific sender ID\nalt_branch = branch.clone(sender=some_id)\n</code></pre>"},{"location":"for-ai-agents/self-improvement/#clone-behavior-by-endpoint-type","title":"Clone Behavior by Endpoint Type","text":"Endpoint Type Clone Behavior API (openai, anthropic, etc.) Shared endpoint (same connection pool) CLI (claude_code, gemini_code, codex) Fresh endpoint copy (independent session)"},{"location":"for-ai-agents/self-improvement/#exploring-alternatives","title":"Exploring Alternatives","text":"<pre><code># Original conversation\nawait branch.communicate(\"Analyze this code for bugs\")\n\n# Fork and try a different approach\nalt = branch.clone()\nawait alt.communicate(\"Now focus specifically on security vulnerabilities\")\n\n# Compare results\noriginal_response = branch.messages[-1].content\nalternative_response = alt.messages[-1].content\n</code></pre>"},{"location":"for-ai-agents/self-improvement/#sessionsplit-fork-within-a-session","title":"Session.split() -- Fork Within a Session","text":"<pre><code>session = Session()\nsession.include_branches([branch])\n\n# Split creates a clone and adds it to the session\nforked = session.split(branch)\n# forked is now managed by the session alongside the original\n</code></pre>"},{"location":"for-ai-agents/self-improvement/#debugging-patterns","title":"Debugging Patterns","text":""},{"location":"for-ai-agents/self-improvement/#inspect-what-the-llm-received","title":"Inspect What the LLM Received","text":"<pre><code># Get the full message sequence that was sent\nfor msg in branch.messages:\n    print(f\"Role: {msg.role}\")\n    print(f\"Content: {msg.content}\")\n    print(f\"Created: {msg.created_at}\")\n    print(\"---\")\n</code></pre>"},{"location":"for-ai-agents/self-improvement/#inspect-api-call-details","title":"Inspect API Call Details","text":"<pre><code># Logs contain raw API call information\nfor log in branch.logs:\n    content = log.content\n    if isinstance(content, dict):\n        # Check for API payload\n        if \"payload\" in content:\n            print(f\"Request: {content['payload']}\")\n        # Check for response\n        if \"response\" in content:\n            print(f\"Response: {content['response']}\")\n</code></pre>"},{"location":"for-ai-agents/self-improvement/#check-token-usage-cli-providers","title":"Check Token Usage (CLI providers)","text":"<pre><code>from lionagi.protocols.messages import AssistantResponse\n\nfor msg in branch.messages:\n    if isinstance(msg, AssistantResponse):\n        resp = msg.model_response\n        if isinstance(resp, dict):\n            cost = resp.get(\"total_cost_usd\")\n            if cost:\n                print(f\"Cost: ${cost:.4f}\")\n</code></pre>"},{"location":"for-ai-agents/self-improvement/#validate-structured-output-quality","title":"Validate Structured Output Quality","text":"<pre><code>from pydantic import BaseModel, ValidationError\n\nclass Expected(BaseModel):\n    summary: str\n    score: float\n\n# Test parsing reliability\nsuccesses = 0\nfor i in range(5):\n    alt = branch.clone()\n    result = await alt.communicate(\n        \"Score this code quality\",\n        response_format=Expected,\n    )\n    if isinstance(result, Expected):\n        successes += 1\n        print(f\"Trial {i}: score={result.score}\")\n    else:\n        print(f\"Trial {i}: parse failed, got {type(result)}\")\n\nprint(f\"Success rate: {successes}/5\")\n</code></pre>"},{"location":"for-ai-agents/self-improvement/#adapting-model-configuration","title":"Adapting Model Configuration","text":""},{"location":"for-ai-agents/self-improvement/#swap-models-at-runtime","title":"Swap Models at Runtime","text":"<pre><code>from lionagi import iModel\n\n# Upgrade to a more capable model for complex tasks\nbranch.chat_model = iModel(provider=\"openai\", model=\"gpt-4.1\")\n\n# Use a faster model for simple follow-ups\nbranch.chat_model = iModel(provider=\"openai\", model=\"gpt-4.1-mini\")\n</code></pre>"},{"location":"for-ai-agents/self-improvement/#use-different-models-for-chat-vs-parse","title":"Use Different Models for Chat vs Parse","text":"<pre><code># Expensive model for conversation, cheap model for parsing\nbranch = Branch(\n    chat_model=iModel(provider=\"anthropic\", model=\"claude-sonnet-4-20250514\"),\n    parse_model=iModel(provider=\"openai\", model=\"gpt-4.1-mini\"),\n)\n</code></pre>"},{"location":"for-ai-agents/self-improvement/#override-model-per-call","title":"Override Model Per Call","text":"<pre><code># Use a specific model for just this call\nresult = await branch.communicate(\n    \"Complex analysis requiring high capability\",\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4.1\"),\n)\n</code></pre>"},{"location":"for-ai-agents/self-improvement/#workflow-debugging","title":"Workflow Debugging","text":""},{"location":"for-ai-agents/self-improvement/#sessionflow-results","title":"Session.flow() Results","text":"<pre><code>result = await session.flow(builder.get_graph(), verbose=True)\n\n# Inspect results per operation\nfor node_id, op_result in result.get(\"operation_results\", {}).items():\n    print(f\"Node {str(node_id)[:8]}: {type(op_result).__name__}\")\n    if op_result is None:\n        print(\"  -&gt; Operation returned None (possible parse failure)\")\n\n# Check what was skipped\nfor skipped in result.get(\"skipped_operations\", []):\n    print(f\"Skipped: {str(skipped)[:8]}\")\n\n# Check completed\nfor completed in result.get(\"completed_operations\", []):\n    print(f\"Completed: {str(completed)[:8]}\")\n</code></pre>"},{"location":"for-ai-agents/self-improvement/#builder-state-inspection","title":"Builder State Inspection","text":"<pre><code>state = builder.visualize_state()\nprint(f\"Total nodes: {state['total_nodes']}\")\nprint(f\"Executed: {state['executed_nodes']}\")\nprint(f\"Remaining: {state['unexecuted_nodes']}\")\nprint(f\"Current heads: {state['current_heads']}\")\nprint(f\"Expansions: {state['expansions']}\")\n</code></pre>"},{"location":"for-ai-agents/self-improvement/#property-reference","title":"Property Reference","text":""},{"location":"for-ai-agents/self-improvement/#branch-properties","title":"Branch Properties","text":"Property Type Description <code>branch.messages</code> <code>Pile[RoledMessage]</code> All conversation messages <code>branch.logs</code> <code>Pile[Log]</code> Activity logs <code>branch.system</code> <code>System \\| None</code> System message <code>branch.chat_model</code> <code>iModel</code> Chat model (settable) <code>branch.parse_model</code> <code>iModel</code> Parse model (settable) <code>branch.tools</code> <code>dict[str, Tool]</code> Registered tools <code>branch.msgs</code> <code>MessageManager</code> Message manager (advanced) <code>branch.acts</code> <code>ActionManager</code> Action manager (advanced) <code>branch.mdls</code> <code>iModelManager</code> Model manager (advanced)"},{"location":"for-ai-agents/self-improvement/#branch-methods-for-state-management","title":"Branch Methods for State Management","text":"Method Description <code>to_dict()</code> Serialize to dict <code>from_dict(data)</code> Restore from dict (classmethod) <code>to_df()</code> Convert messages to DataFrame <code>clone(sender=None)</code> Synchronous fork <code>aclone(sender=None)</code> Async fork (with lock) <code>dump_logs(clear, persist_path)</code> Save logs to file <code>adump_logs(clear, persist_path)</code> Async save logs <code>register_tools(tools)</code> Add tools"},{"location":"for-ai-agents/self-improvement/#imodel-properties","title":"iModel Properties","text":"Property Type Description <code>model.is_cli</code> <code>bool</code> Whether this is a CLI endpoint <code>model.model_name</code> <code>str</code> Model name string <code>model.request_options</code> <code>type[BaseModel] \\| None</code> Request schema <code>model.id</code> <code>UUID</code> Unique identifier <code>model.created_at</code> <code>float</code> Creation timestamp"},{"location":"includes/abbreviations/","title":"Abbreviations","text":"<p>systems *[RAG]: Retrieval-Augmented Generation *[LLM]: Large Language Model</p> <p>Learning *[NLP]: Natural Language Processing *[DSPy]: Declarative Self-improving Python *[MCP]: Model Context Protocol *[GPU]: Graphics Processing Unit *[CPU]: Central Processing Unit *[RAM]: Random Access Memory *[SDK]: Software Development Kit *[UUID]: Universally Unique Identifier *[HTTP]: HyperText Transfer Protocol *[HTTPS]: HyperText Transfer Protocol Secure *[URL]: Uniform Resource Locator *[URI]: Uniform Resource Identifier *[SQL]: Structured Query Language *[NoSQL]: Not Only SQL *[CRUD]: Create, Read, Update, Delete *[REST]: Representational State Transfer *[GraphQL]: Graph Query Language *[JWT]: JSON Web Token *[OAuth]: Open Authorization *[SSL]: Secure Sockets Layer *[TLS]: Transport Layer Security *[CORS]: Cross-Origin Resource Sharing *[YAML]: YAML Ain't Markup Language *[XML]: eXtensible Markup Language *[CSV]: Comma-Separated Values *[PDF]: Portable Document Format *[HTML]: HyperText Markup Language</p> <p>Package Manager *[YARN]: Yet Another Resource Negotiator *[Git]: Global Information Tracker *[GitHub]: Git-based hosting service *[GitLab]: Git-based DevOps platform *[CI]: Continuous Integration *[CD]: Continuous Deployment</p> <p>Cloud Platform *[Azure]: Microsoft Azure *[Docker]: Container platform *[K8s]: Kubernetes *[VM]: Virtual Machine *[OS]: Operating System *[Linux]: Linux Operating System *[macOS]: Mac Operating System *[Windows]: Microsoft Windows</p> <p>General Public License *[MIT]: Massachusetts Institute of Technology *[Apache]: Apache Software Foundation *[BSD]: Berkeley Software Distribution *[ISC]: Internet Software Consortium *[CC]: Creative Commons *[FOSS]: Free and Open Source Software *[OSS]: Open Source Software *[SaaS]: Software as a Service</p> <p>Content Delivery Network *[DNS]: Domain Name System *[IP]: Internet Protocol</p> <p>Simple Mail Transfer Protocol *[FTP]: File Transfer Protocol *[SSH]: Secure Shell *[VPN]: Virtual Private Network *[VPS]: Virtual Private Server *[IDE]: Integrated Development Environment *[CLI]: Command Line Interface *[GUI]: Graphical User Interface *[UI]: User Interface *[UX]: User Experience *[QA]: Quality Assurance *[QC]: Quality Control *[TDD]: Test-Driven Development *[BDD]: Behavior-Driven Development *[DDD]: Domain-Driven Design *[SOLID]: Single Responsibility, Open-Closed, Liskov Substitution, Interface Segregation, Dependency Inversion *[DRY]: Don't Repeat Yourself *[KISS]: Keep It Simple, Stupid *[YAGNI]: You Aren't Gonna Need It *[MVC]: Model-View-Controller *[MVP]: Model-View-Presenter *[MVVM]: Model-View-ViewModel *[SPA]: Single Page Application *[PWA]: Progressive Web Application *[RPC]: Remote Procedure Call</p> <p>Peer-to-Peer *[B2B]: Business-to-Business *[B2C]: Business-to-Consumer *[C2C]: Consumer-to-Consumer *[B2G]: Business-to-Government *[G2B]: Government-to-Business *[G2C]: Government-to-Consumer *[IoT]: Internet of Things</p> <p>Mixed Reality *[5G]: Fifth Generation *[4G]: Fourth Generation *[3G]: Third Generation *[2G]: Second Generation *[WiFi]: Wireless Fidelity *[Bluetooth]: Short-range wireless technology *[NFC]: Near Field Communication *[RFID]: Radio Frequency Identification *[GPS]: Global Positioning System *[GNSS]: Global Navigation Satellite System *[ISP]: Internet Service Provider *[ISV]: Independent Software Vendor *[OEM]: Original Equipment Manufacturer *[ODM]: Original Design Manufacturer *[VAR]: Value-Added Reseller *[MSP]: Managed Service Provider *[SLA]: Service Level Agreement *[SLO]: Service Level Objective</p> <p>on Investment *[TCO]: Total Cost of Ownership *[CAPEX]: Capital Expenditure</p> <p>Depreciation, and Amortization *[IPO]: Initial Public Offering *[VC]: Venture Capital *[PE]: Private Equity *[M&amp;A]: Mergers and Acquisitions *[CEO]: Chief Executive Officer *[CTO]: Chief Technology Officer *[CIO]: Chief Information Officer *[CISO]: Chief Information Security Officer *[CFO]: Chief Financial Officer *[COO]: Chief Operating Officer *[CMO]: Chief Marketing Officer *[CPO]: Chief Product Officer *[CDO]: Chief Data Officer *[VP]: Vice President *[SVP]: Senior Vice President *[EVP]: Executive Vice President *[GM]: General Manager</p> <p>Management Professional *[Agile]: Agile methodology *[Scrum]: Scrum framework</p> <p>methodology *[ITIL]: Information Technology Infrastructure Library *[COBIT]: Control Objectives for Information and Related Technologies *[SOX]: Sarbanes-Oxley Act *[GDPR]: General Data Protection Regulation *[HIPAA]: Health Insurance Portability and Accountability Act *[PCI DSS]: Payment Card Industry Data Security Standard *[SOC]: Service Organization Control *[ISO]: International Organization for Standardization *[NIST]: National Institute of Standards and Technology *[IEEE]: Institute of Electrical and Electronics Engineers *[ANSI]: American National Standards Institute *[W3C]: World Wide Web Consortium *[WHATWG]: Web Hypertext Application Technology Working Group</p> <p>Multipurpose Internet Mail Extensions *[ASCII]: American Standard Code for Information Interchange *[UTF-8]: 8-bit Unicode Transformation Format *[UTF-16]: 16-bit Unicode Transformation Format *[UTF-32]: 32-bit Unicode Transformation Format *[Base64]: Base64 encoding *[MD5]: Message Digest 5 *[SHA]: Secure Hash Algorithm *[AES]: Advanced Encryption Standard *[RSA]: Rivest-Shamir-Adleman</p> <p>Algorithm *[PKI]: Public Key Infrastructure *[CA]: Certificate Authority *[CRL]: Certificate Revocation List *[OCSP]: Online Certificate Status Protocol *[SAML]: Security Assertion Markup Language *[LDAP]: Lightweight Directory Access Protocol *[AD]: Active Directory *[RBAC]: Role-Based Access Control *[ABAC]: Attribute-Based Access Control *[MFA]: Multi-Factor Authentication *[2FA]: Two-Factor Authentication *[SSO]: Single Sign-On *[CAPTCHA]: Completely Automated Public Turing test to tell Computers and Humans Apart *[DDoS]: Distributed Denial of Service *[DoS]: Denial of Service *[WAF]: Web Application Firewall *[IDS]: Intrusion Detection System *[IPS]: Intrusion Prevention System</p> <p>Center *[CSIRT]: Computer Security Incident Response Team *[CERT]: Computer Emergency Response Team *[CVE]: Common Vulnerabilities and Exposures *[CVSS]: Common Vulnerability Scoring System *[CWE]: Common Weakness Enumeration</p> <p>Network, and Security *[NIST CSF]: NIST Cybersecurity Framework *[STRIDE]: Spoofing, Tampering, Repudiation, Information Disclosure, Denial of Service, Elevation of Privilege *[DREAD]: Damage, Reproducibility, Exploitability, Affected users, Discoverability</p>"},{"location":"integrations/","title":"Integrations","text":"<p>Connect lionagi with external services, tools, and frameworks.</p>"},{"location":"integrations/#core","title":"Core","text":"<ul> <li>LLM Providers -- OpenAI, Anthropic, Gemini, Ollama, and more</li> <li>Tools -- Turn any Python function into an LLM-callable tool</li> <li>MCP Servers -- Connect to Model Context Protocol tool servers</li> </ul>"},{"location":"integrations/#data","title":"Data","text":"<ul> <li>Databases -- Serializing Branch state; pydapter for storage adapters</li> <li>Vector Stores -- Using embeddings and vector search with tools</li> </ul>"},{"location":"integrations/#external-frameworks","title":"External Frameworks","text":"<p>lionagi does not have native integrations with other AI frameworks, but you can wrap any framework's functionality as a lionagi tool:</p> <ul> <li>DSPy -- Wrap DSPy modules as tools</li> <li>LlamaIndex -- Wrap LlamaIndex query engines as tools</li> </ul>"},{"location":"integrations/#integration-pattern","title":"Integration Pattern","text":"<p>Any Python function (sync or async) can become a tool:</p> <pre><code>branch = Branch(tools=[your_function])\nresult = await branch.operate(instruction=\"...\", actions=True)\n</code></pre> <p>This works with any external library -- database clients, HTTP APIs, vector stores, or other AI frameworks. See Tools for details.</p>"},{"location":"integrations/databases/","title":"Database Integration","text":"<p>lionagi does not include built-in database adapters. For storage, use <code>Branch.to_dict()</code> and <code>Branch.from_dict()</code> to serialize and restore conversation state.</p> <p>For full storage adapter support (PostgreSQL, MongoDB, Redis, etc.), see the sister project pydapter.</p>"},{"location":"integrations/databases/#saving-and-loading-branch-state","title":"Saving and Loading Branch State","text":"<pre><code>import json\nfrom lionagi import Branch\n\n# Create and use a branch\nbranch = Branch(system=\"You are a helpful assistant.\")\nresponse = await branch.communicate(\"Explain quantum computing briefly.\")\n\n# Serialize to dict\nstate = branch.to_dict()\n\n# Save to JSON file\nwith open(\"branch_state.json\", \"w\") as f:\n    json.dump(state, f, default=str)\n\n# Load from JSON file\nwith open(\"branch_state.json\") as f:\n    data = json.load(f)\n\nrestored = Branch.from_dict(data)\n</code></pre> <p>The serialized state includes messages, logs, chat/parse model configs, system message, and metadata. Tools (callables) are not serialized.</p>"},{"location":"integrations/databases/#async-context-manager","title":"Async Context Manager","text":"<p><code>Branch</code> supports <code>async with</code> for automatic log persistence:</p> <pre><code>async with Branch(system=\"Assistant\") as branch:\n    await branch.communicate(\"Hello\")\n# Logs are automatically dumped on exit\n</code></pre>"},{"location":"integrations/databases/#pydapter","title":"pydapter","text":"<p>pydapter provides storage-agnostic adapters for persisting Pydantic models to various backends:</p> <ul> <li>PostgreSQL (async)</li> <li>MongoDB</li> <li>Neo4j</li> <li>Qdrant</li> <li>Redis</li> <li>JSON, CSV, TOML, Excel</li> </ul> <p>Combine <code>Branch.to_dict()</code> with pydapter adapters for database persistence.</p>"},{"location":"integrations/dspy-optimization/","title":"DSPy Integration","text":"<p>lionagi does not have a native DSPy integration. The two frameworks solve different problems: DSPy focuses on prompt optimization and compilation, while lionagi focuses on multi-model orchestration and tool calling.</p>"},{"location":"integrations/dspy-optimization/#using-dspy-with-lionagi","title":"Using DSPy with lionagi","text":"<p>You can wrap DSPy modules as lionagi tools or call them within your application code alongside lionagi branches:</p> <pre><code>import dspy\nfrom lionagi import Branch\n\n# Your DSPy module\nclass Summarizer(dspy.Signature):\n    text = dspy.InputField()\n    summary = dspy.OutputField()\n\nsummarizer = dspy.ChainOfThought(Summarizer)\n\n# Wrap as a lionagi tool\ndef dspy_summarize(text: str) -&gt; str:\n    \"\"\"Summarize text using an optimized DSPy module.\n\n    Args:\n        text: The text to summarize.\n    \"\"\"\n    result = summarizer(text=text)\n    return result.summary\n\nbranch = Branch(tools=[dspy_summarize])\n</code></pre> <p>Both frameworks can share the same LLM API keys and run in the same process.</p>"},{"location":"integrations/llamaindex-rag/","title":"LlamaIndex Integration","text":"<p>lionagi does not have a native LlamaIndex integration. The two frameworks serve different purposes: LlamaIndex focuses on RAG pipelines and document indexing, while lionagi focuses on multi-model orchestration and tool calling.</p>"},{"location":"integrations/llamaindex-rag/#using-llamaindex-with-lionagi","title":"Using LlamaIndex with lionagi","text":"<p>You can wrap LlamaIndex query engines as lionagi tools:</p> <pre><code>from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\nfrom lionagi import Branch\n\n# Your LlamaIndex setup\ndocuments = SimpleDirectoryReader(\"./data\").load_data()\nindex = VectorStoreIndex.from_documents(documents)\nquery_engine = index.as_query_engine()\n\n# Wrap as a lionagi tool\ndef rag_search(question: str) -&gt; str:\n    \"\"\"Search the document index for relevant information.\n\n    Args:\n        question: The question to search for.\n    \"\"\"\n    response = query_engine.query(question)\n    return str(response)\n\nbranch = Branch(tools=[rag_search])\n\nresult = await branch.ReAct(\n    instruct={\"instruction\": \"What are the key findings in the research papers?\"},\n    max_extensions=2,\n)\n</code></pre> <p>Both frameworks can share the same LLM API keys and run in the same process.</p>"},{"location":"integrations/llm-providers/","title":"LLM Provider Integration","text":"<p>LionAGI supports 12 providers through a unified <code>iModel</code> interface. All providers work with the same <code>Branch</code> API -- swap the model and your application code stays the same.</p>"},{"location":"integrations/llm-providers/#how-provider-selection-works","title":"How Provider Selection Works","text":"<p>Every LLM call in LionAGI goes through <code>iModel</code>, which wraps an <code>Endpoint</code> for a specific provider. When you create an <code>iModel</code>, the <code>match_endpoint</code> function selects the right endpoint class based on the <code>provider</code> and <code>endpoint</code> strings you pass in. API keys are resolved automatically from environment variables (or <code>.env</code> / <code>.env.local</code> / <code>.secrets.env</code> files) via pydantic-settings.</p> <pre><code>from lionagi import Branch, iModel\n\n# Explicit provider + model\nmodel = iModel(provider=\"openai\", model=\"gpt-4.1-mini\")\n\n# Slash-syntax auto-detects provider from the model string\nmodel = iModel(model=\"openai/gpt-4.1-mini\")\n\n# Branch uses the model for all LLM operations\nbranch = Branch(chat_model=model)\n</code></pre>"},{"location":"integrations/llm-providers/#quick-reference","title":"Quick Reference","text":"Provider <code>provider=</code> Env Var for API Key Endpoint(s) Default Model OpenAI <code>\"openai\"</code> <code>OPENAI_API_KEY</code> <code>\"chat\"</code>, <code>\"response\"</code> <code>gpt-4.1-mini</code> Anthropic <code>\"anthropic\"</code> <code>ANTHROPIC_API_KEY</code> <code>\"chat\"</code> / <code>\"messages\"</code> -- Google Gemini <code>\"gemini\"</code> <code>GEMINI_API_KEY</code> <code>\"chat\"</code> <code>gemini-2.5-flash</code> Ollama <code>\"ollama\"</code> (none required) <code>\"chat\"</code> -- Perplexity <code>\"perplexity\"</code> <code>PERPLEXITY_API_KEY</code> <code>\"chat\"</code> <code>sonar</code> Groq <code>\"groq\"</code> <code>GROQ_API_KEY</code> <code>\"chat\"</code> <code>llama-3.3-70b-versatile</code> OpenRouter <code>\"openrouter\"</code> <code>OPENROUTER_API_KEY</code> <code>\"chat\"</code> <code>google/gemini-2.5-flash</code> NVIDIA NIM <code>\"nvidia_nim\"</code> <code>NVIDIA_NIM_API_KEY</code> <code>\"chat\"</code>, <code>\"embed\"</code> <code>meta/llama3-8b-instruct</code> Exa <code>\"exa\"</code> <code>EXA_API_KEY</code> <code>\"search\"</code> -- Claude Code CLI <code>\"claude_code\"</code> (uses local CLI) <code>\"query_cli\"</code> <code>sonnet</code> Gemini CLI <code>\"gemini_code\"</code> (uses local CLI) <code>\"query_cli\"</code> <code>gemini-2.5-pro</code> Codex CLI <code>\"codex\"</code> (uses local CLI) <code>\"query_cli\"</code> <code>gpt-5.3-codex</code>"},{"location":"integrations/llm-providers/#installation","title":"Installation","text":"<pre><code># Core package (includes OpenAI, Anthropic, Gemini, Groq, OpenRouter,\n# Perplexity, NVIDIA NIM, Exa support)\nuv pip install lionagi\n\n# For Ollama local models\nuv pip install \"lionagi[ollama]\"\n</code></pre>"},{"location":"integrations/llm-providers/#openai","title":"OpenAI","text":""},{"location":"integrations/llm-providers/#setup","title":"Setup","text":"<pre><code>export OPENAI_API_KEY=\"sk-...\"\n</code></pre>"},{"location":"integrations/llm-providers/#basic-usage","title":"Basic Usage","text":"<pre><code>import asyncio\nfrom lionagi import Branch, iModel\n\nasync def main():\n    branch = Branch(\n        chat_model=iModel(provider=\"openai\", model=\"gpt-4.1-mini\")\n    )\n    result = await branch.chat(\n        instruction=\"Explain the difference between async and sync programming.\"\n    )\n    print(result)\n\nasyncio.run(main())\n</code></pre> <p>If <code>OPENAI_API_KEY</code> is set and you do not pass a <code>chat_model</code>, Branch defaults to <code>provider=\"openai\"</code>, <code>model=\"gpt-4.1-mini\"</code> automatically (configurable via the <code>LIONAGI_CHAT_PROVIDER</code> and <code>LIONAGI_CHAT_MODEL</code> environment variables).</p>"},{"location":"integrations/llm-providers/#endpoints","title":"Endpoints","text":"<p>OpenAI has two endpoint types:</p> <ul> <li>Chat Completions (<code>endpoint=\"chat\"</code>, the default) -- standard   <code>chat/completions</code> API.</li> <li>Responses (<code>endpoint=\"response\"</code>) -- the newer <code>responses</code> API.</li> </ul> <pre><code># Responses endpoint\nmodel = iModel(provider=\"openai\", model=\"gpt-4.1-mini\", endpoint=\"response\")\n</code></pre>"},{"location":"integrations/llm-providers/#structured-output","title":"Structured Output","text":"<pre><code>import asyncio\nfrom pydantic import BaseModel, Field\nfrom lionagi import Branch, iModel\n\nclass SentimentResult(BaseModel):\n    sentiment: str = Field(description=\"positive, negative, or neutral\")\n    confidence: float = Field(description=\"Confidence score 0-1\")\n\nasync def main():\n    branch = Branch(\n        chat_model=iModel(provider=\"openai\", model=\"gpt-4.1-mini\")\n    )\n    result = await branch.operate(\n        instruction=\"Analyze the sentiment of this review.\",\n        context=\"The product exceeded all my expectations!\",\n        operative_model=SentimentResult,\n    )\n    print(result)  # Operative with .output containing a SentimentResult\n\nasyncio.run(main())\n</code></pre>"},{"location":"integrations/llm-providers/#available-models","title":"Available Models","text":"<p>Any model available through the OpenAI API works. Common choices:</p> <ul> <li><code>gpt-4.1-mini</code> (default, cost-effective)</li> <li><code>gpt-4.1</code></li> <li><code>gpt-4o</code></li> <li><code>gpt-4o-mini</code></li> <li><code>o3-mini</code></li> </ul>"},{"location":"integrations/llm-providers/#anthropic","title":"Anthropic","text":""},{"location":"integrations/llm-providers/#setup_1","title":"Setup","text":"<pre><code>export ANTHROPIC_API_KEY=\"sk-ant-...\"\n</code></pre>"},{"location":"integrations/llm-providers/#basic-usage_1","title":"Basic Usage","text":"<pre><code>import asyncio\nfrom lionagi import Branch, iModel\n\nasync def main():\n    branch = Branch(\n        chat_model=iModel(provider=\"anthropic\", model=\"claude-sonnet-4-20250514\")\n    )\n    result = await branch.chat(\n        instruction=\"Analyze this code for potential issues.\",\n        context=\"def divide(a, b): return a / b\",\n    )\n    print(result)\n\nasyncio.run(main())\n</code></pre> <p>The endpoint string can be <code>\"chat\"</code> or <code>\"messages\"</code> -- both resolve to the Anthropic Messages API. System messages in the conversation are automatically extracted and passed as the Anthropic <code>system</code> parameter.</p>"},{"location":"integrations/llm-providers/#prompt-caching","title":"Prompt Caching","text":"<p>Anthropic supports prompt caching via <code>cache_control</code>. Pass it when invoking:</p> <pre><code>result = await branch.chat(\n    instruction=\"Summarize this long document.\",\n    context=long_document,\n    cache_control=True,\n)\n</code></pre>"},{"location":"integrations/llm-providers/#available-models_1","title":"Available Models","text":"<ul> <li><code>claude-sonnet-4-20250514</code></li> <li><code>claude-opus-4-20250514</code></li> <li><code>claude-3-5-sonnet-20241022</code></li> <li><code>claude-3-5-haiku-20241022</code></li> </ul>"},{"location":"integrations/llm-providers/#google-gemini-native-api","title":"Google Gemini (Native API)","text":""},{"location":"integrations/llm-providers/#setup_2","title":"Setup","text":"<pre><code>export GEMINI_API_KEY=\"...\"\n</code></pre>"},{"location":"integrations/llm-providers/#basic-usage_2","title":"Basic Usage","text":"<pre><code>import asyncio\nfrom lionagi import Branch, iModel\n\nasync def main():\n    branch = Branch(\n        chat_model=iModel(provider=\"gemini\", model=\"gemini-2.5-flash\")\n    )\n    result = await branch.chat(\n        instruction=\"Compare Python and Rust for systems programming.\"\n    )\n    print(result)\n\nasyncio.run(main())\n</code></pre> <p>The Gemini provider uses Google's OpenAI-compatible endpoint at <code>generativelanguage.googleapis.com/v1beta/openai</code>, so it accepts standard chat-completion parameters.</p>"},{"location":"integrations/llm-providers/#available-models_2","title":"Available Models","text":"<ul> <li><code>gemini-2.5-flash</code> (default)</li> <li><code>gemini-2.5-pro</code></li> <li><code>gemini-2.0-flash</code></li> </ul>"},{"location":"integrations/llm-providers/#ollama-local-models","title":"Ollama (Local Models)","text":""},{"location":"integrations/llm-providers/#setup_3","title":"Setup","text":"<pre><code># 1. Install Ollama (https://ollama.com)\ncurl -fsSL https://ollama.ai/install.sh | sh\n\n# 2. Install the lionagi Ollama extra\nuv pip install \"lionagi[ollama]\"\n\n# 3. Pull a model\nollama pull llama3.2:3b\n</code></pre> <p>No API key is needed. The endpoint defaults to <code>http://localhost:11434/v1</code>.</p>"},{"location":"integrations/llm-providers/#basic-usage_3","title":"Basic Usage","text":"<pre><code>import asyncio\nfrom lionagi import Branch, iModel\n\nasync def main():\n    branch = Branch(\n        chat_model=iModel(provider=\"ollama\", model=\"llama3.2:3b\")\n    )\n    result = await branch.chat(\n        instruction=\"Explain how transformers work in machine learning.\"\n    )\n    print(result)\n\nasyncio.run(main())\n</code></pre>"},{"location":"integrations/llm-providers/#auto-pull","title":"Auto-Pull","text":"<p>If the requested model is not available locally, LionAGI will pull it from the Ollama registry automatically before the first call (with a progress bar via <code>tqdm</code>).</p>"},{"location":"integrations/llm-providers/#custom-base-url","title":"Custom Base URL","text":"<p>If Ollama runs on a different host or port:</p> <pre><code>model = iModel(\n    provider=\"ollama\",\n    model=\"llama3.2:3b\",\n    base_url=\"http://my-server:11434/v1\",\n)\n</code></pre>"},{"location":"integrations/llm-providers/#perplexity","title":"Perplexity","text":"<p>Perplexity provides real-time web search and Q&amp;A through their Sonar API.</p>"},{"location":"integrations/llm-providers/#setup_4","title":"Setup","text":"<pre><code>export PERPLEXITY_API_KEY=\"pplx-...\"\n</code></pre>"},{"location":"integrations/llm-providers/#basic-usage_4","title":"Basic Usage","text":"<pre><code>import asyncio\nfrom lionagi import Branch, iModel\n\nasync def main():\n    branch = Branch(\n        chat_model=iModel(provider=\"perplexity\", model=\"sonar\")\n    )\n    result = await branch.chat(\n        instruction=\"What are the latest developments in quantum computing?\"\n    )\n    print(result)\n\nasyncio.run(main())\n</code></pre>"},{"location":"integrations/llm-providers/#available-models_3","title":"Available Models","text":"<ul> <li><code>sonar</code> (default)</li> <li><code>sonar-pro</code></li> <li><code>sonar-reasoning</code></li> <li><code>sonar-reasoning-pro</code></li> <li><code>sonar-deep-research</code></li> </ul>"},{"location":"integrations/llm-providers/#provider-specific-parameters","title":"Provider-Specific Parameters","text":"<p>Perplexity supports additional parameters that can be passed as kwargs:</p> <ul> <li><code>search_mode</code> -- <code>\"default\"</code> or <code>\"academic\"</code> (restricts to scholarly sources)</li> <li><code>search_domain_filter</code> -- list of domains to include/exclude (prefix with <code>\"-\"</code>)</li> <li><code>search_recency_filter</code> -- <code>\"month\"</code>, <code>\"week\"</code>, <code>\"day\"</code>, or <code>\"hour\"</code></li> <li><code>return_related_questions</code> -- <code>True</code>/<code>False</code></li> </ul>"},{"location":"integrations/llm-providers/#groq","title":"Groq","text":""},{"location":"integrations/llm-providers/#setup_5","title":"Setup","text":"<pre><code>export GROQ_API_KEY=\"gsk_...\"\n</code></pre>"},{"location":"integrations/llm-providers/#basic-usage_5","title":"Basic Usage","text":"<pre><code>import asyncio\nfrom lionagi import Branch, iModel\n\nasync def main():\n    branch = Branch(\n        chat_model=iModel(provider=\"groq\", model=\"llama-3.3-70b-versatile\")\n    )\n    result = await branch.chat(\n        instruction=\"Fast inference test -- explain recursion in one paragraph.\"\n    )\n    print(result)\n\nasyncio.run(main())\n</code></pre> <p>Groq uses an OpenAI-compatible API, so standard chat-completion parameters (<code>temperature</code>, <code>max_tokens</code>, <code>top_p</code>, etc.) are supported.</p>"},{"location":"integrations/llm-providers/#available-models_4","title":"Available Models","text":"<ul> <li><code>llama-3.3-70b-versatile</code> (default)</li> <li><code>llama-3.1-8b-instant</code></li> <li><code>mixtral-8x7b-32768</code></li> </ul>"},{"location":"integrations/llm-providers/#openrouter","title":"OpenRouter","text":"<p>OpenRouter provides access to many models through a single API.</p>"},{"location":"integrations/llm-providers/#setup_6","title":"Setup","text":"<pre><code>export OPENROUTER_API_KEY=\"sk-or-...\"\n</code></pre>"},{"location":"integrations/llm-providers/#basic-usage_6","title":"Basic Usage","text":"<pre><code>import asyncio\nfrom lionagi import Branch, iModel\n\nasync def main():\n    branch = Branch(\n        chat_model=iModel(\n            provider=\"openrouter\",\n            model=\"google/gemini-2.5-flash\",\n        )\n    )\n    result = await branch.chat(instruction=\"Hello from OpenRouter!\")\n    print(result)\n\nasyncio.run(main())\n</code></pre> <p>OpenRouter is OpenAI-compatible. Use any model ID from the OpenRouter model list.</p>"},{"location":"integrations/llm-providers/#nvidia-nim","title":"NVIDIA NIM","text":""},{"location":"integrations/llm-providers/#setup_7","title":"Setup","text":"<p>Get an API key from build.nvidia.com.</p> <pre><code>export NVIDIA_NIM_API_KEY=\"nvapi-...\"\n</code></pre>"},{"location":"integrations/llm-providers/#chat-endpoint","title":"Chat Endpoint","text":"<pre><code>import asyncio\nfrom lionagi import Branch, iModel\n\nasync def main():\n    branch = Branch(\n        chat_model=iModel(\n            provider=\"nvidia_nim\",\n            model=\"meta/llama3-8b-instruct\",\n        )\n    )\n    result = await branch.chat(instruction=\"Explain GPU parallelism.\")\n    print(result)\n\nasyncio.run(main())\n</code></pre>"},{"location":"integrations/llm-providers/#embedding-endpoint","title":"Embedding Endpoint","text":"<pre><code>embed_model = iModel(\n    provider=\"nvidia_nim\",\n    endpoint=\"embed\",\n    model=\"nvidia/nv-embed-v1\",\n)\n</code></pre>"},{"location":"integrations/llm-providers/#exa-search","title":"Exa (Search)","text":"<p>Exa is a search provider, not a chat provider. It uses <code>endpoint=\"search\"</code>.</p>"},{"location":"integrations/llm-providers/#setup_8","title":"Setup","text":"<pre><code>export EXA_API_KEY=\"...\"\n</code></pre>"},{"location":"integrations/llm-providers/#basic-usage_7","title":"Basic Usage","text":"<pre><code>from lionagi import iModel\n\nsearch_model = iModel(provider=\"exa\", endpoint=\"search\")\n</code></pre> <p>Exa supports search-specific parameters including <code>query</code>, <code>category</code>, <code>type</code> (<code>\"keyword\"</code>, <code>\"neural\"</code>, <code>\"auto\"</code>), <code>num_results</code>, <code>include_domains</code>, <code>exclude_domains</code>, date filters, and content retrieval options.</p>"},{"location":"integrations/llm-providers/#cli-providers-coding-agents","title":"CLI Providers (Coding Agents)","text":"<p>LionAGI integrates three CLI-based coding agents as first-class iModel providers. These wrap local CLI binaries (subprocess, not HTTP) and enable agent-to-agent orchestration -- your outer agent uses lionagi to spawn and coordinate inner coding agents.</p>"},{"location":"integrations/llm-providers/#how-cli-endpoints-work","title":"How CLI Endpoints Work","text":"<p>CLI providers inherit from <code>CLIEndpoint</code>, a subclass of <code>Endpoint</code> that replaces HTTP transport with subprocess execution:</p> <ul> <li>Subprocess execution -- each call spawns the CLI binary and streams NDJSON   from stdout, decoded incrementally (handles multibyte UTF-8 splits).</li> <li>Session persistence -- the endpoint stores a <code>session_id</code> from the first   response and automatically passes <code>--resume</code> on subsequent calls.</li> <li>Conservative concurrency -- <code>DEFAULT_CONCURRENCY_LIMIT = 3</code>,   <code>DEFAULT_QUEUE_CAPACITY = 10</code> (vs 100 for HTTP).</li> <li>No API key needed -- authentication is handled by the installed CLI tool.</li> <li>Event handlers -- optional callbacks (<code>on_text</code>, <code>on_tool_use</code>,   <code>on_tool_result</code>, <code>on_final</code>) fire as the subprocess streams output.</li> </ul> <pre><code>from lionagi import iModel\n\nmodel = iModel(provider=\"claude_code\", model=\"sonnet\")\n\n# Check if a model uses a CLI endpoint\nmodel.is_cli  # True\n</code></pre>"},{"location":"integrations/llm-providers/#prerequisites","title":"Prerequisites","text":"<p>Each CLI provider requires its binary installed and on <code>PATH</code>:</p> Provider Binary Install Command Claude Code <code>claude</code> <code>npm i -g @anthropic-ai/claude-code</code> Gemini CLI <code>gemini</code> <code>npm i -g @anthropic-ai/gemini-cli</code> Codex <code>codex</code> <code>npm i -g codex</code> <p>LionAGI detects availability at import time via <code>shutil.which()</code>.</p>"},{"location":"integrations/llm-providers/#claude-code-cli","title":"Claude Code CLI","text":"<pre><code>import asyncio\nfrom lionagi import Branch, iModel\n\nasync def main():\n    model = iModel(\n        provider=\"claude_code\",\n        model=\"sonnet\",                          # \"sonnet\" or \"opus\"\n        system_prompt=\"You are a code reviewer.\", # system instructions\n        permission_mode=\"bypassPermissions\",      # skip approval prompts\n        allowed_tools=[\"Read\", \"Grep\", \"Glob\", \"Bash\"],\n        max_turns=10,                            # conversation turn limit\n    )\n    branch = Branch(chat_model=model, name=\"reviewer\")\n    result = await branch.communicate(\"Review src/auth.py for security issues\")\n    print(result)\n\nasyncio.run(main())\n</code></pre> <p>Claude Code request parameters:</p> Parameter Type Default Description <code>model</code> <code>str</code> <code>\"sonnet\"</code> <code>\"sonnet\"</code>, <code>\"opus\"</code>, or model ID <code>system_prompt</code> <code>str</code> None System instructions for the agent <code>append_system_prompt</code> <code>str</code> None Appended to existing system prompt <code>permission_mode</code> <code>str</code> None <code>\"default\"</code>, <code>\"acceptEdits\"</code>, <code>\"bypassPermissions\"</code> <code>allowed_tools</code> <code>list[str]</code> None Restrict to these tools only <code>disallowed_tools</code> <code>list[str]</code> <code>[]</code> Block specific tools <code>max_turns</code> <code>int</code> None Max conversation turns <code>max_thinking_tokens</code> <code>int</code> None Thinking budget <code>continue_conversation</code> <code>bool</code> <code>False</code> Continue previous session <code>resume</code> <code>str</code> None Resume a specific session ID <code>ws</code> <code>str</code> None Subdirectory within repo <code>add_dir</code> <code>str</code> None Extra read-only directory mount <code>mcp_tools</code> <code>list[str]</code> <code>[]</code> MCP tool names to enable <code>mcp_servers</code> <code>dict</code> <code>{}</code> MCP server configurations <code>mcp_config</code> <code>str\\|Path</code> None Path to MCP config file <code>auto_finish</code> <code>bool</code> <code>False</code> Auto-append result extraction if agent doesn't finish <code>verbose_output</code> <code>bool</code> <code>False</code> Show full agent output <code>cli_include_summary</code> <code>bool</code> <code>False</code> Include cost/usage summary"},{"location":"integrations/llm-providers/#gemini-cli","title":"Gemini CLI","text":"<pre><code>import asyncio\nfrom lionagi import Branch, iModel\n\nasync def main():\n    model = iModel(\n        provider=\"gemini_code\",\n        model=\"gemini-2.5-pro\",\n        sandbox=True,                      # safety sandboxing (default)\n        approval_mode=\"auto_edit\",         # \"suggest\", \"auto_edit\", \"full_auto\"\n    )\n    branch = Branch(chat_model=model, name=\"gemini_agent\")\n    result = await branch.communicate(\"Analyze the project structure\")\n    print(result)\n\nasyncio.run(main())\n</code></pre> <p>Gemini CLI request parameters:</p> Parameter Type Default Description <code>model</code> <code>str</code> <code>\"gemini-2.5-pro\"</code> Gemini model name <code>system_prompt</code> <code>str</code> None System instructions <code>sandbox</code> <code>bool</code> <code>True</code> Enable sandbox protection <code>yolo</code> <code>bool</code> <code>False</code> Auto-approve all actions (emits warning) <code>approval_mode</code> <code>str</code> None <code>\"suggest\"</code>, <code>\"auto_edit\"</code>, <code>\"full_auto\"</code> <code>debug</code> <code>bool</code> <code>False</code> Debug mode <code>include_directories</code> <code>list[str]</code> <code>[]</code> Extra directories to include <code>ws</code> <code>str</code> None Subdirectory within repo <code>verbose_output</code> <code>bool</code> <code>False</code> Show full agent output <code>cli_include_summary</code> <code>bool</code> <code>False</code> Include cost/usage summary"},{"location":"integrations/llm-providers/#codex-cli","title":"Codex CLI","text":"<pre><code>import asyncio\nfrom lionagi import Branch, iModel\n\nasync def main():\n    model = iModel(\n        provider=\"codex\",\n        model=\"gpt-5.3-codex\",\n        sandbox=\"workspace-write\",         # \"read-only\", \"workspace-write\", \"danger-full-access\"\n        full_auto=True,                    # auto-approve with workspace-write sandbox\n    )\n    branch = Branch(chat_model=model, name=\"codex_agent\")\n    result = await branch.communicate(\"Fix the failing tests in this project\")\n    print(result)\n\nasyncio.run(main())\n</code></pre> <p>Codex CLI request parameters:</p> Parameter Type Default Description <code>model</code> <code>str</code> <code>\"gpt-5.3-codex\"</code> Codex model name <code>system_prompt</code> <code>str</code> None System instructions <code>full_auto</code> <code>bool</code> <code>False</code> Auto-approve with workspace-write sandbox <code>sandbox</code> <code>str</code> None <code>\"read-only\"</code>, <code>\"workspace-write\"</code>, <code>\"danger-full-access\"</code> <code>bypass_approvals</code> <code>bool</code> <code>False</code> Skip all approvals and sandbox <code>skip_git_repo_check</code> <code>bool</code> <code>False</code> Don't require git repo <code>output_schema</code> <code>str\\|Path</code> None JSON Schema file for structured output <code>include_plan_tool</code> <code>bool</code> <code>False</code> Enable planning tool <code>images</code> <code>list[str]</code> <code>[]</code> Image attachments <code>config_overrides</code> <code>dict</code> <code>{}</code> Custom config key-value overrides <code>ws</code> <code>str</code> None Subdirectory within repo <code>verbose_output</code> <code>bool</code> <code>False</code> Show full agent output <code>cli_include_summary</code> <code>bool</code> <code>False</code> Include cost/usage summary"},{"location":"integrations/llm-providers/#context-and-session-management","title":"Context and Session Management","text":"<p>CLI providers have two layers of context:</p> <ul> <li>Within a session: the CLI agent manages its own context via   <code>--resume</code>. lionagi stores the <code>session_id</code> automatically and passes   it on subsequent calls.</li> <li>Across sessions: when a session grows too long, start a fresh CLI   instance and use <code>progression=</code> to select which prior messages to   inject as context. Branch's MessageManager is the durable record.</li> </ul> <pre><code>model = iModel(provider=\"claude_code\", model=\"sonnet\")\n\n# Normal: session resume is automatic\nawait branch.communicate(\"First task\")\nawait branch.communicate(\"Follow-up\")  # resumes same session\n\n# Session too long -- rotate to fresh instance, carry context\nbranch.chat_model = iModel(provider=\"claude_code\", model=\"sonnet\")\nrecent = list(branch.msgs.progression)[-30:]\nawait branch.communicate(\"Continue from here.\", progression=recent)\n</code></pre> <p>Session management:</p> <ol> <li>First call creates a new session. The CLI returns a <code>session_id</code>.</li> <li>lionagi stores the <code>session_id</code> on the endpoint.</li> <li>Subsequent calls pass <code>--resume</code> automatically.</li> <li>If the resumed session gets a new ID, lionagi updates automatically.</li> <li><code>iModel.copy()</code> creates a fresh session; <code>iModel.copy(share_session=True)</code>    carries over the session ID.</li> </ol> <pre><code># Fresh copy, independent session\nnew_model = model.copy()\n\n# Copy that resumes the same CLI session\nresumed = model.copy(share_session=True)\n</code></pre>"},{"location":"integrations/llm-providers/#event-handlers","title":"Event Handlers","text":"<p>Set callbacks to observe the agent's streaming output:</p> <pre><code>model = iModel(provider=\"claude_code\", model=\"sonnet\")\n\nmodel.endpoint.update_handlers(\n    on_text=lambda chunk: print(f\"Text: {chunk.text}\"),\n    on_tool_use=lambda chunk: print(f\"Tool: {chunk}\"),\n    on_thinking=lambda chunk: print(f\"Thinking: {chunk.text}\"),\n    on_final=lambda session: print(f\"Done: {session.result}\"),\n)\n</code></pre> Provider Available Handlers <code>claude_code</code> <code>on_thinking</code>, <code>on_text</code>, <code>on_tool_use</code>, <code>on_tool_result</code>, <code>on_system</code>, <code>on_final</code> <code>gemini_code</code> <code>on_text</code>, <code>on_tool_use</code>, <code>on_tool_result</code>, <code>on_final</code> <code>codex</code> <code>on_text</code>, <code>on_tool_use</code>, <code>on_tool_result</code>, <code>on_final</code> <p>For more on multi-agent orchestration patterns with CLI providers, see CLI Agent Providers.</p>"},{"location":"integrations/llm-providers/#provider-auto-detection","title":"Provider Auto-Detection","text":"<p>If the <code>model</code> string contains a <code>/</code>, the prefix is treated as the provider name. This lets you skip the <code>provider=</code> parameter:</p> <pre><code>from lionagi import iModel\n\n# These two are equivalent:\nm1 = iModel(provider=\"openai\", model=\"gpt-4.1-mini\")\nm2 = iModel(model=\"openai/gpt-4.1-mini\")\n\n# Works with any provider\nm3 = iModel(model=\"anthropic/claude-sonnet-4-20250514\")\nm4 = iModel(model=\"groq/llama-3.3-70b-versatile\")\nm5 = iModel(model=\"gemini/gemini-2.5-flash\")\n</code></pre>"},{"location":"integrations/llm-providers/#openai-compatible-custom-providers","title":"OpenAI-Compatible Custom Providers","text":"<p>Any provider that implements the OpenAI chat completions API can be used with LionAGI. When <code>match_endpoint</code> does not recognize the provider name, it falls back to a generic OpenAI-compatible endpoint. Pass <code>base_url</code> to point at the custom server:</p> <pre><code>import asyncio\nfrom lionagi import Branch, iModel\n\nasync def main():\n    model = iModel(\n        provider=\"together\",\n        model=\"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\",\n        base_url=\"https://api.together.xyz/v1\",\n        api_key=\"your-together-key\",\n    )\n    branch = Branch(chat_model=model)\n    result = await branch.chat(instruction=\"Hello from a custom provider!\")\n    print(result)\n\nasyncio.run(main())\n</code></pre> <p>The fallback endpoint uses <code>chat/completions</code>, bearer auth, and <code>application/json</code> content type.</p>"},{"location":"integrations/llm-providers/#async-context-manager","title":"Async Context Manager","text":"<p>Both <code>iModel</code> and <code>Branch</code> support <code>async with</code> for resource cleanup:</p> <pre><code>import asyncio\nfrom lionagi import Branch, iModel\n\nasync def main():\n    # iModel context manager -- stops the rate-limited executor on exit\n    async with iModel(provider=\"openai\", model=\"gpt-4.1-mini\") as model:\n        branch = Branch(chat_model=model)\n        result = await branch.chat(instruction=\"Hello!\")\n        print(result)\n\n    # Branch context manager -- flushes logs on exit\n    async with Branch(\n        chat_model=iModel(provider=\"openai\", model=\"gpt-4.1-mini\")\n    ) as branch:\n        result = await branch.chat(instruction=\"Hello again!\")\n        print(result)\n\nasyncio.run(main())\n</code></pre>"},{"location":"integrations/llm-providers/#copying-models","title":"Copying Models","text":"<p>Use <code>iModel.copy()</code> to create an independent instance with the same configuration but a fresh ID and executor. This is useful when you need separate rate-limiting or session state:</p> <pre><code>from lionagi import iModel\n\nmodel = iModel(provider=\"openai\", model=\"gpt-4.1-mini\")\n\n# Fresh copy, no shared state\nmodel2 = model.copy()\n\n# For CLI providers: share_session=True carries over the session ID\ncli_model = iModel(provider=\"claude_code\", model=\"sonnet\")\ncli_model2 = cli_model.copy(share_session=True)\n</code></pre>"},{"location":"integrations/llm-providers/#rate-limiting-and-concurrency","title":"Rate Limiting and Concurrency","text":"<p><code>iModel</code> wraps a <code>RateLimitedAPIExecutor</code> that handles queuing and throttling. You can configure it at construction time:</p> <pre><code>from lionagi import iModel\n\nmodel = iModel(\n    provider=\"openai\",\n    model=\"gpt-4.1-mini\",\n    queue_capacity=100,            # Max queued requests (default: 100, CLI: 10)\n    capacity_refresh_time=60,      # Seconds between capacity refreshes\n    limit_requests=50,             # Max requests per cycle\n    limit_tokens=100_000,          # Max tokens per cycle\n    concurrency_limit=10,          # Max concurrent streaming requests\n)\n</code></pre> <p>CLI providers use lower defaults (<code>queue_capacity=10</code>, <code>concurrency_limit=3</code>) because each call spawns a subprocess.</p>"},{"location":"integrations/llm-providers/#multiple-models-in-one-branch","title":"Multiple Models in One Branch","text":"<p>A <code>Branch</code> maintains a <code>chat_model</code> and a <code>parse_model</code>. By default <code>parse_model</code> mirrors <code>chat_model</code>, but you can set them independently:</p> <pre><code>from lionagi import Branch, iModel\n\nbranch = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4.1-mini\"),\n    parse_model=iModel(provider=\"anthropic\", model=\"claude-sonnet-4-20250514\"),\n)\n</code></pre> <p>You can also override the model per-call:</p> <pre><code>fast_model = iModel(provider=\"groq\", model=\"llama-3.3-70b-versatile\")\n\n# Use the fast model just for this one call\nresult = await branch.chat(\n    instruction=\"Quick question.\",\n    imodel=fast_model,\n)\n</code></pre>"},{"location":"integrations/llm-providers/#environment-variable-reference","title":"Environment Variable Reference","text":"Variable Provider <code>OPENAI_API_KEY</code> OpenAI <code>ANTHROPIC_API_KEY</code> Anthropic <code>GEMINI_API_KEY</code> Google Gemini <code>GROQ_API_KEY</code> Groq <code>OPENROUTER_API_KEY</code> OpenRouter <code>PERPLEXITY_API_KEY</code> Perplexity <code>NVIDIA_NIM_API_KEY</code> NVIDIA NIM <code>EXA_API_KEY</code> Exa <code>LIONAGI_CHAT_PROVIDER</code> Default provider (default: <code>openai</code>) <code>LIONAGI_CHAT_MODEL</code> Default model (default: <code>gpt-4.1-mini</code>) <p>These can be set as environment variables or placed in <code>.env</code>, <code>.env.local</code>, or <code>.secrets.env</code> files in your project root.</p>"},{"location":"integrations/llm-providers/#troubleshooting","title":"Troubleshooting","text":"<p>\"Provider must be provided\" -- You passed a <code>model</code> string without a <code>/</code> separator and did not set <code>provider=</code>. Either use slash syntax (<code>model=\"openai/gpt-4.1-mini\"</code>) or pass <code>provider=</code> explicitly.</p> <p>\"API key is required for authentication\" -- The environment variable for your provider is not set. Check the table above for the correct variable name.</p> <p>\"ollama is not installed\" -- Install the Ollama extra: <code>uv pip install \"lionagi[ollama]\"</code>.</p> <p>Slow CLI providers -- CLI providers spawn subprocesses. If calls seem slow, that is expected. Avoid high concurrency with CLI providers.</p> <p>Rate limit errors (429) -- LionAGI retries with exponential backoff automatically. If you still hit limits, reduce <code>limit_requests</code> or <code>limit_tokens</code> in the <code>iModel</code> constructor.</p>"},{"location":"integrations/mcp-servers/","title":"MCP Server Integration","text":"<p>lionagi supports the Model Context Protocol (MCP) for connecting to external tool servers. MCP tools are registered and invoked the same way as native tools -- the LLM sees them as regular function calls.</p>"},{"location":"integrations/mcp-servers/#how-it-works","title":"How It Works","text":"<p>The <code>Tool</code> class accepts an <code>mcp_config</code> parameter instead of <code>func_callable</code>. When the LLM calls the tool, lionagi connects to the MCP server via <code>fastmcp</code> and executes the tool remotely. Connections are pooled by <code>MCPConnectionPool</code> for reuse.</p>"},{"location":"integrations/mcp-servers/#configuration","title":"Configuration","text":""},{"location":"integrations/mcp-servers/#mcpjson-file","title":".mcp.json File","text":"<p>Define MCP servers in a <code>.mcp.json</code> file:</p> <pre><code>{\n  \"mcpServers\": {\n    \"search\": {\n      \"command\": \"python\",\n      \"args\": [\"-m\", \"my_search_server\"],\n      \"env\": {\n        \"API_KEY\": \"your-key\"\n      }\n    },\n    \"memory\": {\n      \"url\": \"http://localhost:8000/mcp\"\n    }\n  }\n}\n</code></pre> <p>Servers can be configured with either a <code>command</code> (stdio transport) or a <code>url</code> (HTTP transport).</p>"},{"location":"integrations/mcp-servers/#auto-discovery","title":"Auto-Discovery","text":"<p>Register all tools from an MCP server automatically:</p> <pre><code>from lionagi import Branch\nfrom lionagi.protocols.action.manager import load_mcp_tools\n\n# Load tools from .mcp.json\ntools = await load_mcp_tools(\n    config_path=\".mcp.json\",\n    server_names=[\"search\"],\n)\n\nbranch = Branch(tools=tools)\n</code></pre>"},{"location":"integrations/mcp-servers/#manual-registration-via-actionmanager","title":"Manual Registration via ActionManager","text":"<p>For more control, use the <code>ActionManager</code> directly:</p> <pre><code>branch = Branch()\n\n# Auto-discover and register all tools from a server\nregistered = await branch.acts.register_mcp_server(\n    server_config={\"server\": \"search\"},\n)\nprint(registered)  # ['exa_search', 'web_fetch', ...]\n\n# Or register specific tools only\nregistered = await branch.acts.register_mcp_server(\n    server_config={\"command\": \"python\", \"args\": [\"-m\", \"my_server\"]},\n    tool_names=[\"search\", \"fetch\"],\n)\n</code></pre>"},{"location":"integrations/mcp-servers/#inline-mcp-config","title":"Inline MCP Config","text":"<p>Register a single MCP tool inline via dict config:</p> <pre><code>branch = Branch(\n    tools=[\n        {\"my_tool\": {\"command\": \"python\", \"args\": [\"-m\", \"server\"]}}\n    ]\n)\n</code></pre> <p>The dict must have exactly one key (the tool name) mapping to the server config.</p>"},{"location":"integrations/mcp-servers/#pydantic-validation-for-mcp-tools","title":"Pydantic Validation for MCP Tools","text":"<p>Add request validation to MCP tools with <code>request_options</code>:</p> <pre><code>from pydantic import BaseModel, Field\n\nclass SearchRequest(BaseModel):\n    query: str = Field(..., min_length=1)\n    num_results: int = Field(default=5, ge=1, le=20)\n\ntools = await load_mcp_tools(\n    config_path=\".mcp.json\",\n    server_names=[\"search\"],\n    request_options_map={\n        \"search\": {\"exa_search\": SearchRequest}\n    },\n)\n</code></pre>"},{"location":"integrations/mcp-servers/#connection-pool","title":"Connection Pool","text":"<p><code>MCPConnectionPool</code> manages MCP client connections:</p> <ul> <li>Connections are cached by server config and reused across calls</li> <li>Stale connections are automatically detected and replaced</li> <li>Use <code>await MCPConnectionPool.cleanup()</code> to close all connections</li> <li>The pool supports <code>async with</code> context management</li> </ul>"},{"location":"integrations/mcp-servers/#requirements","title":"Requirements","text":"<p>MCP support requires the <code>fastmcp</code> package:</p> <pre><code>pip install fastmcp\n</code></pre>"},{"location":"integrations/tools/","title":"Tool Integration","text":"<p>lionagi converts Python functions into LLM-callable tools automatically. The <code>Tool</code> class wraps any callable with schema generation, preprocessing, postprocessing, and argument validation.</p>"},{"location":"integrations/tools/#how-it-works","title":"How It Works","text":"<p>When you pass a function to <code>Branch(tools=[...])</code>, lionagi:</p> <ol> <li>Inspects the function signature and docstring</li> <li>Generates an OpenAI-compatible JSON schema via <code>function_to_schema()</code></li> <li>Wraps it in a <code>Tool</code> object registered with the <code>ActionManager</code></li> <li>Sends the schema to the LLM during <code>operate()</code> or <code>ReAct()</code> calls</li> <li>Automatically invokes the function when the LLM requests it</li> </ol>"},{"location":"integrations/tools/#basic-usage","title":"Basic Usage","text":"<pre><code>from lionagi import Branch\n\ndef calculate_sum(a: float, b: float) -&gt; float:\n    \"\"\"Add two numbers together.\n\n    Args:\n        a: First number.\n        b: Second number.\n    \"\"\"\n    return a + b\n\nbranch = Branch(\n    tools=[calculate_sum],\n    system=\"You have access to a calculator tool.\"\n)\n\n# operate() with actions=True enables tool invocation\nresult = await branch.operate(\n    instruction=\"What is 15 + 27?\",\n    actions=True,\n)\n</code></pre>"},{"location":"integrations/tools/#the-tool-class","title":"The Tool Class","text":"<p><code>Tool</code> extends <code>Element</code> and wraps a callable with metadata:</p> <pre><code>from lionagi.protocols.action.tool import Tool\n\ntool = Tool(\n    func_callable=calculate_sum,\n    # Optional: pre/post processing\n    preprocessor=lambda args: {k: float(v) for k, v in args.items()},\n    postprocessor=lambda result: f\"The answer is {result}\",\n    # Optional: strict argument validation\n    strict_func_call=False,\n)\n\n# Auto-generated schema\nprint(tool.tool_schema)\n# {'type': 'function', 'function': {'name': 'calculate_sum', ...}}\n\nprint(tool.function)       # 'calculate_sum'\nprint(tool.required_fields) # {'a', 'b'}\n</code></pre>"},{"location":"integrations/tools/#key-tool-parameters","title":"Key Tool Parameters","text":"Parameter Type Description <code>func_callable</code> <code>Callable</code> The function to wrap (required) <code>tool_schema</code> <code>dict</code> Custom schema (auto-generated if omitted) <code>request_options</code> <code>type[BaseModel]</code> Pydantic model for input validation <code>preprocessor</code> <code>Callable</code> Transform arguments before execution <code>postprocessor</code> <code>Callable</code> Transform results after execution <code>strict_func_call</code> <code>bool</code> Enforce exact parameter matching <code>mcp_config</code> <code>dict</code> MCP server tool config (see MCP Servers)"},{"location":"integrations/tools/#async-tools","title":"Async Tools","text":"<p>Both sync and async functions are supported. Async functions are awaited automatically:</p> <pre><code>import httpx\n\nasync def fetch_url(url: str) -&gt; str:\n    \"\"\"Fetch content from a URL.\n\n    Args:\n        url: The URL to fetch.\n    \"\"\"\n    async with httpx.AsyncClient() as client:\n        response = await client.get(url)\n        return response.text[:500]\n\nbranch = Branch(tools=[fetch_url])\n</code></pre>"},{"location":"integrations/tools/#registering-tools","title":"Registering Tools","text":"<p>Tools can be registered at construction or later:</p> <pre><code># At construction\nbranch = Branch(tools=[func_a, func_b])\n\n# After construction\nbranch.register_tools([func_c, func_d])\n\n# With update=True to replace existing tools\nbranch.register_tools([func_a_v2], update=True)\n\n# Check registered tools\nprint(branch.tools)  # dict of {name: Tool}\n</code></pre> <p>You can pass raw functions, <code>Tool</code> objects, <code>LionTool</code> subclasses, or MCP config dicts.</p>"},{"location":"integrations/tools/#pydantic-request-validation","title":"Pydantic Request Validation","text":"<p>Use <code>request_options</code> to validate tool arguments with a Pydantic model:</p> <pre><code>from pydantic import BaseModel, Field\n\nclass SearchRequest(BaseModel):\n    query: str = Field(..., min_length=1)\n    max_results: int = Field(default=5, ge=1, le=20)\n\ndef search(query: str, max_results: int = 5) -&gt; str:\n    \"\"\"Search for information.\"\"\"\n    return f\"Results for '{query}' (max {max_results})\"\n\ntool = Tool(\n    func_callable=search,\n    request_options=SearchRequest,\n)\n</code></pre>"},{"location":"integrations/tools/#function_to_schema","title":"function_to_schema()","text":"<p>The schema generator extracts function metadata from type hints and docstrings:</p> <pre><code>from lionagi.libs.schema.function_to_schema import function_to_schema\n\ndef example(name: str, count: int) -&gt; bool:\n    \"\"\"Do something with name and count.\n\n    Args:\n        name: The name to process.\n        count: How many times to process.\n    \"\"\"\n    return True\n\nschema = function_to_schema(example)\n# {\n#   'type': 'function',\n#   'function': {\n#     'name': 'example',\n#     'description': 'Do something with name and count.',\n#     'parameters': {\n#       'type': 'object',\n#       'properties': {\n#         'name': {'type': 'string', 'description': 'The name to process.'},\n#         'count': {'type': 'number', 'description': 'How many times to process.'}\n#       },\n#       'required': ['name', 'count']\n#     }\n#   }\n# }\n</code></pre> <p>Supported type mappings: <code>str</code> -&gt; <code>string</code>, <code>int</code>/<code>float</code> -&gt; <code>number</code>, <code>list</code>/<code>tuple</code> -&gt; <code>array</code>, <code>bool</code> -&gt; <code>boolean</code>, <code>dict</code> -&gt; <code>object</code>.</p>"},{"location":"integrations/tools/#using-tools-with-operate-and-react","title":"Using Tools with operate() and ReAct()","text":""},{"location":"integrations/tools/#operate","title":"operate()","text":"<pre><code>result = await branch.operate(\n    instruction=\"Search for Python tutorials\",\n    actions=True,          # Enable tool calling\n    invoke_actions=True,   # Auto-invoke tools (default)\n    action_strategy=\"concurrent\",  # or \"sequential\"\n)\n</code></pre>"},{"location":"integrations/tools/#react","title":"ReAct()","text":"<p>ReAct runs a think-act-observe loop, automatically using registered tools:</p> <pre><code>result = await branch.ReAct(\n    instruct={\n        \"instruction\": \"Research Python best practices and summarize findings\",\n        \"context\": {\"focus\": \"async programming\"},\n    },\n    tools=True,            # Use all registered tools (default)\n    max_extensions=3,      # Max reasoning iterations\n    verbose=True,\n)\n</code></pre>"},{"location":"integrations/tools/#built-in-tools","title":"Built-in Tools","text":""},{"location":"integrations/tools/#readertool","title":"ReaderTool","text":"<p>Reads files and URLs using the <code>docling</code> document converter. Requires <code>pip install lionagi[reader]</code>.</p> <pre><code>from lionagi.tools import ReaderTool\n\nbranch = Branch(\n    tools=[ReaderTool],\n    system=\"You can read documents and URLs.\"\n)\n\nresult = await branch.ReAct(\n    instruct={\"instruction\": \"Read and summarize this document: report.pdf\"},\n    max_extensions=4,\n)\n</code></pre> <p>ReaderTool supports three actions:</p> <ul> <li>open: Convert a file or URL to text, returns a <code>doc_id</code> and length</li> <li>read: Read a slice of a previously opened document by <code>doc_id</code> and offset</li> <li>list_dir: List files in a directory</li> </ul>"},{"location":"integrations/tools/#custom-liontool","title":"Custom LionTool","text":"<p>For reusable tools with internal state, subclass <code>LionTool</code>:</p> <pre><code>from lionagi.tools.base import LionTool\nfrom lionagi.protocols.action.tool import Tool\n\nclass MyStatefulTool(LionTool):\n    is_lion_system_tool = True\n    system_tool_name = \"my_tool\"\n\n    def __init__(self):\n        super().__init__()\n        self.state = {}\n\n    def to_tool(self) -&gt; Tool:\n        def my_tool(action: str, key: str, value: str = None) -&gt; str:\n            \"\"\"Stateful key-value store.\"\"\"\n            if action == \"set\":\n                self.state[key] = value\n                return f\"Set {key}\"\n            return self.state.get(key, \"Not found\")\n\n        return Tool(func_callable=my_tool)\n\n# Register like any other tool\nbranch = Branch(tools=[MyStatefulTool])\n</code></pre>"},{"location":"integrations/tools/#best-practices","title":"Best Practices","text":"<ul> <li>Type hints: Always add type hints -- they drive schema generation</li> <li>Docstrings: Use Google-style docstrings with <code>Args:</code> sections for parameter descriptions</li> <li>Return strings: LLMs work best when tools return string results</li> <li>Error handling: Wrap tool logic in try/except and return error messages as strings</li> <li>Async preferred: Use async functions for I/O-bound tools to avoid blocking</li> </ul>"},{"location":"integrations/vector-stores/","title":"Vector Stores","text":"<p>lionagi does not include built-in vector store integration. However, its data primitives support embedding vectors, and you can connect any vector store through tools.</p>"},{"location":"integrations/vector-stores/#node-embeddings","title":"Node Embeddings","text":"<p>The <code>Node</code> class (used throughout lionagi's graph system) has an optional <code>embedding</code> field:</p> <pre><code>from lionagi.protocols.graph.node import Node\n\nnode = Node(\n    content=\"Some text to embed\",\n    embedding=[0.1, 0.2, 0.3, ...],  # Your embedding vector\n)\n</code></pre> <p>This lets you store embeddings alongside content in lionagi's graph structures, but querying and indexing are left to your vector store of choice.</p>"},{"location":"integrations/vector-stores/#connecting-a-vector-store-as-a-tool","title":"Connecting a Vector Store as a Tool","text":"<p>The recommended approach is to wrap your vector store operations as tool functions:</p> <pre><code>from lionagi import Branch\n\nasync def vector_search(query: str, top_k: int = 5) -&gt; str:\n    \"\"\"Search the vector database for relevant documents.\n\n    Args:\n        query: The search query.\n        top_k: Number of results to return.\n    \"\"\"\n    # Your vector store client (Qdrant, Pinecone, Chroma, etc.)\n    results = await your_vector_client.search(query, limit=top_k)\n    return \"\\n\".join(r.text for r in results)\n\nbranch = Branch(\n    tools=[vector_search],\n    system=\"Search the knowledge base before answering questions.\"\n)\n\nresult = await branch.ReAct(\n    instruct={\"instruction\": \"What are the latest findings on climate change?\"},\n    max_extensions=2,\n)\n</code></pre> <p>This approach works with any vector store and keeps lionagi decoupled from specific database choices.</p>"},{"location":"integrations/vector-stores/#related","title":"Related","text":"<ul> <li>pydapter includes a Qdrant adapter for vector storage</li> <li>Tools covers how to create and register custom tools</li> </ul>"},{"location":"patterns/","title":"Patterns","text":"<p>You're in Step 4 of the Learning Path</p> <p>You've mastered the core concepts. Now let's apply them with proven workflow patterns that solve real-world problems.</p> <p>These patterns represent battle-tested approaches to common multi-agent scenarios. Each pattern includes complete working code, use cases, and performance characteristics.</p>"},{"location":"patterns/#available-patterns","title":"Available Patterns","text":""},{"location":"patterns/#fan-outin","title":"Fan-Out/In","text":"<p>Distribute work to parallel agents, then synthesize results. Use for, Research, analysis, brainstorming</p>"},{"location":"patterns/#sequential-analysis","title":"Sequential Analysis","text":"<p>Build understanding step-by-step through dependent operations. Use for, Document processing, complex reasoning</p>"},{"location":"patterns/#conditional-flows","title":"Conditional Flows","text":"<p>Execute different paths based on runtime conditions. Use for, Dynamic workflows, decision trees</p>"},{"location":"patterns/#react-with-rag","title":"ReAct with RAG","text":"<p>Combine reasoning with retrieval-augmented generation. Use for, Knowledge-intensive tasks</p>"},{"location":"patterns/#tournament-validation","title":"Tournament Validation","text":"<p>Multiple approaches compete, best solution wins. Use for, Quality-critical outputs</p>"},{"location":"patterns/#pattern-selection-guide","title":"Pattern Selection Guide","text":"<pre><code>def select_pattern(task):\n    if task.needs_multiple_perspectives:\n        return \"fan-out-in\"\n    elif task.requires_step_by_step_building:\n        return \"sequential-analysis\"\n    elif task.has_conditional_logic:\n        return \"conditional-flows\"\n    elif task.needs_external_knowledge:\n        return \"react-with-rag\"\n    elif task.requires_best_quality:\n        return \"tournament-validation\"\n    else:\n        return \"simple-branch\"\n</code></pre>"},{"location":"patterns/#common-combinations","title":"Common Combinations","text":""},{"location":"patterns/#research-pipeline","title":"Research Pipeline","text":"<pre><code>Fan-Out (gather) \u2192 Sequential (analyze) \u2192 Fan-Out (verify)\n</code></pre>"},{"location":"patterns/#document-processing","title":"Document Processing","text":"<pre><code>Sequential (extract) \u2192 Conditional (classify) \u2192 Fan-Out (process by type)\n</code></pre>"},{"location":"patterns/#decision-making","title":"Decision Making","text":"<pre><code>Fan-Out (options) \u2192 Tournament (evaluate) \u2192 Sequential (implement)\n</code></pre> <p>Ready for Production Examples?</p> <p>You've learned the patterns - now see them implemented in complete, production-ready workflows:</p> <p>Next: Cookbook - Complete working examples you can copy and modify Or: Advanced Topics - Deep dive into custom operations and optimization</p>"},{"location":"patterns/conditional-flows/","title":"Conditional Flows Pattern","text":"<p>Dynamic workflow paths based on runtime conditions and decision points.</p>"},{"location":"patterns/conditional-flows/#when-to-use-this-pattern","title":"When to Use This Pattern","text":"<p>Use conditional flows when:</p> <ul> <li>Different inputs need different processing paths</li> <li>Decision points determine next steps</li> <li>Workflow adapts based on intermediate results</li> <li>Error conditions need special handling</li> <li>Quality gates determine continuation</li> </ul>"},{"location":"patterns/conditional-flows/#basic-conditional-flow","title":"Basic Conditional Flow","text":"<pre><code>from lionagi import Branch, Session, Builder, iModel\n\nsession = Session()\nbuilder = Builder(\"conditional_example\")\n\n# Classifier branch\nclassifier = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4.1-mini\"),\n    system=\"Classify requests as: question, task, or creative.\"\n)\n\n# Specialized handlers  \nqa_expert = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4.1-mini\"),\n    system=\"Answer questions clearly and accurately.\"\n)\n\ntask_expert = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4.1-mini\"),\n    system=\"Provide step-by-step task guidance.\"\n)\n\ncreative_expert = Branch(\n    chat_model=iModel(provider=\"anthropic\", model=\"claude-sonnet-4-20250514\"),\n    system=\"Help with creative projects and brainstorming.\"\n)\n\nsession.include_branches([classifier, qa_expert, task_expert, creative_expert])\n\nuser_input = \"How do I learn Python programming?\"\n\n# Step 1: Classify the request\nclassify_op = builder.add_operation(\n    \"communicate\",\n    branch=classifier,\n    instruction=f\"Classify as 'question', 'task', or 'creative': {user_input}\"\n)\n\n# Step 2: Route based on classification\nresult = await session.flow(builder.get_graph())\nclassification = result[\"operation_results\"][classify_op]\n\n# Route to appropriate handler\nif \"question\" in classification.lower():\n    handler = qa_expert\nelif \"task\" in classification.lower():\n    handler = task_expert\nelse:\n    handler = creative_expert\n\n# Execute chosen path\nresponse = await handler.communicate(f\"Handle this request: {user_input}\")\n</code></pre>"},{"location":"patterns/conditional-flows/#quality-gate-pattern","title":"Quality Gate Pattern","text":"<p>Content processing with quality thresholds:</p> <pre><code># Content creation and review branches\ncreator = Branch(\n    chat_model=iModel(provider=\"anthropic\", model=\"claude-sonnet-4-20250514\"),\n    system=\"Create high-quality content.\"\n)\n\nreviewer = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4.1-mini\"),\n    system=\"Review content quality and provide scores 1-10.\"\n)\n\neditor = Branch(\n    chat_model=iModel(provider=\"anthropic\", model=\"claude-sonnet-4-20250514\"),\n    system=\"Improve and refine content.\"\n)\n\ntopic = \"The future of renewable energy\"\n\n# Stage 1: Create initial content\ninitial_content = await creator.communicate(f\"Write article about: {topic}\")\n\n# Stage 2: Quality review \nquality_review = await reviewer.communicate(\n    f\"Rate this content 1-10: {initial_content}\"\n)\n\n# Quality gate: improve if below threshold\nif any(str(i) in quality_review for i in range(1, 7)):\n    # Below threshold - improve content\n    final_content = await editor.communicate(\n        f\"Improve based on review: {initial_content}\\n\\nReview: {quality_review}\"\n    )\nelse:\n    # Quality acceptable\n    final_content = initial_content\n</code></pre>"},{"location":"patterns/conditional-flows/#error-handling-with-fallbacks","title":"Error Handling with Fallbacks","text":"<p>Graceful degradation when primary processing fails:</p> <pre><code># Main and fallback processors\nprocessor = Branch(\n    chat_model=iModel(provider=\"anthropic\", model=\"claude-sonnet-4-20250514\"),\n    system=\"Process complex requests thoroughly.\"\n)\n\nfallback = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4.1-mini\"), \n    system=\"Handle requests with simplified approaches.\"\n)\n\n# Process with fallback\ntry:\n    # Try main processor first\n    result = await processor.communicate(request)\nexcept Exception:\n    # Fallback to simpler processor\n    try:\n        result = await fallback.communicate(f\"Simplified: {request}\")\n    except Exception:\n        result = \"Unable to process request\"\n</code></pre>"},{"location":"patterns/conditional-flows/#best-practices","title":"Best Practices","text":""},{"location":"patterns/conditional-flows/#clear-decision-criteria","title":"Clear Decision Criteria","text":"<pre><code># Good: Specific, measurable criteria\n\"Rate complexity as 'simple' (1-3 steps) or 'complex' (4+ steps)\"\n\n# Avoid: Vague criteria  \n\"Is this hard?\"\n</code></pre>"},{"location":"patterns/conditional-flows/#explicit-routing-logic","title":"Explicit Routing Logic","text":"<pre><code># Clear routing based on parsed results\nif \"urgent\" in priority_assessment.lower():\n    handler = urgent_processor\nelse:\n    handler = standard_processor\n</code></pre>"},{"location":"patterns/conditional-flows/#always-have-fallbacks","title":"Always Have Fallbacks","text":"<pre><code># Graceful degradation\ntry:\n    result = await complex_processor.communicate(request)\nexcept Exception:\n    result = await simple_processor.communicate(f\"Simplified: {request}\")\n</code></pre>"},{"location":"patterns/conditional-flows/#when-to-use","title":"When to Use","text":"<p>Perfect for: Content routing, difficulty adaptation, quality control, error recovery, resource optimization</p> <p>Key advantage: Runtime decision-making optimizes processing paths and handles varying input conditions intelligently.</p> <p>Conditional flows create adaptive workflows that route processing based on content, quality thresholds, and error conditions.</p>"},{"location":"patterns/fan-out-in/","title":"Fan-Out/In Pattern","text":"<p>Distribute work to multiple agents in parallel, then combine their results.</p>"},{"location":"patterns/fan-out-in/#basic-pattern","title":"Basic Pattern","text":"<pre><code>from lionagi import Session, Builder, Branch, iModel\n\nsession = Session()\nbuilder = Builder()\n\n# Fan-out: Create parallel research tasks\ntopics = [\"market analysis\", \"competitor review\", \"customer feedback\"]\nresearch_nodes = []\n\nfor topic in topics:\n    node = builder.add_operation(\"communicate\", instruction=f\"Research {topic}\")\n    research_nodes.append(node)\n\n# Fan-in: Synthesize all results\nsynthesis = builder.add_operation(\n    \"communicate\",\n    depends_on=research_nodes,\n    instruction=\"Combine all research into comprehensive report\"\n)\n\nresult = await session.flow(builder.get_graph())\n</code></pre> <p>This pattern demonstrates the power of parallel processing in LionAGI. The fan-out phase creates three research operations that run simultaneously, each focusing on a different aspect. The fan-in phase waits for all research to complete, then synthesizes the findings into a comprehensive report. This approach is 3x faster than sequential processing while providing more thorough coverage than any single analysis.</p>"},{"location":"patterns/fan-out-in/#production-implementation-with-cost-tracking","title":"Production Implementation with Cost Tracking","text":"<pre><code>from lionagi import Branch, iModel, Session, Builder\nfrom lionagi.fields import LIST_INSTRUCT_FIELD_MODEL, Instruct\nfrom lionagi.models import AssistantResponse\n\nasync def production_fan_out_in():\n    \"\"\"Production version with error handling and cost tracking\"\"\"\n\n    try:\n        orc_branch = Branch(\n            chat_model=iModel(provider=\"openai\", model=\"gpt-4.1-mini\"),\n            name=\"orchestrator\"\n        )\n        session = Session(default_branch=orc_branch)\n        builder = Builder(\"ProductionFanOut\")\n\n        # Initial decomposition\n        root = builder.add_operation(\n            \"operate\",\n            instruct=Instruct(\n                instruction=\"Break down the analysis task into parallel components\",\n                context=\"market_analysis\"\n            ),\n            reason=True,\n            field_models=[LIST_INSTRUCT_FIELD_MODEL]\n        )\n\n        result = await session.flow(builder.get_graph())\n        instruct_models = result[\"operation_results\"][root].instruct_models\n\n        # Create research nodes\n        research_nodes = []\n        for instruction in instruct_models:\n            node = builder.add_operation(\n                \"communicate\",\n                depends_on=[root],\n                chat_model=iModel(provider=\"openai\", model=\"gpt-4.1-mini\"),\n                **instruction.to_dict()\n            )\n            research_nodes.append(node)\n\n        # Execute research with cost tracking\n        costs = 0\n\n        def get_context(node_id):\n            nonlocal costs\n            graph = builder.get_graph()\n            node = graph.internal_nodes[node_id]\n            branch = session.get_branch(node.branch_id, None)\n            if (branch and len(branch.messages) &gt; 0 and \n                isinstance(msg := branch.messages[-1], AssistantResponse)):\n                costs += msg.model_response.get(\"total_cost_usd\") or 0\n                return f\"\"\"\nResponse: {msg.model_response.get(\"result\") or \"Not available\"}\nSummary: {msg.model_response.get(\"summary\") or \"Not available\"}\n                \"\"\".strip()\n\n        await session.flow(builder.get_graph())\n        ctx = [get_context(i) for i in research_nodes]\n\n        # Synthesis\n        synthesis = builder.add_operation(\n            \"communicate\",\n            depends_on=research_nodes,\n            branch=orc_branch,\n            instruction=\"Synthesize researcher findings\",\n            context=[i for i in ctx if i is not None]\n        )\n\n        final_result = await session.flow(builder.get_graph())\n        result_synthesis = final_result[\"operation_results\"][synthesis]\n\n        # Optional: Visualize execution graph\n        builder.visualize(\"Fan-out/in execution pattern\")\n\n        print(f\"Analysis complete. Total cost: ${costs:.4f}\")\n        return result_synthesis\n\n    except Exception as e:\n        print(f\"Fan-out-in failed: {e}\")\n        import traceback\n        traceback.print_exc()\n        return None\n\nasyncio.run(production_fan_out_in())\n</code></pre>"},{"location":"patterns/fan-out-in/#when-to-use","title":"When to Use","text":"<p>Perfect For</p> <ul> <li>Complex research: Multiple perspectives on the same topic</li> <li>Code reviews: Security, performance, style analysis in parallel</li> <li>Market analysis: Different domain experts working simultaneously  </li> <li>Large datasets: Parallel investigation and analysis</li> </ul> <p>Pattern Indicators</p> <p>Use fan-out/in when:</p> <ul> <li>Problem benefits from simultaneous analysis</li> <li>Individual tasks can run independently  </li> <li>Final answer requires synthesis of perspectives</li> <li>Time constraints favor parallel over sequential execution</li> </ul>"},{"location":"patterns/fan-out-in/#execution-flow","title":"Execution Flow","text":"<pre><code>[Orchestrator]\n     \u2193 (decompose task)\n[Task Breakdown]\n     \u2193 (fan-out)\n\u250c\u2500[Researcher 1]\u2500\u2510\n\u251c\u2500[Researcher 2]\u2500\u2524 \u2192 (parallel execution)\n\u251c\u2500[Researcher 3]\u2500\u2524\n\u2514\u2500[Researcher 4]\u2500\u2518\n     \u2193 (fan-in)\n[Synthesis]\n     \u2193\n[Final Result]\n</code></pre>"},{"location":"patterns/fan-out-in/#key-implementation-notes","title":"Key Implementation Notes","text":""},{"location":"patterns/fan-out-in/#context-extraction-pattern","title":"Context Extraction Pattern","text":"<pre><code>def get_context(node_id):\n    graph = builder.get_graph()\n    node = graph.internal_nodes[node_id]\n    branch = session.get_branch(node.branch_id, None)\n    if (branch and len(branch.messages) &gt; 0 and \n        isinstance(msg := branch.messages[-1], AssistantResponse)):\n        return msg.model_response.get(\"result\") or \"Not available\"\n</code></pre>"},{"location":"patterns/fan-out-in/#cost-tracking-pattern","title":"Cost Tracking Pattern","text":"<pre><code>costs = 0\ndef track_costs(msg):\n    nonlocal costs\n    costs += msg.model_response.get(\"total_cost_usd\") or 0\n</code></pre>"},{"location":"patterns/fan-out-in/#error-handling","title":"Error Handling","text":"<pre><code>try:\n    result = await session.flow(builder.get_graph())\nexcept Exception as e:\n    print(f\"Execution failed: {e}\")\n    import traceback\n    traceback.print_exc()\n    return None\n</code></pre>"},{"location":"patterns/fan-out-in/#performance-characteristics","title":"Performance Characteristics","text":"<p>Expected Performance</p> <ul> <li>Speed: 3-4x faster than sequential for complex analysis</li> <li>Quality: Higher insights through diverse perspectives  </li> <li>Success Rate: ~95% completion rate</li> <li>Scale: Optimal with 3-5 parallel researchers</li> </ul> <p>Cost Considerations</p> <p>Cost scales proportionally with number of parallel researchers. Balance thoroughness vs. expense based on your use case.</p> <p>Fan-out/in delivers comprehensive analysis through parallel specialization, making complex investigations both faster and more thorough.</p>"},{"location":"patterns/react-with-rag/","title":"ReAct with RAG Pattern","text":"<p>Tool-augmented reasoning with retrieval for knowledge-intensive tasks.</p>"},{"location":"patterns/react-with-rag/#when-to-use-this-pattern","title":"When to Use This Pattern","text":"<p>Use ReAct with RAG when:</p> <ul> <li>Tasks require external knowledge or data</li> <li>Multi-step reasoning is needed</li> <li>Information must be gathered and synthesized</li> <li>Complex problem-solving requires both thinking and acting</li> </ul>"},{"location":"patterns/react-with-rag/#pattern-structure","title":"Pattern Structure","text":"<p>ReAct cycles through: Reason \u2192 Act \u2192 Observe \u2192 Repeat</p>"},{"location":"patterns/react-with-rag/#basic-implementation","title":"Basic Implementation","text":"<pre><code>from lionagi import Branch, Session, Builder, iModel\n\ndef search_knowledge(query: str) -&gt; dict:\n    \"\"\"Your knowledge retrieval function\"\"\"\n    return {\"query\": query, \"results\": [...]}\n\n# ReAct-enabled Branch\nresearcher = Branch(\n    chat_model=iModel(provider=\"anthropic\", model=\"claude-sonnet-4-20250514\"),\n    system=\"Research and reason step by step using available tools.\",\n    tools=[search_knowledge]\n)\n\nsession = Session()\nbuilder = Builder()\n\n# ReAct operation\ntask = builder.add_operation(\n    \"ReAct\",\n    branch=researcher,\n    instruct={\n        \"instruction\": \"Research neural networks in machine learning\",\n        \"context\": \"Provide comprehensive analysis with specific details\"\n    },\n    max_extensions=3  # Reasoning steps limit\n)\n\nresult = await session.flow(builder.get_graph())\n</code></pre>"},{"location":"patterns/react-with-rag/#multi-tool-react","title":"Multi-Tool ReAct","text":"<pre><code>def search_papers(query: str) -&gt; dict:\n    \"\"\"Search academic sources\"\"\"\n    return {\"source\": \"papers\", \"results\": [...]}\n\ndef search_docs(query: str) -&gt; dict:\n    \"\"\"Search documentation\"\"\"\n    return {\"source\": \"docs\", \"results\": [...]}\n\n# Multi-tool researcher\nresearcher = Branch(\n    chat_model=iModel(provider=\"anthropic\", model=\"claude-sonnet-4-20250514\"),\n    system=\"Expert researcher with access to multiple knowledge sources.\",\n    tools=[search_papers, search_docs]\n)\n\n# Complex research task\ntask = builder.add_operation(\n    \"ReAct\",\n    branch=researcher,\n    instruct={\n        \"instruction\": \"Compare transformer architectures and performance metrics\",\n        \"guidance\": \"Use academic sources, then analyze technical documentation\"\n    },\n    max_extensions=5,\n    reason=True  # Include reasoning in output\n)\n</code></pre>"},{"location":"patterns/react-with-rag/#file-based-rag","title":"File-Based RAG","text":"<pre><code>from lionagi.tools.file.reader import ReaderTool\n\n# Document analyst\nanalyst = Branch(\n    chat_model=iModel(provider=\"anthropic\", model=\"claude-sonnet-4-20250514\"),\n    system=\"Analyze documents systematically, extracting key information.\",\n    tools=[ReaderTool]\n)\n\n# Document analysis task\nanalysis = builder.add_operation(\n    \"ReAct\",\n    branch=analyst,\n    instruct={\n        \"instruction\": \"Analyze project documentation for core architecture\",\n        \"guidance\": \"Start with README, then extract key technical details\"\n    },\n    max_extensions=4\n)\n</code></pre>"},{"location":"patterns/react-with-rag/#key-patterns","title":"Key Patterns","text":""},{"location":"patterns/react-with-rag/#tool-selection-strategy","title":"Tool Selection Strategy","text":"<pre><code># Phase-based tool usage\ninformation_gathering = [\"search\", \"retrieve\", \"extract\"]\nanalysis_phase = [\"analyze\", \"calculate\", \"compare\"] \nsynthesis_phase = [\"verify\", \"synthesize\", \"conclude\"]\n</code></pre>"},{"location":"patterns/react-with-rag/#reasoning-control","title":"Reasoning Control","text":"<pre><code>instruct = {\n    \"instruction\": \"Clear task definition\",\n    \"guidance\": \"Step-by-step process guidance\", \n    \"context\": \"Background for decision making\"\n}\n</code></pre>"},{"location":"patterns/react-with-rag/#performance-limits","title":"Performance Limits","text":"<pre><code>max_extensions=5  # Prevent infinite reasoning loops\nreason=True      # Include reasoning traces\n</code></pre>"},{"location":"patterns/react-with-rag/#best-practices","title":"Best Practices","text":"<ul> <li>Tool Design: Return structured data with consistent formats</li> <li>Reasoning Guidance: Provide clear step-by-step instructions</li> <li>Context Management: Keep context focused and relevant</li> <li>Error Handling: Set reasonable limits on reasoning steps</li> </ul> <p>ReAct with RAG enables systematic information gathering and reasoning for complex, knowledge-intensive tasks.</p>"},{"location":"patterns/sequential-analysis/","title":"Sequential Analysis Pattern","text":"<p>Build complex understanding step-by-step through dependent operations.</p>"},{"location":"patterns/sequential-analysis/#when-to-use-this-pattern","title":"When to Use This Pattern","text":"<p>Use sequential analysis when:</p> <ul> <li>Each step builds upon previous findings</li> <li>Processing requires logical progression</li> <li>Context accumulation improves quality</li> <li>Complex documents need structured analysis</li> </ul>"},{"location":"patterns/sequential-analysis/#basic-pattern-structure","title":"Basic Pattern Structure","text":"<pre><code>from lionagi import Branch, Session, Builder, iModel\n\nsession = Session()\nbuilder = Builder(\"document_analysis\")\n\n# Create analyzer branch\nanalyzer = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4.1-mini\"),\n    system=\"You are a document analysis expert.\"\n)\nsession.include_branches([analyzer])\n\ndocument = \"Your document content here...\"\n\n# Step 1: Extract key topics\nextract_topics = builder.add_operation(\n    \"communicate\",\n    branch=analyzer,\n    instruction=f\"Extract 3-5 key topics from this document: {document}\"\n)\n\n# Step 2: Analyze each topic (depends on step 1)\nanalyze_topics = builder.add_operation(\n    \"communicate\",\n    branch=analyzer,\n    instruction=\"For each topic, provide detailed analysis\",\n    depends_on=[extract_topics]\n)\n\n# Step 3: Synthesize insights (depends on step 2)\nsynthesize = builder.add_operation(\n    \"communicate\",\n    branch=analyzer,\n    instruction=\"What are the 3 most important insights?\",\n    depends_on=[analyze_topics]\n)\n\n# Execute the sequential workflow\nresult = await session.flow(builder.get_graph())\n</code></pre>"},{"location":"patterns/sequential-analysis/#multi-step-analysis","title":"Multi-Step Analysis","text":"<p>Research paper analysis with sequential dependency:</p> <pre><code># Specialized research analyzer\nresearcher = Branch(\n    chat_model=iModel(provider=\"anthropic\", model=\"claude-sonnet-4-20250514\"),\n    system=\"Research analyst specializing in academic papers.\"\n)\n\npaper_text = \"Your research paper content...\"\n\n# Step 1: Structure identification\nidentify_structure = builder.add_operation(\n    \"communicate\",\n    branch=researcher,\n    instruction=f\"Identify and summarize each section: {paper_text}\"\n)\n\n# Step 2: Technical analysis\nanalyze_technical = builder.add_operation(\n    \"communicate\",\n    branch=researcher,\n    instruction=\"Analyze technical contributions and methodology\",\n    depends_on=[identify_structure]\n)\n\n# Step 3: Evaluate novelty\nevaluate_novelty = builder.add_operation(\n    \"communicate\",\n    branch=researcher,\n    instruction=\"Assess novelty and significance of contributions\",\n    depends_on=[analyze_technical]\n)\n\n# Step 4: Final assessment\nfinal_assessment = builder.add_operation(\n    \"communicate\",\n    branch=researcher,\n    instruction=\"Provide comprehensive evaluation\",\n    depends_on=[evaluate_novelty]\n)\n\nresult = await session.flow(builder.get_graph())\n</code></pre>"},{"location":"patterns/sequential-analysis/#context-building","title":"Context Building","text":"<p>Each step accumulates context for deeper analysis:</p> <pre><code>investigator = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4.1-mini\"),\n    system=\"Thorough investigator building understanding incrementally.\"\n)\n\n# Sequential investigation steps\nobserve = builder.add_operation(\n    \"communicate\",\n    branch=investigator,\n    instruction=\"Make initial observations about the data\"\n)\n\nhypothesize = builder.add_operation(\n    \"communicate\", \n    branch=investigator,\n    instruction=\"Generate 3 hypotheses based on observations\",\n    depends_on=[observe]\n)\n\nanalyze = builder.add_operation(\n    \"communicate\",\n    branch=investigator, \n    instruction=\"Analyze each hypothesis for evidence\",\n    depends_on=[hypothesize]\n)\n\nconclude = builder.add_operation(\n    \"communicate\",\n    branch=investigator,\n    instruction=\"Draw conclusions with confidence levels\",\n    depends_on=[analyze]\n)\n\nresult = await session.flow(builder.get_graph())\n</code></pre>"},{"location":"patterns/sequential-analysis/#best-practices","title":"Best Practices","text":""},{"location":"patterns/sequential-analysis/#clear-dependencies","title":"Clear Dependencies","text":"<pre><code># Good: Clear progression\nstep1 = builder.add_operation(\"communicate\", instruction=\"Extract facts\")\nstep2 = builder.add_operation(\"communicate\", instruction=\"Analyze facts\", depends_on=[step1])\nstep3 = builder.add_operation(\"communicate\", instruction=\"Draw conclusions\", depends_on=[step2])\n</code></pre>"},{"location":"patterns/sequential-analysis/#consistent-context","title":"Consistent Context","text":"<pre><code># Use same branch for context continuity\nanalyzer = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4.1-mini\"),\n    system=\"Maintain context across analysis steps.\"\n)\n</code></pre>"},{"location":"patterns/sequential-analysis/#quality-assessment","title":"Quality Assessment","text":"<pre><code># Include data assessment as first step\nassess_data = builder.add_operation(\n    \"communicate\",\n    instruction=\"Assess data quality and identify limitations\"\n)\n</code></pre>"},{"location":"patterns/sequential-analysis/#when-to-use","title":"When to Use","text":"<p>Perfect for: Document analysis, research workflows, investigations, decision making, problem solving</p> <p>Key advantage: Each step builds meaningfully on previous work, leading to more thorough and accurate results than parallel analysis.</p> <p>Sequential analysis creates structured understanding through logical progression and context accumulation.</p>"},{"location":"patterns/tournament-validation/","title":"Tournament Validation Pattern","text":"<p>Competitive refinement where multiple agents propose solutions, then compete for the best result.</p>"},{"location":"patterns/tournament-validation/#when-to-use-this-pattern","title":"When to Use This Pattern","text":"<p>Use tournament validation when:</p> <ul> <li>Quality matters more than speed</li> <li>Multiple valid approaches exist</li> <li>Objective evaluation criteria can be defined</li> <li>Stakes are high (important decisions, critical code, etc.)</li> <li>You want to minimize bias from single perspectives</li> </ul>"},{"location":"patterns/tournament-validation/#pattern-structure","title":"Pattern Structure","text":"<ol> <li>Generation: Multiple agents create different solutions</li> <li>Evaluation: Judge agents rate each solution</li> <li>Tournament: Solutions compete head-to-head</li> <li>Refinement: Winners refine their approach</li> <li>Selection: Best solution emerges</li> </ol>"},{"location":"patterns/tournament-validation/#basic-tournament-pattern","title":"Basic Tournament Pattern","text":"<pre><code>from lionagi import Branch, Session, Builder, iModel\n\nsession = Session()\nbuilder = Builder(\"solution_tournament\")\n\n# Create diverse problem solvers\ncreative_solver = Branch(\n    chat_model=iModel(provider=\"anthropic\", model=\"claude-sonnet-4-20250514\"),\n    system=\"Approach problems creatively with unconventional solutions.\"\n)\n\nanalytical_solver = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4.1-mini\"),\n    system=\"Solve problems with systematic, analytical approaches.\"\n)\n\npractical_solver = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4.1-mini\"),\n    system=\"Focus on practical, implementable solutions.\"\n)\n\n# Judge for evaluation\njudge = Branch(\n    chat_model=iModel(provider=\"anthropic\", model=\"claude-sonnet-4-20250514\"),\n    system=\"Evaluate solutions objectively on feasibility and effectiveness.\"\n)\n\nsession.include_branches([creative_solver, analytical_solver, practical_solver, judge])\n\nproblem = \"Design a system to reduce food waste while maintaining profitability\"\n\n# Phase 1: Generate competing solutions\ncreative_solution = builder.add_operation(\n    \"communicate\",\n    branch=creative_solver,\n    instruction=f\"Propose creative solution: {problem}\"\n)\n\nanalytical_solution = builder.add_operation(\n    \"communicate\", \n    branch=analytical_solver,\n    instruction=f\"Propose systematic solution: {problem}\"\n)\n\npractical_solution = builder.add_operation(\n    \"communicate\",\n    branch=practical_solver,\n    instruction=f\"Propose practical solution: {problem}\"\n)\n\n# Execute solution generation\nresult = await session.flow(builder.get_graph())\n\nsolutions = {\n    \"creative\": result[\"operation_results\"][creative_solution],\n    \"analytical\": result[\"operation_results\"][analytical_solution],\n    \"practical\": result[\"operation_results\"][practical_solution]\n}\n\n# Phase 2: Judge evaluates all solutions\nevaluation = await judge.communicate(f\"\"\"\nEvaluate these solutions for: {problem}\n\nCreative: {solutions['creative']}\nAnalytical: {solutions['analytical']}\nPractical: {solutions['practical']}\n\nRate each 1-10 on feasibility, effectiveness, innovation.\nDeclare winner with reasoning.\n\"\"\")\n</code></pre>"},{"location":"patterns/tournament-validation/#multi-round-tournament","title":"Multi-Round Tournament","text":"<p>Elimination rounds with refinement:</p> <pre><code># Create multiple competitors with different approaches\ncompetitors = [\n    Branch(system=\"Prioritize user experience\", chat_model=iModel(provider=\"openai\", model=\"gpt-4.1-mini\")),\n    Branch(system=\"Focus on technical excellence\", chat_model=iModel(provider=\"openai\", model=\"gpt-4.1-mini\")),\n    Branch(system=\"Emphasize cost-effectiveness\", chat_model=iModel(provider=\"openai\", model=\"gpt-4.1-mini\"))\n]\n\n# Panel of specialized judges\njudges = [\n    Branch(system=\"Judge business viability\", chat_model=iModel(provider=\"anthropic\", model=\"claude-sonnet-4-20250514\")),\n    Branch(system=\"Judge technical feasibility\", chat_model=iModel(provider=\"anthropic\", model=\"claude-sonnet-4-20250514\"))\n]\n\nchallenge = \"Create a mobile app for better sleep habits\"\n\n# Round 1: Initial proposals\nproposals = {}\nfor i, competitor in enumerate(competitors):\n    proposal = await competitor.communicate(f\"Propose solution: {challenge}\")\n    proposals[f\"competitor_{i}\"] = proposal\n\n# Round 1: Judging\nscores = {}\nfor judge in judges:\n    judge_scores = await judge.communicate(f\"\"\"\n    Score these proposals 1-10: {proposals}\n    Format: competitor_0: X/10, competitor_1: Y/10, etc.\n    \"\"\")\n    scores[judge] = judge_scores\n\n# Round 2: Top performers refine solutions\n# (Parse scores to select finalists)\nfinalists = competitors[:2]  # Top 2\nfor finalist in finalists:\n    refined = await finalist.communicate(\"Refine your solution based on judge feedback\")\n\n# Final judging\nwinner = await judges[0].communicate(\"Select winner from refined proposals\")\n</code></pre>"},{"location":"patterns/tournament-validation/#code-tournament","title":"Code Tournament","text":"<p>Specialized competition for code solutions:</p> <pre><code># Different coding philosophies\nperformance_coder = Branch(\n    system=\"Write optimized, performance-focused code\",\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4.1-mini\")\n)\n\nreadable_coder = Branch(\n    system=\"Write clean, maintainable code\",\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4.1-mini\")\n)\n\nsecure_coder = Branch(\n    system=\"Write secure, robust code with error handling\",\n    chat_model=iModel(provider=\"anthropic\", model=\"claude-sonnet-4-20250514\")\n)\n\ncode_judge = Branch(\n    system=\"Senior developer evaluating code quality and best practices\",\n    chat_model=iModel(provider=\"anthropic\", model=\"claude-sonnet-4-20250514\")\n)\n\ncoding_challenge = \"Write Python function to get top 10 users by score from list of dicts\"\n\n# Generate solutions\nsolutions = {}\nsolutions[\"performance\"] = await performance_coder.communicate(f\"Optimize for speed: {coding_challenge}\")\nsolutions[\"readable\"] = await readable_coder.communicate(f\"Optimize for readability: {coding_challenge}\")  \nsolutions[\"secure\"] = await secure_coder.communicate(f\"Optimize for security: {coding_challenge}\")\n\n# Judge evaluates all solutions\nevaluation = await code_judge.communicate(f\"\"\"\nRate these solutions 1-10 on correctness, performance, readability, security:\n{solutions}\n\nDeclare winner and suggest hybrid approach.\n\"\"\")\n</code></pre>"},{"location":"patterns/tournament-validation/#collaborative-tournament","title":"Collaborative Tournament","text":"<p>Competition with cross-pollination:</p> <pre><code># Competing approaches\ninnovative = Branch(system=\"Focus on disruption\", chat_model=iModel(provider=\"openai\", model=\"gpt-4.1-mini\"))\npractical = Branch(system=\"Focus on execution\", chat_model=iModel(provider=\"openai\", model=\"gpt-4.1-mini\"))\nuser_focused = Branch(system=\"Focus on user experience\", chat_model=iModel(provider=\"anthropic\", model=\"claude-sonnet-4-20250514\"))\n\nproject = \"Sustainable urban transportation\"\n\n# Initial proposals\nproposals = {}\nfor team in [innovative, practical, user_focused]:\n    proposals[team] = await team.communicate(f\"Propose approach: {project}\")\n\n# Cross-pollination round\nrefined_proposals = {}\nfor team in [innovative, practical, user_focused]:\n    refined_proposals[team] = await team.communicate(f\"\"\"\n    Review other proposals: {proposals}\n    Refine your approach by incorporating best elements from others.\n    \"\"\")\n\n# Collaborative synthesis\nmediator = Branch(system=\"Identify synergies between approaches\", chat_model=iModel(provider=\"anthropic\", model=\"claude-sonnet-4-20250514\"))\n\nfinal_solution = await mediator.communicate(f\"\"\"\nCreate hybrid solution combining strengths: {refined_proposals}\n\"\"\")\n</code></pre>"},{"location":"patterns/tournament-validation/#best-practices","title":"Best Practices","text":""},{"location":"patterns/tournament-validation/#clear-evaluation-criteria","title":"Clear Evaluation Criteria","text":"<pre><code># Define specific, measurable criteria\nevaluation_criteria = {\n    \"feasibility\": \"Can this be realistically implemented?\",\n    \"effectiveness\": \"Will this solve the problem effectively?\", \n    \"innovation\": \"How creative/novel is this approach?\",\n    \"scalability\": \"Can this work at larger scales?\"\n}\n</code></pre>"},{"location":"patterns/tournament-validation/#diverse-perspectives","title":"Diverse Perspectives","text":"<pre><code># Different specialties and approaches\ncompetitors = [\n    Branch(system=\"Focus on technical excellence\"),\n    Branch(system=\"Prioritize user experience\"),\n    Branch(system=\"Emphasize cost-effectiveness\"),\n    Branch(system=\"Consider sustainability\")\n]\n</code></pre>"},{"location":"patterns/tournament-validation/#objective-judging","title":"Objective Judging","text":"<pre><code># Use specific scoring rubrics\njudge_prompt = \"\"\"\nRate 1-10 on:\n1. Technical feasibility\n2. Market viability  \n3. Implementation complexity\n4. Expected impact\n\nProvide scores and reasoning.\n\"\"\"\n</code></pre>"},{"location":"patterns/tournament-validation/#iterative-refinement","title":"Iterative Refinement","text":"<pre><code># Allow winners to improve based on feedback\nrefinement_prompt = f\"\"\"\nYour solution scored highest but judges noted: {feedback}\nRefine to address concerns while maintaining strengths.\n\"\"\"\n</code></pre>"},{"location":"patterns/tournament-validation/#when-to-use","title":"When to Use","text":"<p>Perfect for: High-stakes decisions, creative problems, quality-critical tasks, complex analysis, innovation challenges</p> <p>Key advantage: Competitive dynamics drive higher quality through diverse perspectives, objective evaluation, and iterative refinement.</p> <p>Tournament validation creates the highest quality solutions by leveraging competition and collaborative improvement.</p>"},{"location":"quickstart/installation/","title":"Installation","text":""},{"location":"quickstart/installation/#install-lionagi","title":"Install LionAGI","text":"pipuv <pre><code>pip install lionagi\n</code></pre> <pre><code>uv add lionagi\n</code></pre> <p>Requirements: Python &gt;= 3.10</p>"},{"location":"quickstart/installation/#configure-api-keys","title":"Configure API Keys","text":"<p>LionAGI loads API keys from environment variables or <code>.env</code> files automatically (via <code>pydantic-settings</code> and <code>python-dotenv</code>). Set at least one provider key.</p>"},{"location":"quickstart/installation/#environment-variables","title":"Environment variables","text":"<pre><code># OpenAI (default provider)\nexport OPENAI_API_KEY=sk-...\n\n# Or any other provider\nexport ANTHROPIC_API_KEY=sk-ant-...\nexport GEMINI_API_KEY=...\nexport NVIDIA_NIM_API_KEY=nvapi-...\nexport GROQ_API_KEY=gsk_...\nexport PERPLEXITY_API_KEY=pplx-...\nexport OPENROUTER_API_KEY=sk-or-...\n</code></pre>"},{"location":"quickstart/installation/#env-file","title":"<code>.env</code> file","text":"<p>Create a <code>.env</code> file in your project root:</p> <pre><code>OPENAI_API_KEY=sk-...\nANTHROPIC_API_KEY=sk-ant-...\n</code></pre> <p>LionAGI checks <code>.env</code>, <code>.env.local</code>, and <code>.secrets.env</code> in that order.</p>"},{"location":"quickstart/installation/#ollama-local-models","title":"Ollama (local models)","text":"<p>Ollama requires no API key. Install and run Ollama, then:</p> <pre><code>from lionagi import Branch, iModel\n\nbranch = Branch(\n    chat_model=iModel(provider=\"ollama\", model=\"llama3.2\")\n)\n</code></pre>"},{"location":"quickstart/installation/#verify-installation","title":"Verify Installation","text":"<pre><code>import lionagi\n\nprint(lionagi.__version__)\n</code></pre> <p>To verify an API connection works:</p> <pre><code>import asyncio\nfrom lionagi import Branch\n\nasync def main():\n    branch = Branch()\n    reply = await branch.communicate(\"Say hello in one word.\")\n    print(reply)\n\nasyncio.run(main())\n</code></pre> <p>If this prints a response, your installation and API key are working.</p>"},{"location":"quickstart/installation/#optional-extras","title":"Optional Extras","text":"<p>LionAGI ships with optional dependencies for specific features:</p> <pre><code># Install with specific extras\npip install \"lionagi[ollama]\"    # Ollama client library\npip install \"lionagi[mcp]\"      # MCP (Model Context Protocol) server support\npip install \"lionagi[reader]\"   # Document reading (docling)\npip install \"lionagi[rich]\"     # Rich terminal output\npip install \"lionagi[schema]\"   # Schema code generation\npip install \"lionagi[graph]\"    # Graph visualization (matplotlib, networkx)\npip install \"lionagi[postgres]\" # PostgreSQL storage\npip install \"lionagi[sqlite]\"   # SQLite storage\npip install \"lionagi[xml]\"      # XML parsing\n\n# Install everything\npip install \"lionagi[all]\"\n</code></pre>"},{"location":"quickstart/installation/#provider-reference","title":"Provider Reference","text":"Provider <code>provider=</code> Environment Variable Notes OpenAI <code>\"openai\"</code> <code>OPENAI_API_KEY</code> Default provider. GPT-4.1, GPT-4o, o-series. Anthropic <code>\"anthropic\"</code> <code>ANTHROPIC_API_KEY</code> Claude models via Messages API. Google Gemini <code>\"gemini\"</code> <code>GEMINI_API_KEY</code> Uses OpenAI-compatible endpoint. Ollama <code>\"ollama\"</code> (none) Local models. Requires Ollama running on <code>localhost:11434</code>. NVIDIA NIM <code>\"nvidia_nim\"</code> <code>NVIDIA_NIM_API_KEY</code> NVIDIA inference microservices. Groq <code>\"groq\"</code> <code>GROQ_API_KEY</code> Fast inference on LPU hardware. Perplexity <code>\"perplexity\"</code> <code>PERPLEXITY_API_KEY</code> Search-augmented responses. OpenRouter <code>\"openrouter\"</code> <code>OPENROUTER_API_KEY</code> Access 200+ models via single API."},{"location":"quickstart/installation/#custom-openai-compatible-endpoints","title":"Custom / OpenAI-compatible endpoints","text":"<p>Any endpoint that implements the OpenAI chat completions API:</p> <pre><code>from lionagi import iModel\n\nmodel = iModel(\n    provider=\"my_custom_provider\",\n    model=\"my-model-name\",\n    api_key=\"your-api-key\",\n    base_url=\"https://your-endpoint.com/v1\",\n)\n</code></pre>"},{"location":"quickstart/installation/#defaults","title":"Defaults","text":"<p>When you create a <code>Branch()</code> without specifying a model, LionAGI uses:</p> <ul> <li>Provider: <code>openai</code> (configurable via <code>LIONAGI_CHAT_PROVIDER</code> env var)</li> <li>Model: <code>gpt-4.1-mini</code> (configurable via <code>LIONAGI_CHAT_MODEL</code> env var)</li> </ul> <pre><code># Override defaults via environment\nexport LIONAGI_CHAT_PROVIDER=anthropic\nexport LIONAGI_CHAT_MODEL=claude-sonnet-4-20250514\n</code></pre>"},{"location":"quickstart/installation/#next-steps","title":"Next Steps","text":"<p>Ready to write code? Continue to Quick Start.</p>"},{"location":"quickstart/your-first-flow/","title":"Quick Start","text":"<p>Four examples, from simplest to most powerful. Each is complete and copy-pasteable.</p> <p>Prerequisites: <code>pip install lionagi</code> and <code>OPENAI_API_KEY</code> set. See Installation if you haven't done this yet.</p>"},{"location":"quickstart/your-first-flow/#1-simple-conversation-communicate","title":"1. Simple Conversation (<code>communicate</code>)","text":"<p><code>communicate()</code> sends a message, adds it and the response to the conversation history, and returns the response text.</p> <pre><code>import asyncio\nfrom lionagi import Branch\n\nasync def main():\n    branch = Branch(system=\"You are a concise assistant. Answer in one sentence.\")\n\n    # First message\n    answer = await branch.communicate(\"What causes rainbows?\")\n    print(answer)\n\n    # Follow-up -- the branch remembers the conversation\n    followup = await branch.communicate(\"How long do they typically last?\")\n    print(followup)\n\nasyncio.run(main())\n</code></pre> <p>What happened: Branch created a default iModel (OpenAI <code>gpt-4.1-mini</code>), sent your message with the system prompt, and stored both sides of the conversation. The follow-up question has full context of the previous exchange.</p> <p><code>chat()</code> vs <code>communicate()</code></p> <p><code>chat()</code> does not add messages to the conversation history. It is a lower-level method for orchestration where you manage message flow yourself. For most use cases, use <code>communicate()</code>.</p>"},{"location":"quickstart/your-first-flow/#2-structured-output-response_format","title":"2. Structured Output (<code>response_format</code>)","text":"<p>Pass a Pydantic model as <code>response_format</code> to get typed, validated output instead of raw text.</p> <pre><code>import asyncio\nfrom pydantic import BaseModel, Field\nfrom lionagi import Branch\n\nclass MovieReview(BaseModel):\n    title: str\n    year: int\n    rating: float = Field(ge=0, le=10)\n    summary: str\n    pros: list[str]\n    cons: list[str]\n\nasync def main():\n    branch = Branch(system=\"You are a film critic.\")\n    review = await branch.communicate(\n        \"Review the movie Inception.\",\n        response_format=MovieReview,\n    )\n\n    # review is a MovieReview instance, not a string\n    print(f\"{review.title} ({review.year}): {review.rating}/10\")\n    print(f\"Summary: {review.summary}\")\n    for pro in review.pros:\n        print(f\"  + {pro}\")\n    for con in review.cons:\n        print(f\"  - {con}\")\n\nasyncio.run(main())\n</code></pre> <p>What happened: LionAGI sent the Pydantic schema to the LLM as a response format constraint, then validated and parsed the response. If parsing fails, it retries with the parse model (up to 3 times by default). The return value is a <code>MovieReview</code> instance with full type safety.</p>"},{"location":"quickstart/your-first-flow/#3-tool-calling-operate","title":"3. Tool Calling (<code>operate</code>)","text":"<p>Register Python functions as tools. <code>operate()</code> lets the LLM call them and incorporates the results into its response.</p> <pre><code>import asyncio\nfrom lionagi import Branch\n\ndef calculate(expression: str) -&gt; str:\n    \"\"\"Evaluate a mathematical expression and return the result.\"\"\"\n    try:\n        result = eval(expression)  # In production, use a safe evaluator\n        return str(result)\n    except Exception as e:\n        return f\"Error: {e}\"\n\ndef lookup_constant(name: str) -&gt; str:\n    \"\"\"Look up a mathematical or physical constant by name.\"\"\"\n    constants = {\n        \"pi\": \"3.14159265358979\",\n        \"e\": \"2.71828182845905\",\n        \"c\": \"299792458 m/s\",\n        \"golden_ratio\": \"1.61803398874989\",\n    }\n    return constants.get(name.lower(), f\"Unknown constant: {name}\")\n\nasync def main():\n    branch = Branch(\n        system=\"You are a math assistant. Use the provided tools to compute answers.\",\n        tools=[calculate, lookup_constant],\n    )\n\n    result = await branch.operate(\n        instruction=\"What is pi squared plus the golden ratio?\",\n        actions=True,\n    )\n    print(result)\n\nasyncio.run(main())\n</code></pre> <p>What happened: <code>operate()</code> sent the instruction along with auto-generated JSON schemas for <code>calculate</code> and <code>lookup_constant</code>. The LLM decided which tools to call and with what arguments. LionAGI executed the function calls, fed the results back, and returned the final answer. All messages (instruction, tool calls, tool results, final response) are added to the conversation.</p> <p>Tool schema generation</p> <p>LionAGI generates OpenAI-compatible function schemas from your Python function signatures and docstrings automatically. Type hints and docstrings improve schema quality.</p>"},{"location":"quickstart/your-first-flow/#4-graph-workflows-session-builder","title":"4. Graph Workflows (<code>Session</code> + <code>Builder</code>)","text":"<p>For multi-step workflows with dependencies between operations, use <code>Session</code> with <code>Builder</code> to define and execute a DAG.</p> <pre><code>import asyncio\nfrom pydantic import BaseModel, Field\nfrom lionagi import Session, Builder\n\nclass StartupIdeas(BaseModel):\n    ideas: list[str] = Field(description=\"List of startup ideas\")\n\nclass Evaluation(BaseModel):\n    best_idea: str\n    reasoning: str\n    market_size: str\n    main_risk: str\n\nasync def main():\n    session = Session()\n    builder = Builder()\n\n    # Step 1: Generate ideas\n    step1 = builder.add_operation(\n        \"communicate\",\n        instruction=\"Generate 5 AI startup ideas focused on healthcare.\",\n        response_format=StartupIdeas,\n    )\n\n    # Step 2: Evaluate ideas (depends on step 1)\n    step2 = builder.add_operation(\n        \"communicate\",\n        instruction=(\n            \"Evaluate the ideas from the previous step. \"\n            \"Pick the best one and explain why.\"\n        ),\n        response_format=Evaluation,\n        depends_on=[step1],\n    )\n\n    # Execute the workflow\n    results = await session.flow(builder.get_graph())\n    print(results)\n\nasyncio.run(main())\n</code></pre> <p>What happened: <code>Builder</code> created two operation nodes in a directed graph, with step 2 depending on step 1. <code>session.flow()</code> executed the graph, running step 1 first and passing its context to step 2. Independent nodes (none in this example) execute in parallel automatically.</p>"},{"location":"quickstart/your-first-flow/#choosing-the-right-method","title":"Choosing the Right Method","text":"Method Adds to history? Tool calling? Use when <code>chat()</code> No No Low-level orchestration; you manage messages yourself <code>communicate()</code> Yes No Simple conversations and structured output <code>operate()</code> Yes Yes LLM needs to call functions/tools <code>parse()</code> No No Parse arbitrary text into a Pydantic model (no LLM conversation) <code>ReAct()</code> Yes Yes Multi-step reasoning with tool use (think-act-observe loops)"},{"location":"quickstart/your-first-flow/#using-a-different-provider","title":"Using a Different Provider","text":"<p>Every example above works with any provider. Just pass an <code>iModel</code>:</p> <pre><code>from lionagi import Branch, iModel\n\n# Anthropic\nbranch = Branch(\n    chat_model=iModel(provider=\"anthropic\", model=\"claude-sonnet-4-20250514\"),\n    system=\"You are a helpful assistant.\",\n)\n\n# Gemini\nbranch = Branch(\n    chat_model=iModel(provider=\"gemini\", model=\"gemini-2.5-flash\"),\n    system=\"You are a helpful assistant.\",\n)\n\n# Local Ollama\nbranch = Branch(\n    chat_model=iModel(provider=\"ollama\", model=\"llama3.2\"),\n    system=\"You are a helpful assistant.\",\n)\n</code></pre>"},{"location":"quickstart/your-first-flow/#next-steps","title":"Next Steps","text":"<ul> <li>Core Concepts -- understand Branch, Session, and iModel in depth</li> <li>Operations -- detailed guide to <code>chat</code>, <code>communicate</code>, <code>operate</code>, <code>parse</code>, and <code>ReAct</code></li> <li>Patterns -- production workflow patterns (fan-out/in, sequential analysis, tournaments)</li> <li>LLM Providers -- provider-specific configuration and features</li> </ul>"},{"location":"reference/changelog/","title":"Changelog","text":"<p>All notable changes to LionAGI are documented here.</p>"},{"location":"reference/changelog/#unreleased","title":"[Unreleased]","text":""},{"location":"reference/changelog/#added","title":"Added","text":"<ul> <li>Comprehensive documentation overhaul with streamlined examples</li> <li>Migration guides for AutoGen, CrewAI, and LangChain</li> <li>Enterprise-focused cookbook examples</li> <li>Integration guides for tools, vector stores, and MCP servers</li> </ul>"},{"location":"reference/changelog/#changed","title":"Changed","text":"<ul> <li>Simplified tool integration: functions can be passed directly without   <code>func_to_tool</code> wrapper</li> <li>Documentation style updated to focus on practical, production-ready patterns</li> <li>Tool API streamlined for better developer experience</li> </ul>"},{"location":"reference/changelog/#01511-2025-08-24","title":"[0.15.11] - 2025-08-24","text":""},{"location":"reference/changelog/#added_1","title":"Added","text":"<ul> <li><code>extract_json</code> and <code>fuzzy_json</code> functions in new <code>lionagi.ln</code> module for   robust JSON handling</li> <li>Availability check functions for optional dependencies:   <code>check_docling_available</code>, <code>check_networkx_available</code>,   <code>check_matplotlib_available</code></li> <li><code>list_adapters</code> method in Pile class for adapter enumeration</li> <li>Content serialization and validation methods in Node class</li> <li>New concurrency-related classes exported in <code>__all__</code> for better accessibility</li> </ul>"},{"location":"reference/changelog/#changed_1","title":"Changed","text":"<ul> <li>Performance: Replaced standard <code>json</code> with <code>orjson</code> for faster JSON   operations</li> <li>PostgreSQL Adapter: Major cleanup and refactoring (374 lines removed, 88   added) with enhanced table creation logic</li> <li>Utils Refactoring: Moved utilities from monolithic <code>utils.py</code> to organized   <code>lionagi.ln</code> module (393 lines removed)</li> <li>Node Serialization: Updated adaptation methods to use <code>as_jsonable</code>   instead of custom serialization</li> <li>Element Methods: Refactored serialization methods and enhanced <code>to_dict</code>   functionality</li> <li>MessageManager: Simplified methods by leveraging <code>filter_by_type</code>   functionality</li> <li>Type Consistency: Updated Progression class type variable from <code>E</code> to <code>T</code></li> <li>Updated pydapter dependency to v1.0.2</li> </ul>"},{"location":"reference/changelog/#fixed","title":"Fixed","text":"<ul> <li>Parameter name in MessageManager methods: <code>reversed</code> \u2192 <code>reverse</code></li> <li>Import statement for <code>fix_json_string</code> in test files</li> <li>Output examples in <code>persist_to_postgres_supabase</code> notebook</li> <li>Docling import handling in ReaderTool initialization</li> <li>Item type validation improvements in Pile class</li> </ul>"},{"location":"reference/changelog/#removed","title":"Removed","text":"<ul> <li>Package Management Module: Deleted entire <code>lionagi.libs.package</code> module   (138 lines removed)</li> <li>Removed <code>imports.py</code>, <code>management.py</code>, <code>params.py</code>, <code>system.py</code></li> <li>Redundant import statements and dead code cleanup</li> <li>StepModel and related tests from step module</li> </ul>"},{"location":"reference/changelog/#0159-2025-08-20","title":"[0.15.9] - 2025-08-20","text":""},{"location":"reference/changelog/#added_2","title":"Added","text":"<ul> <li>JSON serialization utilities with orjson support</li> <li>Enhanced Element class with orjson-based JSON serialization methods</li> <li>User serialization method in Session class</li> </ul>"},{"location":"reference/changelog/#changed_2","title":"Changed","text":"<ul> <li>JSON Performance: Replaced <code>json.dumps</code> with <code>ln.json_dumps</code> using orjson   for consistent, faster serialization</li> <li>EventStatus Refactoring: Updated to use <code>ln.Enum</code> with improved JSON   serialization</li> <li>CI/CD: Upgraded actions/checkout to v5, removed documentation build   workflow</li> </ul>"},{"location":"reference/changelog/#0158-2025-08-20","title":"[0.15.8] - 2025-08-20","text":""},{"location":"reference/changelog/#changed_3","title":"Changed","text":"<ul> <li>Lowered psutil dependency requirements for broader compatibility</li> </ul>"},{"location":"reference/changelog/#0157-2025-08-18","title":"[0.15.7] - 2025-08-18","text":""},{"location":"reference/changelog/#fixed_1","title":"Fixed","text":"<ul> <li>Enhanced Params initialization to validate allowed keys</li> <li>Parameter validation improvements</li> </ul>"},{"location":"reference/changelog/#0156-2025-08-18","title":"[0.15.6] - 2025-08-18","text":""},{"location":"reference/changelog/#added_3","title":"Added","text":"<ul> <li><code>aicall_params</code> to register_operation for async parallel execution support</li> </ul>"},{"location":"reference/changelog/#fixed_2","title":"Fixed","text":"<ul> <li>Flow execution refactored to use <code>alcall</code> for improved concurrency handling</li> <li>Updated <code>alcall</code> and <code>bcall</code> parameter handling with better kwargs support</li> <li>Import statements for ConcurrencyEvent and Semaphore consistency</li> </ul>"},{"location":"reference/changelog/#0155-2025-08-17","title":"[0.15.5] - 2025-08-17","text":""},{"location":"reference/changelog/#added_4","title":"Added","text":"<ul> <li><code>aiofiles</code> dependency for async file operations</li> <li>Utility functions for union type handling and type annotations</li> <li>Async PostgreSQL adapter registration with availability checks</li> </ul>"},{"location":"reference/changelog/#changed_4","title":"Changed","text":"<ul> <li>Enhanced Pile class with validation and serialization methods</li> <li>Refactored PostgreSQL adapter checks into utility functions</li> </ul>"},{"location":"reference/changelog/#fixed_3","title":"Fixed","text":"<ul> <li>Parameter name correction: <code>strict</code> \u2192 <code>strict_type</code> in Pile initialization</li> <li>Exception handling: <code>TypeError</code> \u2192 <code>ValidationError</code> in collection validation</li> <li>Explicit boolean checks for async PostgreSQL availability</li> </ul>"},{"location":"reference/changelog/#0154-2025-08-17","title":"[0.15.4] - 2025-08-17","text":""},{"location":"reference/changelog/#added_5","title":"Added","text":"<ul> <li>User serialization functionality in Branch class</li> </ul>"},{"location":"reference/changelog/#0153-2025-08-16","title":"[0.15.3] - 2025-08-16","text":""},{"location":"reference/changelog/#added_6","title":"Added","text":"<ul> <li>Comprehensive tests for operation cancellation and edge conditions</li> <li>SKIPPED status to EventStatus for better execution tracking</li> </ul>"},{"location":"reference/changelog/#changed_5","title":"Changed","text":"<ul> <li>Execution status set to CANCELLED for cancelled API calls</li> <li>Enhanced operation handling with edge condition validation and filtering   aggregation metadata</li> </ul>"},{"location":"reference/changelog/#fixed_4","title":"Fixed","text":"<ul> <li>Flow regression issues in operation execution</li> <li>Parameter handling cleanup and improved cancellation error handling</li> </ul>"},{"location":"reference/changelog/#0152-2025-08-16","title":"[0.15.2] - 2025-08-16","text":""},{"location":"reference/changelog/#added_7","title":"Added","text":"<ul> <li>Operation decorator to simplify function registration as operations</li> <li>Comprehensive tests for Session.operation() decorator functionality</li> </ul>"},{"location":"reference/changelog/#changed_6","title":"Changed","text":"<ul> <li>Updated author information in README</li> </ul>"},{"location":"reference/changelog/#0151-2025-08-16","title":"[0.15.1] - 2025-08-16","text":""},{"location":"reference/changelog/#added_8","title":"Added","text":"<ul> <li>Mock operation methods for improved async operation handling in tests</li> <li><code>to_dict</code> method to Execution class for better serialization</li> </ul>"},{"location":"reference/changelog/#changed_7","title":"Changed","text":"<ul> <li>Integrated OperationManager into Branch and Session classes for enhanced   operation management</li> <li>Simplified OperationManager initialization with enhanced operation   registration logic</li> <li>Enhanced Execution class response serialization handling</li> </ul>"},{"location":"reference/changelog/#fixed_5","title":"Fixed","text":"<ul> <li>Import statement cleanup across multiple files for consistency</li> </ul>"},{"location":"reference/changelog/#01411-2025-08-14","title":"[0.14.11] - 2025-08-14","text":""},{"location":"reference/changelog/#added_9","title":"Added","text":"<ul> <li>Updated operation_builder notebook to demonstrate graph serialization</li> </ul>"},{"location":"reference/changelog/#fixed_6","title":"Fixed","text":"<ul> <li>Graph serialization/deserialization in Graph class</li> <li>Field parameters in Operation class</li> <li>Redundant file mode in open() calls</li> </ul>"},{"location":"reference/changelog/#changed_8","title":"Changed","text":"<ul> <li>Organized imports in throttle.py, cleaned up unused imports in test files</li> </ul>"},{"location":"reference/changelog/#01410-2025-08-08","title":"[0.14.10] - 2025-08-08","text":""},{"location":"reference/changelog/#added_10","title":"Added","text":"<ul> <li>XML parsing and conversion utilities with new XMLParser class</li> </ul>"},{"location":"reference/changelog/#fixed_7","title":"Fixed","text":"<ul> <li>Hook registry async function calls with proper await usage</li> <li>Import path for to_num utility in test files</li> <li>Error handling improvements in HookRegistry</li> </ul>"},{"location":"reference/changelog/#0137-2025-07-21","title":"[0.13.7] - 2025-07-21","text":""},{"location":"reference/changelog/#added_11","title":"Added","text":"<ul> <li>Notebook for sequential analysis of academic claims using Operation Graphs</li> </ul>"},{"location":"reference/changelog/#removed_1","title":"Removed","text":"<ul> <li><code>action_batch_size</code> parameter from operate and branch methods</li> </ul>"},{"location":"reference/changelog/#changed_9","title":"Changed","text":"<ul> <li>Updated documentation to reflect parameter changes</li> </ul>"},{"location":"reference/troubleshooting/","title":"Troubleshooting Guide","text":"<p>Quick solutions to common LionAGI issues.</p>"},{"location":"reference/troubleshooting/#installation-issues","title":"Installation Issues","text":"<p>Import Error: Module not found</p> <pre><code># Error: ModuleNotFoundError: No module named 'lionagi'\nuv add lionagi\n\n# For latest development version\nuv add git+https://github.com/khive-ai/lionagi.git\n</code></pre> <p>Missing Optional Dependencies</p> <pre><code># Error: docling not available\nuv add \"lionagi[pdf]\"  # For PDF processing\n\n# Error: matplotlib not available\nuv add matplotlib\n\n# Error: networkx not available\nuv add networkx\n</code></pre> <p>Python Version Issues</p> <pre><code># LionAGI requires Python 3.10+\npython --version  # Check version\nuv add lionagi  # Only works on 3.10+\n</code></pre>"},{"location":"reference/troubleshooting/#api-key-errors","title":"API Key Errors","text":"<p>OpenAI Authentication</p> <pre><code>import os\nfrom lionagi import Branch, iModel\n\n# Set API key\nos.environ[\"OPENAI_API_KEY\"] = \"your-key-here\"\n\nbranch = Branch(chat_model=iModel(provider=\"openai\", model=\"gpt-4.1-mini\"))\n</code></pre> <p>Multiple Providers</p> <pre><code># Different ways to set keys\nos.environ[\"ANTHROPIC_API_KEY\"] = \"your-anthropic-key\"\nos.environ[\"OPENAI_API_KEY\"] = \"your-openai-key\"\n\n# Or in iModel directly\nmodel = iModel(provider=\"openai\", model=\"gpt-4.1-mini\", api_key=\"your-key\")\n</code></pre>"},{"location":"reference/troubleshooting/#asyncawait-problems","title":"Async/Await Problems","text":"<p>Missing await</p> <pre><code># \u274c Wrong - missing await\nbranch = Branch()\nresult = branch.communicate(\"Hello\")  # Returns coroutine\n\n# \u2705 Correct\nresult = await branch.communicate(\"Hello\")\n</code></pre> <p>Running in Jupyter</p> <pre><code># Jupyter handles async automatically\nbranch = Branch()\nresult = await branch.communicate(\"Hello\")  # Works in Jupyter\n\n# For scripts, use asyncio.run()\nimport asyncio\n\nasync def main():\n    branch = Branch()\n    result = await branch.communicate(\"Hello\")\n    return result\n\n# \u274c Wrong in script\nresult = await main()\n\n# \u2705 Correct in script  \nresult = asyncio.run(main())\n</code></pre> <p>Mixing sync/async</p> <pre><code># \u274c Wrong - can't await in sync function\ndef sync_function():\n    branch = Branch()\n    return await branch.communicate(\"Hello\")  # SyntaxError\n\n# \u2705 Correct - make function async\nasync def async_function():\n    branch = Branch()\n    return await branch.communicate(\"Hello\")\n</code></pre>"},{"location":"reference/troubleshooting/#performance-issues","title":"Performance Issues","text":"<p>Slow Parallel Execution</p> <pre><code># \u274c Slow - sequential execution\nresults = []\nfor topic in topics:\n    result = await branch.communicate(f\"Research {topic}\")\n    results.append(result)\n\n# \u2705 Fast - parallel execution\nimport asyncio\n\ntasks = [branch.communicate(f\"Research {topic}\") for topic in topics]\nresults = await asyncio.gather(*tasks)\n</code></pre> <p>Graph vs Direct Calls</p> <pre><code># Use graphs for complex workflows\nfrom lionagi import Session, Builder\n\nsession = Session()\nbuilder = Builder()\n\n# Parallel operations\nfor topic in topics:\n    builder.add_operation(\"communicate\", instruction=f\"Research {topic}\")\n\nresults = await session.flow(builder.get_graph())\n</code></pre> <p>Model Rate Limits</p> <pre><code># Add delays between requests\nimport asyncio\n\nasync def rate_limited_call():\n    try:\n        result = await branch.communicate(\"Hello\")\n        return result\n    except Exception as e:\n        if \"rate limit\" in str(e).lower():\n            await asyncio.sleep(1)  # Wait 1 second\n            return await branch.communicate(\"Hello\")\n        raise e\n</code></pre>"},{"location":"reference/troubleshooting/#memory-issues","title":"Memory Issues","text":"<p>Token Limit Exceeded</p> <pre><code># \u274c Error: Context too long\nlong_message = \"very long text...\" * 1000\nresult = await branch.communicate(long_message)  # May fail\n\n# \u2705 Solution: Chunk large inputs\ndef chunk_text(text, chunk_size=4000):\n    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n\nchunks = chunk_text(long_message)\nresults = []\nfor chunk in chunks:\n    result = await branch.communicate(f\"Process this: {chunk}\")\n    results.append(result)\n</code></pre> <p>Branch Memory Accumulation</p> <pre><code># Branch remembers all messages\nbranch = Branch()\nfor i in range(1000):\n    await branch.communicate(f\"Message {i}\")  # Memory keeps growing\n\n# Solution: Create new branch when needed\ndef get_fresh_branch():\n    return Branch(chat_model=iModel(provider=\"openai\", model=\"gpt-4.1-mini\"))\n\n# Or clear messages (if implemented)\n# branch.messages.clear()  # Check if available\n</code></pre>"},{"location":"reference/troubleshooting/#graph-execution-issues","title":"Graph Execution Issues","text":"<p>Circular Dependencies</p> <pre><code># \u274c Error: Circular dependency\nnode_a = builder.add_operation(\"communicate\", depends_on=[node_b])\nnode_b = builder.add_operation(\"communicate\", depends_on=[node_a])  # Error\n\n# \u2705 Solution: Linear dependencies\nnode_a = builder.add_operation(\"communicate\", instruction=\"Step 1\")\nnode_b = builder.add_operation(\"communicate\", depends_on=[node_a], instruction=\"Step 2\")\n</code></pre> <p>Empty Graph Results</p> <pre><code># Check graph execution\ntry:\n    result = await session.flow(builder.get_graph())\n    print(\"Graph result:\", result)\n\n    # Access specific nodes\n    graph = builder.get_graph()\n    for node_id, node in graph.internal_nodes.items():\n        print(f\"Node {node_id}: {node}\")\n\nexcept Exception as e:\n    import traceback\n    traceback.print_exc()\n</code></pre>"},{"location":"reference/troubleshooting/#cost-tracking-issues","title":"Cost Tracking Issues","text":"<p>Missing Cost Data</p> <pre><code>def get_costs(node_id, builder, session):\n    try:\n        graph = builder.get_graph()\n        node = graph.internal_nodes[node_id]\n        branch = session.get_branch(node.branch_id, None)\n\n        if branch and len(branch.messages) &gt; 0:\n            msg = branch.messages[-1]\n            if hasattr(msg, 'model_response'):\n                return msg.model_response.get(\"total_cost_usd\", 0)\n    except Exception as e:\n        print(f\"Cost tracking error: {e}\")\n    return 0\n</code></pre>"},{"location":"reference/troubleshooting/#error-diagnosis","title":"Error Diagnosis","text":"<p>Enable Verbose Logging</p> <pre><code>import logging\nlogging.basicConfig(level=logging.DEBUG)\n\n# Or for specific operations\nresult = await session.flow(builder.get_graph(), verbose=True)\n</code></pre> <p>Debugging Graph State</p> <pre><code># Check graph structure\ngraph = builder.get_graph()\nprint(f\"Nodes: {len(graph.internal_nodes)}\")\nfor node_id, node in graph.internal_nodes.items():\n    print(f\"  {node_id}: {node}\")\n\n# Check session state\nprint(f\"Branches: {len(session.branches)}\")\n</code></pre>"},{"location":"reference/troubleshooting/#getting-help","title":"Getting Help","text":"<p>GitHub Issues: Report bugs at khive-ai/lionagi/issues</p> <p>Check Version: <code>uv pip show lionagi</code> for installed version</p> <p>Minimal Reproduction: Include minimal code that reproduces the issue</p>"},{"location":"reference/api/","title":"API Reference","text":"<p>Complete reference for the lionagi public API. All classes documented here are importable from the top-level package:</p> <pre><code>from lionagi import Branch, Session, iModel, Builder, Operation\n</code></pre> <p>Provider API keys are read from environment variables (<code>OPENAI_API_KEY</code>, <code>ANTHROPIC_API_KEY</code>, etc.) or can be passed explicitly.</p>"},{"location":"reference/api/#branch","title":"Branch","text":"<p>The primary API surface for lionagi. A <code>Branch</code> manages a single conversation thread with message history, tool registration, and LLM operations.</p> <p><code>Branch</code> inherits from <code>Element</code> (UUID identity + timestamp + metadata) and supports the async context manager protocol.</p>"},{"location":"reference/api/#constructor","title":"Constructor","text":"<pre><code>Branch(\n    *,\n    user: SenderRecipient = None,\n    name: str | None = None,\n    messages: Pile[RoledMessage] = None,\n    system: System | JsonValue = None,\n    system_sender: SenderRecipient = None,\n    chat_model: iModel | dict = None,\n    parse_model: iModel | dict = None,\n    imodel: iModel = None,                    # deprecated, alias of chat_model\n    tools: FuncTool | list[FuncTool] = None,\n    log_config: DataLoggerConfig | dict = None,\n    system_datetime: bool | str = None,\n    system_template = None,\n    system_template_context: dict = None,\n    logs: Pile[Log] = None,\n    use_lion_system_message: bool = False,\n    **kwargs,\n)\n</code></pre> Parameter Type Default Description <code>user</code> <code>SenderRecipient \\| None</code> <code>None</code> Owner or sender context for this branch. <code>name</code> <code>str \\| None</code> <code>None</code> Human-readable name for the branch. <code>messages</code> <code>Pile[RoledMessage] \\| None</code> <code>None</code> Pre-existing messages to seed the conversation. <code>system</code> <code>System \\| JsonValue \\| None</code> <code>None</code> System message content for the LLM. <code>system_sender</code> <code>SenderRecipient \\| None</code> <code>None</code> Sender attributed to the system message. <code>chat_model</code> <code>iModel \\| dict \\| None</code> <code>None</code> Primary chat model. Defaults to the provider/model in <code>AppSettings</code>. <code>parse_model</code> <code>iModel \\| dict \\| None</code> <code>None</code> Model for structured parsing. Falls back to <code>chat_model</code>. <code>imodel</code> <code>iModel \\| None</code> <code>None</code> Deprecated. Use <code>chat_model</code> instead. <code>tools</code> <code>FuncTool \\| list[FuncTool] \\| None</code> <code>None</code> Tools (functions or <code>Tool</code> objects) to register. <code>log_config</code> <code>DataLoggerConfig \\| dict \\| None</code> <code>None</code> Logging configuration. <code>system_datetime</code> <code>bool \\| str \\| None</code> <code>None</code> Include timestamps in system messages. <code>True</code> for default format, or a <code>strftime</code> string. <code>system_template</code> <code>Template \\| str \\| None</code> <code>None</code> Jinja2 template for the system message. <code>system_template_context</code> <code>dict \\| None</code> <code>None</code> Variables for rendering the system template. <code>logs</code> <code>Pile[Log] \\| None</code> <code>None</code> Pre-existing logs. <code>use_lion_system_message</code> <code>bool</code> <code>False</code> If <code>True</code>, prepends the default Lion system prompt. <p>Example:</p> <pre><code>from lionagi import Branch, iModel\n\nbranch = Branch(\n    chat_model=iModel(provider=\"openai\", model=\"gpt-4.1-mini\"),\n    system=\"You are a helpful research assistant.\",\n)\n</code></pre>"},{"location":"reference/api/#async-context-manager","title":"Async Context Manager","text":"<p><code>Branch</code> supports <code>async with</code> for automatic log flushing on exit:</p> <pre><code>async with Branch(system=\"You are helpful.\") as branch:\n    result = await branch.communicate(\"Hello!\")\n# logs are automatically dumped on exit\n</code></pre>"},{"location":"reference/api/#properties","title":"Properties","text":"Property Type Description <code>id</code> <code>UUID</code> Unique identifier (inherited from <code>Element</code>). <code>created_at</code> <code>float</code> Creation timestamp (Unix epoch). <code>metadata</code> <code>dict</code> Arbitrary metadata dictionary. <code>system</code> <code>System \\| None</code> The system message, if any. <code>messages</code> <code>Pile[RoledMessage]</code> All messages in the conversation. <code>logs</code> <code>Pile[Log]</code> All activity logs. <code>chat_model</code> <code>iModel</code> The primary chat model (read/write). <code>parse_model</code> <code>iModel</code> The parsing model (read/write). <code>tools</code> <code>dict[str, Tool]</code> Registered tools, keyed by name. <code>msgs</code> <code>MessageManager</code> The underlying message manager. <code>acts</code> <code>ActionManager</code> The underlying action/tool manager. <code>mdls</code> <code>iModelManager</code> The underlying model manager."},{"location":"reference/api/#methods","title":"Methods","text":""},{"location":"reference/api/#chat","title":"chat","text":"<pre><code>async def chat(\n    self,\n    instruction: Instruction | JsonValue = None,\n    guidance: JsonValue = None,\n    context: JsonValue = None,\n    sender: ID.Ref = None,\n    recipient: ID.Ref = None,\n    request_fields: list[str] | dict[str, JsonValue] = None,\n    response_format: type[BaseModel] | BaseModel = None,\n    progression: Progression | list = None,\n    imodel: iModel = None,\n    tool_schemas: list[dict] = None,\n    images: list = None,\n    image_detail: Literal[\"low\", \"high\", \"auto\"] = None,\n    plain_content: str = None,\n    return_ins_res_message: bool = False,\n    include_token_usage_to_model: bool = False,\n    **kwargs,\n) -&gt; tuple[Instruction, AssistantResponse]\n</code></pre> <p>Low-level LLM invocation using the current conversation history. Messages are not automatically appended to the branch -- use <code>communicate</code> or <code>operate</code> for that.</p> <p>Parameters:</p> Parameter Type Default Description <code>instruction</code> <code>Instruction \\| JsonValue</code> <code>None</code> Main user instruction text or data. <code>guidance</code> <code>JsonValue</code> <code>None</code> Additional system/user guidance. <code>context</code> <code>JsonValue</code> <code>None</code> Context data for the model. <code>sender</code> <code>ID.Ref</code> <code>None</code> Message sender (defaults to <code>branch.user</code>). <code>recipient</code> <code>ID.Ref</code> <code>None</code> Message recipient (defaults to <code>branch.id</code>). <code>request_fields</code> <code>list[str] \\| dict</code> <code>None</code> Field-level validation hints. <code>response_format</code> <code>type[BaseModel]</code> <code>None</code> Pydantic model for structured responses. <code>progression</code> <code>Progression \\| list</code> <code>None</code> Custom message ordering. <code>imodel</code> <code>iModel</code> <code>None</code> Override the default chat model. <code>tool_schemas</code> <code>list[dict]</code> <code>None</code> Tool schemas for function calling. <code>images</code> <code>list</code> <code>None</code> Images to include in the prompt. <code>image_detail</code> <code>\"low\" \\| \"high\" \\| \"auto\"</code> <code>None</code> Image detail level. <code>plain_content</code> <code>str</code> <code>None</code> Plain text content, overrides other content. <code>return_ins_res_message</code> <code>bool</code> <code>False</code> If <code>True</code>, returns <code>(Instruction, AssistantResponse)</code>. Otherwise returns response content only. <code>include_token_usage_to_model</code> <code>bool</code> <code>False</code> Include token usage in model messages. <p>Returns: <code>tuple[Instruction, AssistantResponse]</code> -- the instruction and the model's response.</p>"},{"location":"reference/api/#communicate","title":"communicate","text":"<pre><code>async def communicate(\n    self,\n    instruction: Instruction | JsonValue = None,\n    *,\n    guidance: JsonValue = None,\n    context: JsonValue = None,\n    plain_content: str = None,\n    sender: SenderRecipient = None,\n    recipient: SenderRecipient = None,\n    progression: ID.IDSeq = None,\n    response_format: type[BaseModel] = None,\n    request_fields: dict | list[str] = None,\n    chat_model: iModel = None,\n    parse_model: iModel = None,\n    skip_validation: bool = False,\n    images: list = None,\n    image_detail: Literal[\"low\", \"high\", \"auto\"] = None,\n    num_parse_retries: int = 3,\n    clear_messages: bool = False,\n    include_token_usage_to_model: bool = False,\n    **kwargs,\n) -&gt; str | BaseModel | dict | None\n</code></pre> <p>High-level conversational call. Messages are automatically added to the branch. Simpler than <code>operate</code> -- no tool invocation.</p> <p>Parameters:</p> Parameter Type Default Description <code>instruction</code> <code>Instruction \\| JsonValue</code> <code>None</code> The user's main query. <code>guidance</code> <code>JsonValue</code> <code>None</code> Additional guidance for the model. <code>context</code> <code>JsonValue</code> <code>None</code> Extra context data. <code>plain_content</code> <code>str</code> <code>None</code> Plain text appended to the instruction. <code>response_format</code> <code>type[BaseModel]</code> <code>None</code> Pydantic model for structured output. <code>request_fields</code> <code>dict \\| list[str]</code> <code>None</code> Specific fields to extract. <code>chat_model</code> <code>iModel</code> <code>None</code> Override the default chat model. <code>parse_model</code> <code>iModel</code> <code>None</code> Override the default parse model. <code>skip_validation</code> <code>bool</code> <code>False</code> Return raw string without parsing. <code>images</code> <code>list</code> <code>None</code> Images for the context. <code>image_detail</code> <code>\"low\" \\| \"high\" \\| \"auto\"</code> <code>None</code> Image detail level. <code>num_parse_retries</code> <code>int</code> <code>3</code> Max parse retries (capped at 5). <code>clear_messages</code> <code>bool</code> <code>False</code> Clear stored messages before sending. <code>include_token_usage_to_model</code> <code>bool</code> <code>False</code> Include token usage in model messages. <p>Returns: <code>str | BaseModel | dict | None</code> -- raw string, validated model, field dict, or <code>None</code> on parse failure.</p> <p>Example:</p> <pre><code># Simple text response\nanswer = await branch.communicate(\"What is the capital of France?\")\n\n# Structured response\nfrom pydantic import BaseModel\n\nclass Answer(BaseModel):\n    city: str\n    country: str\n\nresult = await branch.communicate(\n    \"What is the capital of France?\",\n    response_format=Answer,\n)\nprint(result.city)  # \"Paris\"\n</code></pre>"},{"location":"reference/api/#operate","title":"operate","text":"<pre><code>async def operate(\n    self,\n    *,\n    instruct: Instruct = None,\n    instruction: Instruction | JsonValue = None,\n    guidance: JsonValue = None,\n    context: JsonValue = None,\n    sender: SenderRecipient = None,\n    recipient: SenderRecipient = None,\n    progression: Progression = None,\n    chat_model: iModel = None,\n    invoke_actions: bool = True,\n    tool_schemas: list[dict] = None,\n    images: list = None,\n    image_detail: Literal[\"low\", \"high\", \"auto\"] = None,\n    parse_model: iModel = None,\n    skip_validation: bool = False,\n    tools: ToolRef = None,\n    operative: Operative = None,\n    response_format: type[BaseModel] = None,\n    actions: bool = False,\n    reason: bool = False,\n    call_params: AlcallParams = None,\n    action_strategy: Literal[\"sequential\", \"concurrent\"] = \"concurrent\",\n    verbose_action: bool = False,\n    field_models: list[FieldModel] = None,\n    exclude_fields: list | dict | None = None,\n    handle_validation: Literal[\"raise\", \"return_value\", \"return_none\"] = \"return_value\",\n    include_token_usage_to_model: bool = False,\n    **kwargs,\n) -&gt; list | BaseModel | None | dict | str\n</code></pre> <p>Full orchestration with optional tool invocation and structured response parsing. Messages are automatically added to the conversation.</p> <p>Key Parameters:</p> Parameter Type Default Description <code>instruct</code> <code>Instruct</code> <code>None</code> Instruction bundle (instruction + guidance + context). <code>instruction</code> <code>Instruction \\| JsonValue</code> <code>None</code> Direct instruction (alternative to <code>instruct</code>). <code>guidance</code> <code>JsonValue</code> <code>None</code> Additional guidance. <code>context</code> <code>JsonValue</code> <code>None</code> Context data. <code>chat_model</code> <code>iModel</code> <code>None</code> Override chat model. <code>invoke_actions</code> <code>bool</code> <code>True</code> Automatically invoke tools from LLM response. <code>tools</code> <code>ToolRef</code> <code>None</code> Tools to make available. <code>response_format</code> <code>type[BaseModel]</code> <code>None</code> Pydantic model for the response. <code>actions</code> <code>bool</code> <code>False</code> Signal that function-calling is expected. <code>reason</code> <code>bool</code> <code>False</code> Request chain-of-thought reasoning. <code>action_strategy</code> <code>\"sequential\" \\| \"concurrent\"</code> <code>\"concurrent\"</code> Tool invocation strategy. <code>skip_validation</code> <code>bool</code> <code>False</code> Skip response parsing/validation. <code>handle_validation</code> <code>\"raise\" \\| \"return_value\" \\| \"return_none\"</code> <code>\"return_value\"</code> Behavior on parse failure. <p>Returns: <code>list | BaseModel | None | dict | str</code> -- parsed response, raw text, or <code>None</code> depending on validation settings.</p> <p>Example:</p> <pre><code>from pydantic import BaseModel\n\nclass AnalysisReport(BaseModel):\n    summary: str\n    key_findings: list[str]\n    confidence: float\n\nreport = await branch.operate(\n    instruction=\"Analyze the quarterly revenue data.\",\n    context={\"revenue\": [100, 120, 115, 140]},\n    response_format=AnalysisReport,\n    reason=True,\n)\nprint(report.summary)\n</code></pre>"},{"location":"reference/api/#parse","title":"parse","text":"<pre><code>async def parse(\n    self,\n    text: str,\n    handle_validation: Literal[\"raise\", \"return_value\", \"return_none\"] = \"return_value\",\n    max_retries: int = 3,\n    request_type: type[BaseModel] = None,\n    operative: Operative = None,\n    similarity_algo: str = \"jaro_winkler\",\n    similarity_threshold: float = 0.85,\n    fuzzy_match: bool = True,\n    handle_unmatched: Literal[\"ignore\", \"raise\", \"remove\", \"fill\", \"force\"] = \"force\",\n    fill_value: Any = None,\n    fill_mapping: dict[str, Any] | None = None,\n    strict: bool = False,\n    suppress_conversion_errors: bool = False,\n    response_format: type[BaseModel] = None,\n) -&gt; BaseModel | dict | str | None\n</code></pre> <p>Parses raw text into a structured Pydantic model using the parse model. Supports fuzzy key matching for malformed LLM output. Does not append messages to the conversation.</p> <p>Key Parameters:</p> Parameter Type Default Description <code>text</code> <code>str</code> (required) Raw text to parse. <code>request_type</code> <code>type[BaseModel]</code> <code>None</code> Target Pydantic model. <code>response_format</code> <code>type[BaseModel]</code> <code>None</code> Alias for <code>request_type</code>. <code>handle_validation</code> <code>\"raise\" \\| \"return_value\" \\| \"return_none\"</code> <code>\"return_value\"</code> Behavior on parse failure. <code>max_retries</code> <code>int</code> <code>3</code> Retry count for failed parses. <code>fuzzy_match</code> <code>bool</code> <code>True</code> Attempt fuzzy key matching. <code>similarity_threshold</code> <code>float</code> <code>0.85</code> Threshold for fuzzy matching (0.0-1.0). <code>handle_unmatched</code> <code>\"ignore\" \\| \"raise\" \\| \"remove\" \\| \"fill\" \\| \"force\"</code> <code>\"force\"</code> Policy for unrecognized fields. <code>strict</code> <code>bool</code> <code>False</code> Raise on ambiguous types/fields. <p>Returns: <code>BaseModel | dict | str | None</code></p>"},{"location":"reference/api/#react","title":"ReAct","text":"<pre><code>async def ReAct(\n    self,\n    instruct: Instruct | dict[str, Any],\n    interpret: bool = False,\n    interpret_domain: str | None = None,\n    interpret_style: str | None = None,\n    interpret_sample: str | None = None,\n    interpret_model: str | None = None,\n    interpret_kwargs: dict | None = None,\n    tools: Any = None,\n    tool_schemas: Any = None,\n    response_format: type[BaseModel] | BaseModel = None,\n    intermediate_response_options: list[BaseModel] | BaseModel = None,\n    intermediate_listable: bool = False,\n    reasoning_effort: Literal[\"low\", \"medium\", \"high\"] = None,\n    extension_allowed: bool = True,\n    max_extensions: int | None = 3,\n    response_kwargs: dict | None = None,\n    display_as: Literal[\"json\", \"yaml\"] = \"yaml\",\n    return_analysis: bool = False,\n    analysis_model: iModel | None = None,\n    verbose: bool = False,\n    verbose_length: int = None,\n    include_token_usage_to_model: bool = True,\n    **kwargs,\n) -&gt; Any | tuple[Any, list]\n</code></pre> <p>Multi-step Reason + Act loop. Iteratively generates chain-of-thought analysis, invokes tools, and optionally extends the reasoning for additional steps. Messages are automatically added to the branch.</p> <p>Key Parameters:</p> Parameter Type Default Description <code>instruct</code> <code>Instruct \\| dict</code> (required) Instruction with <code>instruction</code>, <code>guidance</code>, and <code>context</code> keys. <code>interpret</code> <code>bool</code> <code>False</code> Pre-process the instruction through <code>interpret()</code>. <code>interpret_domain</code> <code>str</code> <code>None</code> Domain hint for interpretation (e.g., <code>\"finance\"</code>). <code>interpret_style</code> <code>str</code> <code>None</code> Style hint (e.g., <code>\"concise\"</code>, <code>\"detailed\"</code>). <code>tools</code> <code>Any</code> <code>None</code> Tools to use. <code>None</code> defaults to all registered tools. <code>response_format</code> <code>type[BaseModel]</code> <code>None</code> Final output schema. <code>extension_allowed</code> <code>bool</code> <code>True</code> Allow multi-step expansion. <code>max_extensions</code> <code>int</code> <code>3</code> Max expansion steps (capped at 5). <code>return_analysis</code> <code>bool</code> <code>False</code> Also return intermediate analysis objects. <code>analysis_model</code> <code>iModel</code> <code>None</code> Override model for analysis steps. <code>reasoning_effort</code> <code>\"low\" \\| \"medium\" \\| \"high\"</code> <code>None</code> Reasoning depth hint. <code>verbose</code> <code>bool</code> <code>False</code> Log detailed analysis and action info. <p>Returns:</p> <ul> <li>If <code>return_analysis=False</code>: the final output (string, dict, or BaseModel).</li> <li>If <code>return_analysis=True</code>: <code>tuple[final_output, list[ReActAnalysis]]</code>.</li> </ul> <p>Example:</p> <pre><code>def search(query: str) -&gt; str:\n    \"\"\"Search the web for information.\"\"\"\n    return f\"Results for: {query}\"\n\nbranch = Branch(\n    system=\"You are a research assistant.\",\n    tools=[search],\n)\n\nresult = await branch.ReAct(\n    instruct={\"instruction\": \"Find the population of Tokyo\"},\n    max_extensions=2,\n)\n</code></pre>"},{"location":"reference/api/#reactstream","title":"ReActStream","text":"<pre><code>async def ReActStream(\n    self,\n    instruct: Instruct | dict[str, Any],\n    # ... same parameters as ReAct ...\n    **kwargs,\n) -&gt; AsyncGenerator\n</code></pre> <p>Streaming variant of <code>ReAct</code>. Yields intermediate <code>ReActAnalysis</code> objects as each reasoning step completes.</p> <p>Returns: <code>AsyncGenerator</code> yielding analysis results per step.</p> <p>Example:</p> <pre><code>async for step in branch.ReActStream(\n    instruct={\"instruction\": \"Research and compare Python web frameworks\"},\n    verbose=True,\n):\n    print(f\"Step completed: {step}\")\n</code></pre>"},{"location":"reference/api/#act","title":"act","text":"<pre><code>async def act(\n    self,\n    action_request: list | ActionRequest | BaseModel | dict,\n    *,\n    strategy: Literal[\"concurrent\", \"sequential\"] = \"concurrent\",\n    verbose_action: bool = False,\n    suppress_errors: bool = True,\n    call_params: AlcallParams = None,\n) -&gt; list[ActionResponse]\n</code></pre> <p>Directly invokes tool actions without an LLM call. Useful for executing tool calls programmatically.</p> <p>Parameters:</p> Parameter Type Default Description <code>action_request</code> <code>list \\| ActionRequest \\| BaseModel \\| dict</code> (required) Tool call request(s) to execute. <code>strategy</code> <code>\"concurrent\" \\| \"sequential\"</code> <code>\"concurrent\"</code> Execution strategy. <code>verbose_action</code> <code>bool</code> <code>False</code> Log action details. <code>suppress_errors</code> <code>bool</code> <code>True</code> Suppress exceptions from tool calls. <p>Returns: <code>list[ActionResponse]</code></p>"},{"location":"reference/api/#interpret","title":"interpret","text":"<pre><code>async def interpret(\n    self,\n    text: str,\n    domain: str | None = None,\n    style: str | None = None,\n    interpret_model = None,\n    **kwargs,\n) -&gt; str\n</code></pre> <p>Rewrites raw user input into a clearer, more structured LLM prompt. Does not add messages to the conversation.</p> <p>Parameters:</p> Parameter Type Default Description <code>text</code> <code>str</code> (required) Raw user input to rewrite. <code>domain</code> <code>str</code> <code>None</code> Domain hint (e.g., <code>\"finance\"</code>, <code>\"devops\"</code>). <code>style</code> <code>str</code> <code>None</code> Style hint (e.g., <code>\"concise\"</code>, <code>\"detailed\"</code>). <p>Returns: <code>str</code> -- the refined prompt.</p> <p>Example:</p> <pre><code>refined = await branch.interpret(\n    \"how do i do marketing stuff\",\n    domain=\"marketing\",\n    style=\"detailed\",\n)\n# =&gt; \"Explain step-by-step how to set up a marketing analytics pipeline...\"\n</code></pre>"},{"location":"reference/api/#register_tools","title":"register_tools","text":"<pre><code>def register_tools(\n    self,\n    tools: FuncTool | list[FuncTool] | LionTool,\n    update: bool = False,\n) -&gt; None\n</code></pre> <p>Registers one or more tools (functions or <code>Tool</code> objects) in the branch.</p> <p>Parameters:</p> Parameter Type Default Description <code>tools</code> <code>FuncTool \\| list[FuncTool] \\| LionTool</code> (required) Tool(s) to register. Can be plain functions, <code>Tool</code> instances, or <code>LionTool</code> subclasses. <code>update</code> <code>bool</code> <code>False</code> Overwrite existing tools with the same name. <p>Example:</p> <pre><code>def multiply(x: float, y: float) -&gt; float:\n    \"\"\"Multiply two numbers.\"\"\"\n    return x * y\n\nbranch.register_tools([multiply])\n</code></pre>"},{"location":"reference/api/#get_operation","title":"get_operation","text":"<pre><code>def get_operation(self, operation: str) -&gt; Callable | None\n</code></pre> <p>Looks up an operation by name. First checks for a method on the Branch, then falls back to the operation registry.</p> <p>Parameters:</p> <ul> <li><code>operation</code> (<code>str</code>): Operation name (e.g., <code>\"chat\"</code>, <code>\"communicate\"</code>).</li> </ul> <p>Returns: <code>Callable | None</code></p>"},{"location":"reference/api/#clone","title":"clone","text":"<pre><code>def clone(self, sender: ID.Ref = None) -&gt; Branch\n</code></pre> <p>Creates a synchronous copy of the branch, including messages, system config, tools, and models. API-backed models are shared; CLI-backed models get fresh copies.</p> <p>Parameters:</p> <ul> <li><code>sender</code> (<code>ID.Ref</code>, optional): New sender ID for all cloned messages.</li> </ul> <p>Returns: <code>Branch</code> -- a new branch instance.</p>"},{"location":"reference/api/#aclone","title":"aclone","text":"<pre><code>async def aclone(self, sender: ID.Ref = None) -&gt; Branch\n</code></pre> <p>Async variant of <code>clone</code>. Acquires the message lock before cloning.</p> <p>Returns: <code>Branch</code></p>"},{"location":"reference/api/#to_dict","title":"to_dict","text":"<pre><code>def to_dict(self) -&gt; dict\n</code></pre> <p>Serializes the branch to a dictionary including messages, logs, models, system message, log config, and metadata.</p> <p>Returns: <code>dict</code></p>"},{"location":"reference/api/#from_dict","title":"from_dict","text":"<pre><code>@classmethod\ndef from_dict(cls, data: dict) -&gt; Branch\n</code></pre> <p>Deserializes a <code>Branch</code> from a dictionary produced by <code>to_dict</code>.</p> <p>Parameters:</p> <ul> <li><code>data</code> (<code>dict</code>): Serialized branch data.</li> </ul> <p>Returns: <code>Branch</code></p>"},{"location":"reference/api/#to_df","title":"to_df","text":"<pre><code>def to_df(self, *, progression: Progression = None) -&gt; pd.DataFrame\n</code></pre> <p>Converts branch messages to a pandas DataFrame.</p> <p>Parameters:</p> <ul> <li><code>progression</code> (<code>Progression</code>, optional): Custom message ordering.</li> </ul> <p>Returns: <code>pd.DataFrame</code></p>"},{"location":"reference/api/#dump_logs-adump_logs","title":"dump_logs / adump_logs","text":"<pre><code>def dump_logs(self, clear: bool = True, persist_path = None) -&gt; None\nasync def adump_logs(self, clear: bool = True, persist_path = None) -&gt; None\n</code></pre> <p>Writes logs to disk. If <code>clear=True</code> (default), clears the in-memory log after writing.</p>"},{"location":"reference/api/#session","title":"Session","text":"<p>Manages multiple <code>Branch</code> instances and executes graph-based workflows. Creates a default branch automatically on initialization.</p>"},{"location":"reference/api/#constructor_1","title":"Constructor","text":"<pre><code>Session(\n    branches: Pile[Branch] = ...,\n    default_branch: Branch | None = None,\n    name: str = \"Session\",\n    user: SenderRecipient | None = None,\n)\n</code></pre> Parameter Type Default Description <code>branches</code> <code>Pile[Branch]</code> auto Collection of branches. A default empty <code>Pile</code> is created. <code>default_branch</code> <code>Branch \\| None</code> <code>None</code> Primary branch. Created automatically if <code>None</code>. <code>name</code> <code>str</code> <code>\"Session\"</code> Session name. <code>user</code> <code>SenderRecipient \\| None</code> <code>None</code> User/owner of the session. Propagated to branches. <p>Example:</p> <pre><code>from lionagi import Session, iModel\n\nsession = Session()\n# session.default_branch is ready to use\nresult = await session.default_branch.communicate(\"Hello!\")\n</code></pre>"},{"location":"reference/api/#properties_1","title":"Properties","text":"Property Type Description <code>branches</code> <code>Pile[Branch]</code> All branches in the session. <code>default_branch</code> <code>Branch</code> The active default branch. <code>name</code> <code>str</code> Session name. <code>user</code> <code>SenderRecipient \\| None</code> Session owner."},{"location":"reference/api/#methods_1","title":"Methods","text":""},{"location":"reference/api/#flow","title":"flow","text":"<pre><code>async def flow(\n    self,\n    graph: Graph,\n    *,\n    context: dict[str, Any] | None = None,\n    parallel: bool = True,\n    max_concurrent: int = 5,\n    verbose: bool = False,\n    default_branch: Branch | ID.Ref | None = None,\n    alcall_params: Any = None,\n) -&gt; dict[str, Any]\n</code></pre> <p>Executes a graph-based workflow using multi-branch orchestration. Independent operations run in parallel; dependent operations run sequentially.</p> <p>Parameters:</p> Parameter Type Default Description <code>graph</code> <code>Graph</code> (required) Workflow graph containing <code>Operation</code> nodes. <code>context</code> <code>dict</code> <code>None</code> Initial context passed to operations. <code>parallel</code> <code>bool</code> <code>True</code> Run independent operations concurrently. <code>max_concurrent</code> <code>int</code> <code>5</code> Max concurrent branches. <code>verbose</code> <code>bool</code> <code>False</code> Enable verbose logging. <code>default_branch</code> <code>Branch \\| ID.Ref</code> <code>None</code> Branch override (defaults to <code>self.default_branch</code>). <p>Returns: <code>dict[str, Any]</code> -- execution results with completed operations and final context.</p> <p>Example:</p> <pre><code>from lionagi import Session, Builder\n\nsession = Session()\nbuilder = Builder()\n\nstep1 = builder.add_operation(\"communicate\", instruction=\"Brainstorm ideas\")\nstep2 = builder.add_operation(\n    \"communicate\",\n    instruction=\"Evaluate the ideas\",\n    depends_on=[step1],\n    inherit_context=True,\n)\n\nresult = await session.flow(builder.get_graph())\n</code></pre>"},{"location":"reference/api/#new_branch","title":"new_branch","text":"<pre><code>def new_branch(\n    self,\n    system: System | JsonValue = None,\n    system_sender: SenderRecipient = None,\n    system_datetime: bool | str = None,\n    user: SenderRecipient = None,\n    name: str | None = None,\n    imodel: iModel | None = None,\n    messages: Pile[RoledMessage] = None,\n    progress: Progression = None,\n    tool_manager: ActionManager = None,\n    tools: Tool | Callable | list = None,\n    as_default_branch: bool = False,\n    **kwargs,\n) -&gt; Branch\n</code></pre> <p>Creates a new branch and includes it in the session.</p> <p>Parameters:</p> Parameter Type Default Description <code>system</code> <code>System \\| JsonValue</code> <code>None</code> System message for the new branch. <code>name</code> <code>str</code> <code>None</code> Branch name. <code>imodel</code> <code>iModel</code> <code>None</code> Model for the branch. <code>tools</code> <code>Tool \\| Callable \\| list</code> <code>None</code> Tools to register. <code>as_default_branch</code> <code>bool</code> <code>False</code> Make this the session's default branch. <p>Returns: <code>Branch</code></p>"},{"location":"reference/api/#get_branch","title":"get_branch","text":"<pre><code>def get_branch(self, branch: ID.Ref | str, default: Any = ..., /) -&gt; Branch\n</code></pre> <p>Retrieves a branch by UUID or name. Raises <code>ItemNotFoundError</code> if not found and no default is given.</p>"},{"location":"reference/api/#split-asplit","title":"split / asplit","text":"<pre><code>def split(self, branch: ID.Ref) -&gt; Branch\nasync def asplit(self, branch: ID.Ref) -&gt; Branch\n</code></pre> <p>Clones a branch and includes the clone in the session.</p> <p>Returns: <code>Branch</code> -- the newly created clone.</p>"},{"location":"reference/api/#remove_branch","title":"remove_branch","text":"<pre><code>def remove_branch(self, branch: ID.Ref, delete: bool = False) -&gt; None\n</code></pre> <p>Removes a branch from the session. If the removed branch was the default, the first remaining branch becomes the new default.</p>"},{"location":"reference/api/#change_default_branch","title":"change_default_branch","text":"<pre><code>def change_default_branch(self, branch: ID.Ref) -&gt; None\n</code></pre> <p>Sets a different branch as the session default.</p>"},{"location":"reference/api/#register_operation","title":"register_operation","text":"<pre><code>def register_operation(\n    self, operation: str, func: Callable, *, update: bool = False\n) -&gt; None\n</code></pre> <p>Registers a custom operation callable, accessible from all branches in the session.</p>"},{"location":"reference/api/#operation-decorator","title":"operation (decorator)","text":"<pre><code>@session.operation(name=None, update=False)\nasync def my_custom_op():\n    ...\n</code></pre> <p>Decorator to register a function as a named operation on the session.</p>"},{"location":"reference/api/#concat_messages","title":"concat_messages","text":"<pre><code>def concat_messages(\n    self,\n    branches: ID.RefSeq = None,\n    exclude_clone: bool = False,\n    exclude_load: bool = False,\n) -&gt; Pile[RoledMessage]\n</code></pre> <p>Concatenates messages from multiple branches into a single <code>Pile</code>.</p>"},{"location":"reference/api/#to_df_1","title":"to_df","text":"<pre><code>def to_df(\n    self,\n    branches: ID.RefSeq = None,\n    exclude_clone: bool = False,\n    exlcude_load: bool = False,\n) -&gt; pd.DataFrame\n</code></pre> <p>Converts session messages across branches into a DataFrame.</p>"},{"location":"reference/api/#imodel","title":"iModel","text":"<p>Unified provider interface for LLM API calls with rate limiting, retry logic, and hook support. Supports OpenAI, Anthropic, Gemini, Ollama, NVIDIA NIM, Perplexity, Groq, OpenRouter, and Claude Code CLI.</p>"},{"location":"reference/api/#constructor_2","title":"Constructor","text":"<pre><code>iModel(\n    provider: str = None,\n    base_url: str = None,\n    endpoint: str | Endpoint = \"chat\",\n    api_key: str = None,\n    queue_capacity: int | None = None,\n    capacity_refresh_time: float = 60,\n    interval: float | None = None,\n    limit_requests: int = None,\n    limit_tokens: int = None,\n    concurrency_limit: int | None = None,\n    streaming_process_func: Callable = None,\n    provider_metadata: dict | None = None,\n    hook_registry: HookRegistry | dict | None = None,\n    exit_hook: bool = False,\n    id: UUID | str = None,\n    created_at: float | None = None,\n    **kwargs,\n)\n</code></pre> Parameter Type Default Description <code>provider</code> <code>str</code> <code>None</code> Provider name: <code>\"openai\"</code>, <code>\"anthropic\"</code>, <code>\"gemini\"</code>, <code>\"ollama\"</code>, <code>\"claude_code\"</code>, etc. Can also be inferred from <code>model</code> if given as <code>\"provider/model\"</code>. <code>base_url</code> <code>str</code> <code>None</code> Custom API base URL. <code>endpoint</code> <code>str \\| Endpoint</code> <code>\"chat\"</code> Endpoint type or a pre-built <code>Endpoint</code> instance. <code>api_key</code> <code>str</code> <code>None</code> API key. Falls back to environment variables. <code>queue_capacity</code> <code>int</code> <code>None</code> Max queued requests before execution. <code>capacity_refresh_time</code> <code>float</code> <code>60</code> Seconds between queue capacity resets. <code>interval</code> <code>float</code> <code>None</code> Request processing interval. Defaults to <code>capacity_refresh_time</code>. <code>limit_requests</code> <code>int</code> <code>None</code> Max requests per cycle. <code>limit_tokens</code> <code>int</code> <code>None</code> Max tokens per cycle. <code>concurrency_limit</code> <code>int</code> <code>None</code> Max concurrent streaming requests (CLI only). <code>streaming_process_func</code> <code>Callable</code> <code>None</code> Custom function to process streaming chunks. <code>provider_metadata</code> <code>dict</code> <code>None</code> Provider-specific metadata (e.g., session IDs). <code>hook_registry</code> <code>HookRegistry \\| dict</code> <code>None</code> Pre/post-invocation hooks. <code>exit_hook</code> <code>bool</code> <code>False</code> Enable exit hooks on invocation. <code>**kwargs</code> Provider-specific parameters (e.g., <code>model</code>, <code>temperature</code>, <code>max_tokens</code>). <p>Example:</p> <pre><code>from lionagi import iModel\n\n# OpenAI\nmodel = iModel(provider=\"openai\", model=\"gpt-4.1-mini\", temperature=0.7)\n\n# Anthropic\nmodel = iModel(provider=\"anthropic\", model=\"claude-sonnet-4-20250514\")\n\n# Shorthand: provider/model\nmodel = iModel(model=\"openai/gpt-4.1-mini\")\n\n# Ollama (local)\nmodel = iModel(provider=\"ollama\", model=\"llama3\")\n</code></pre>"},{"location":"reference/api/#async-context-manager_1","title":"Async Context Manager","text":"<pre><code>async with iModel(provider=\"openai\", model=\"gpt-4.1-mini\") as model:\n    result = await model.invoke(messages=[...])\n# executor is stopped and resources released on exit\n</code></pre>"},{"location":"reference/api/#properties_2","title":"Properties","text":"Property Type Description <code>id</code> <code>UUID</code> Unique model instance identifier. <code>created_at</code> <code>float</code> Creation timestamp. <code>endpoint</code> <code>Endpoint</code> The configured endpoint. <code>executor</code> <code>RateLimitedAPIExecutor</code> Rate-limited request executor. <code>is_cli</code> <code>bool</code> Whether this model uses a CLI endpoint. <code>model_name</code> <code>str</code> The model name string (e.g., <code>\"gpt-4.1-mini\"</code>). <code>request_options</code> <code>type[BaseModel] \\| None</code> Request schema for the endpoint. <code>hook_registry</code> <code>HookRegistry</code> Registered hooks."},{"location":"reference/api/#methods_2","title":"Methods","text":""},{"location":"reference/api/#invoke","title":"invoke","text":"<pre><code>async def invoke(self, api_call: APICalling = None, **kw) -&gt; APICalling\n</code></pre> <p>Performs a rate-limited API call. Starts the executor if needed, enqueues the request, and waits for completion.</p> <p>Parameters:</p> <ul> <li><code>api_call</code> (<code>APICalling</code>, optional): Pre-built API call. If <code>None</code>, one is   created from <code>**kw</code>.</li> <li><code>**kw</code>: Request parameters (e.g., <code>messages</code>, <code>temperature</code>).</li> </ul> <p>Returns: <code>APICalling</code> -- the completed call with <code>.response</code> populated.</p> <p>Raises: <code>ValueError</code> on invocation failure.</p>"},{"location":"reference/api/#stream","title":"stream","text":"<pre><code>async def stream(self, api_call=None, **kw) -&gt; AsyncGenerator\n</code></pre> <p>Performs a streaming API call, yielding chunks as they arrive.</p> <p>Parameters:</p> <ul> <li><code>api_call</code>: Pre-built API call. If <code>None</code>, one is created with <code>stream=True</code>.</li> <li><code>**kw</code>: Request parameters.</li> </ul> <p>Yields: Processed chunks (via <code>streaming_process_func</code> if set) or raw chunks. The final yield is the completed <code>APICalling</code> object.</p>"},{"location":"reference/api/#create_api_calling","title":"create_api_calling","text":"<pre><code>def create_api_calling(\n    self,\n    include_token_usage_to_model: bool = False,\n    **kwargs,\n) -&gt; APICalling\n</code></pre> <p>Builds an <code>APICalling</code> event from keyword arguments using the configured endpoint.</p> <p>Returns: <code>APICalling</code></p>"},{"location":"reference/api/#create_event","title":"create_event","text":"<pre><code>async def create_event(\n    self,\n    create_event_type: type[Event] = APICalling,\n    create_event_exit_hook: bool = None,\n    create_event_hook_timeout: float = 10.0,\n    create_event_hook_params: dict = None,\n    pre_invoke_event_exit_hook: bool = None,\n    pre_invoke_event_hook_timeout: float = 30.0,\n    pre_invoke_event_hook_params: dict = None,\n    post_invoke_event_exit_hook: bool = None,\n    post_invoke_event_hook_timeout: float = 30.0,\n    post_invoke_event_hook_params: dict = None,\n    **kwargs,\n) -&gt; APICalling\n</code></pre> <p>Creates an API call event with optional pre/post-invocation hooks. Used internally by <code>invoke()</code> and <code>stream()</code>.</p>"},{"location":"reference/api/#process_chunk","title":"process_chunk","text":"<pre><code>async def process_chunk(self, chunk) -&gt; Any\n</code></pre> <p>Processes a single streaming chunk. Override this or provide <code>streaming_process_func</code> for custom chunk handling.</p>"},{"location":"reference/api/#copy","title":"copy","text":"<pre><code>def copy(self, share_session: bool = False) -&gt; iModel\n</code></pre> <p>Creates a new <code>iModel</code> with the same configuration but a fresh ID and executor.</p> <p>Parameters:</p> <ul> <li><code>share_session</code> (<code>bool</code>): If <code>True</code>, carries over CLI session state.</li> </ul> <p>Returns: <code>iModel</code></p>"},{"location":"reference/api/#close","title":"close","text":"<pre><code>async def close(self) -&gt; None\n</code></pre> <p>Stops the executor and releases resources.</p>"},{"location":"reference/api/#to_dict-from_dict","title":"to_dict / from_dict","text":"<pre><code>def to_dict(self) -&gt; dict\n@classmethod\ndef from_dict(cls, data: dict) -&gt; iModel\n</code></pre> <p>Serialization and deserialization. The dict includes <code>id</code>, <code>created_at</code>, <code>endpoint</code> config, <code>processor_config</code>, and <code>provider_metadata</code>.</p>"},{"location":"reference/api/#builder","title":"Builder","text":"<p><code>OperationGraphBuilder</code> (aliased as <code>Builder</code>) constructs directed acyclic graphs of operations for execution by <code>Session.flow()</code>. Supports incremental build-execute-expand cycles.</p>"},{"location":"reference/api/#constructor_3","title":"Constructor","text":"<pre><code>Builder(name: str = \"DynamicGraph\")\n</code></pre> Parameter Type Default Description <code>name</code> <code>str</code> <code>\"DynamicGraph\"</code> Name for the graph."},{"location":"reference/api/#attributes","title":"Attributes","text":"Attribute Type Description <code>graph</code> <code>Graph</code> The underlying directed graph. <code>last_operation_id</code> <code>str \\| None</code> ID of the most recently added operation."},{"location":"reference/api/#methods_3","title":"Methods","text":""},{"location":"reference/api/#add_operation","title":"add_operation","text":"<pre><code>def add_operation(\n    self,\n    operation: str,\n    node_id: str | None = None,\n    depends_on: list[str] | None = None,\n    inherit_context: bool = False,\n    branch = None,\n    **parameters,\n) -&gt; str\n</code></pre> <p>Adds an operation node to the graph. If no <code>depends_on</code> is specified, the node is linked sequentially from the current head nodes.</p> <p>Parameters:</p> Parameter Type Default Description <code>operation</code> <code>str</code> (required) Branch operation name: <code>\"chat\"</code>, <code>\"communicate\"</code>, <code>\"operate\"</code>, <code>\"ReAct\"</code>, etc. <code>node_id</code> <code>str</code> <code>None</code> Reference ID for later lookup. <code>depends_on</code> <code>list[str]</code> <code>None</code> Node IDs this operation depends on. <code>inherit_context</code> <code>bool</code> <code>False</code> Inherit conversation context from the primary dependency. <code>branch</code> <code>Branch \\| ID.Ref</code> <code>None</code> Specific branch to run on. <code>**parameters</code> Operation parameters (e.g., <code>instruction</code>, <code>response_format</code>). <p>Returns: <code>str</code> -- UUID of the created operation node.</p> <p>Example:</p> <pre><code>from lionagi import Builder\n\nbuilder = Builder()\nstep1 = builder.add_operation(\"communicate\", instruction=\"List 5 ideas\")\nstep2 = builder.add_operation(\n    \"communicate\",\n    instruction=\"Evaluate these ideas\",\n    depends_on=[step1],\n    inherit_context=True,\n)\n</code></pre>"},{"location":"reference/api/#add_aggregation","title":"add_aggregation","text":"<pre><code>def add_aggregation(\n    self,\n    operation: str,\n    node_id: str | None = None,\n    source_node_ids: list[str] | None = None,\n    inherit_context: bool = False,\n    inherit_from_source: int = 0,\n    branch = None,\n    **parameters,\n) -&gt; str\n</code></pre> <p>Adds a node that aggregates results from multiple source nodes (defaults to current head nodes).</p> <p>Parameters:</p> Parameter Type Default Description <code>operation</code> <code>str</code> (required) The aggregation operation. <code>source_node_ids</code> <code>list[str]</code> <code>None</code> Nodes to aggregate from. Defaults to current heads. <code>inherit_context</code> <code>bool</code> <code>False</code> Inherit context from one source. <code>inherit_from_source</code> <code>int</code> <code>0</code> Index of the source to inherit from. <p>Returns: <code>str</code> -- UUID of the aggregation node.</p> <p>Example:</p> <pre><code># Fan-out then aggregate\nnodes = []\nfor topic in [\"AI\", \"ML\", \"NLP\"]:\n    nodes.append(builder.add_operation(\"communicate\", instruction=f\"Research {topic}\"))\n\nsynthesis = builder.add_aggregation(\n    \"communicate\",\n    source_node_ids=nodes,\n    instruction=\"Synthesize findings\",\n)\n</code></pre>"},{"location":"reference/api/#expand_from_result","title":"expand_from_result","text":"<pre><code>def expand_from_result(\n    self,\n    items: list[Any],\n    source_node_id: str,\n    operation: str,\n    strategy: ExpansionStrategy = ExpansionStrategy.CONCURRENT,\n    inherit_context: bool = False,\n    chain_context: bool = False,\n    **shared_params,\n) -&gt; list[str]\n</code></pre> <p>Expands the graph based on execution results. Creates new operation nodes for each item in <code>items</code>, linked from the source node.</p> <p>Parameters:</p> Parameter Type Default Description <code>items</code> <code>list[Any]</code> (required) Items to expand (e.g., from a previous result). <code>source_node_id</code> <code>str</code> (required) Node that produced the items. <code>operation</code> <code>str</code> (required) Operation to apply to each item. <code>strategy</code> <code>ExpansionStrategy</code> <code>CONCURRENT</code> <code>CONCURRENT</code> or <code>SEQUENTIAL</code>. <code>inherit_context</code> <code>bool</code> <code>False</code> Inherit context from source. <code>chain_context</code> <code>bool</code> <code>False</code> Chain context between sequential expansions. <p>Returns: <code>list[str]</code> -- IDs of the new nodes.</p>"},{"location":"reference/api/#add_conditional_branch","title":"add_conditional_branch","text":"<pre><code>def add_conditional_branch(\n    self,\n    condition_check_op: str,\n    true_op: str,\n    false_op: str | None = None,\n    **check_params,\n) -&gt; dict[str, str]\n</code></pre> <p>Adds a conditional branching structure: a check node with <code>true</code> and optional <code>false</code> paths.</p> <p>Returns: <code>dict</code> with keys <code>\"check\"</code>, <code>\"true\"</code>, and optionally <code>\"false\"</code>, each mapping to a node ID.</p>"},{"location":"reference/api/#get_graph","title":"get_graph","text":"<pre><code>def get_graph(self) -&gt; Graph\n</code></pre> <p>Returns the current graph for execution by <code>Session.flow()</code>.</p> <p>Returns: <code>Graph</code></p>"},{"location":"reference/api/#mark_executed","title":"mark_executed","text":"<pre><code>def mark_executed(self, node_ids: list[str]) -&gt; None\n</code></pre> <p>Marks nodes as executed, useful for incremental build-execute cycles.</p>"},{"location":"reference/api/#get_unexecuted_nodes","title":"get_unexecuted_nodes","text":"<pre><code>def get_unexecuted_nodes(self) -&gt; list[Operation]\n</code></pre> <p>Returns all operations not yet marked as executed.</p>"},{"location":"reference/api/#get_node_by_reference","title":"get_node_by_reference","text":"<pre><code>def get_node_by_reference(self, reference_id: str) -&gt; Operation | None\n</code></pre> <p>Looks up a node by its <code>node_id</code> reference (set via <code>add_operation</code>).</p>"},{"location":"reference/api/#visualize_state","title":"visualize_state","text":"<pre><code>def visualize_state(self) -&gt; dict[str, Any]\n</code></pre> <p>Returns a summary dict with <code>total_nodes</code>, <code>executed_nodes</code>, <code>unexecuted_nodes</code>, <code>current_heads</code>, <code>expansions</code>, and <code>edges</code>.</p>"},{"location":"reference/api/#visualize","title":"visualize","text":"<pre><code>def visualize(self, title: str = \"Operation Graph\", figsize=(14, 10)) -&gt; None\n</code></pre> <p>Renders the graph visually using matplotlib.</p>"},{"location":"reference/api/#operation","title":"Operation","text":"<p>A graph node representing a single branch operation. Extends both <code>Node</code> and <code>Event</code>, tracking execution status, timing, and results.</p>"},{"location":"reference/api/#constructor_4","title":"Constructor","text":"<pre><code>Operation(\n    operation: str,\n    parameters: dict[str, Any] | BaseModel = {},\n)\n</code></pre> Parameter Type Default Description <code>operation</code> <code>str</code> (required) Operation name: <code>\"chat\"</code>, <code>\"operate\"</code>, <code>\"communicate\"</code>, <code>\"parse\"</code>, <code>\"ReAct\"</code>, <code>\"interpret\"</code>, <code>\"act\"</code>, <code>\"ReActStream\"</code>, or a custom name. <code>parameters</code> <code>dict \\| BaseModel</code> <code>{}</code> Parameters passed to the operation."},{"location":"reference/api/#properties_3","title":"Properties","text":"Property Type Description <code>branch_id</code> <code>UUID \\| None</code> ID of the branch that executed this operation. <code>graph_id</code> <code>UUID \\| None</code> ID of the graph containing this operation. <code>request</code> <code>dict</code> Parameters as a dictionary. <code>response</code> <code>Any</code> Execution result (populated after <code>invoke</code>)."},{"location":"reference/api/#methods_4","title":"Methods","text":""},{"location":"reference/api/#invoke_1","title":"invoke","text":"<pre><code>async def invoke(self, branch: Branch) -&gt; None\n</code></pre> <p>Executes the operation on the given branch. Updates <code>execution.status</code>, <code>execution.response</code>, <code>execution.duration</code>, and <code>execution.error</code>.</p> <p>Parameters:</p> <ul> <li><code>branch</code> (<code>Branch</code>): The branch to execute on.</li> </ul>"},{"location":"reference/api/#factory-function","title":"Factory Function","text":"<pre><code>from lionagi.operations.node import create_operation\n\nop = create_operation(\n    operation=\"communicate\",\n    parameters={\"instruction\": \"Hello\"},\n)\n</code></pre>"},{"location":"reference/api/#pile","title":"Pile","text":"<p>Thread-safe, async-compatible ordered collection keyed by UUID. Items are accessed by UUID (not by list index). Supports JSON, CSV, and DataFrame adapters.</p>"},{"location":"reference/api/#constructor_5","title":"Constructor","text":"<pre><code>Pile(\n    collections: list[Element] = None,\n    item_type: set[type] = None,\n    order: list[UUID] = None,\n    strict_type: bool = False,\n)\n</code></pre> Parameter Type Default Description <code>collections</code> <code>list[Element]</code> <code>None</code> Initial items. <code>item_type</code> <code>set[type]</code> <code>None</code> Allowed element types. <code>order</code> <code>list[UUID]</code> <code>None</code> Explicit ordering. <code>strict_type</code> <code>bool</code> <code>False</code> Disallow subtypes if <code>True</code>."},{"location":"reference/api/#key-interface","title":"Key Interface","text":"Method/Operator Description <code>pile[uuid]</code> Get item by UUID. <code>pile[0]</code>, <code>pile[1:3]</code> Get by index or slice. <code>pile.include(item)</code> Add item if not present. <code>pile.exclude(item)</code> Remove item if present. <code>pile.pop(key)</code> Remove and return item. <code>pile.get(key, default)</code> Get item with fallback. <code>pile.update(items)</code> Update/add items. <code>pile.insert(index, item)</code> Insert at position. <code>pile.clear()</code> Remove all items. <code>pile.keys()</code> All UUIDs in order. <code>pile.values()</code> All items in order. <code>pile.items()</code> <code>(UUID, item)</code> pairs in order. <code>len(pile)</code> Number of items. <code>item in pile</code> Membership test. <code>pile.is_empty()</code> Check emptiness. <code>pile.to_df(columns=...)</code> Convert to pandas DataFrame. <code>pile.dump(path, \"json\")</code> Export to JSON, CSV, or Parquet. <code>pile \\| other</code> Union. <code>pile &amp; other</code> Intersection. <code>pile ^ other</code> Symmetric difference. <p>All mutating methods have async variants prefixed with <code>a</code> (e.g., <code>apop</code>, <code>ainclude</code>, <code>aexclude</code>, <code>aclear</code>, <code>aupdate</code>, <code>aget</code>).</p> <p>Pile also works as an async context manager for locked access:</p> <pre><code>async with pile:\n    pile.include(item)\n</code></pre>"},{"location":"reference/api/#progression","title":"Progression","text":"<p>Ordered sequence of UUIDs, decoupled from storage. Multiple progressions can index the same <code>Pile</code> without copying data.</p>"},{"location":"reference/api/#constructor_6","title":"Constructor","text":"<pre><code>Progression(\n    order: list[UUID] = [],\n    name: str | None = None,\n)\n</code></pre>"},{"location":"reference/api/#key-interface_1","title":"Key Interface","text":"Method/Operator Description <code>prog[0]</code>, <code>prog[1:3]</code> Index/slice access. <code>prog.append(item)</code> Append UUID(s). <code>prog.insert(index, item)</code> Insert at position. <code>prog.remove(item)</code> Remove first occurrence. <code>prog.pop(index=-1)</code> Remove and return by index. <code>prog.popleft()</code> Remove and return from front. <code>prog.include(item)</code> Add if not present. <code>prog.exclude(item)</code> Remove if present. <code>prog.clear()</code> Remove all. <code>prog.index(item)</code> Find position. <code>prog.count(item)</code> Count occurrences. <code>prog.extend(other)</code> Extend from another Progression. <code>len(prog)</code> Length. <code>item in prog</code> Membership test. <code>prog + other</code> Concatenation (new Progression). <code>prog - other</code> Difference (new Progression)."},{"location":"reference/api/#supporting-types","title":"Supporting Types","text":"<p>These types are used throughout the API but do not typically need to be instantiated directly.</p>"},{"location":"reference/api/#instruct","title":"Instruct","text":"<p>Instruction bundle used by <code>operate</code> and <code>ReAct</code>:</p> <pre><code>from lionagi.operations.fields import Instruct\n\ninstruct = Instruct(\n    instruction=\"Analyze the data\",\n    guidance=\"Focus on trends\",\n    context={\"data\": [1, 2, 3]},\n    reason=True,\n)\n</code></pre>"},{"location":"reference/api/#element","title":"Element","text":"<p>Base class for all identifiable objects. Provides <code>id</code> (UUID), <code>created_at</code> (float), and <code>metadata</code> (dict).</p> <pre><code>from lionagi import Element\n\nel = Element()\nprint(el.id)          # UUID\nprint(el.created_at)  # float timestamp\n</code></pre>"},{"location":"reference/api/#node","title":"Node","text":"<p><code>Element</code> with arbitrary <code>content</code> and optional embedding vector. Used as the base for graph nodes.</p>"},{"location":"reference/api/#graph","title":"Graph","text":"<p>Directed graph of <code>Node</code> and <code>Edge</code> objects with adjacency tracking.</p>"},{"location":"reference/api/#expansionstrategy","title":"ExpansionStrategy","text":"<p>Enum for <code>Builder.expand_from_result</code>:</p> <pre><code>from lionagi.operations.builder import ExpansionStrategy\n\nExpansionStrategy.CONCURRENT             # Run expanded ops in parallel\nExpansionStrategy.SEQUENTIAL             # Run expanded ops in sequence\nExpansionStrategy.SEQUENTIAL_CONCURRENT_CHUNK\nExpansionStrategy.CONCURRENT_SEQUENTIAL_CHUNK\n</code></pre>"},{"location":"reference/api/#configuration","title":"Configuration","text":""},{"location":"reference/api/#environment-variables","title":"Environment Variables","text":"<pre><code>export OPENAI_API_KEY=\"sk-...\"\nexport ANTHROPIC_API_KEY=\"sk-ant-...\"\nexport GEMINI_API_KEY=\"...\"\nexport OLLAMA_BASE_URL=\"http://localhost:11434\"  # default\n</code></pre>"},{"location":"reference/api/#appsettings","title":"AppSettings","text":"<p>Default model settings are controlled through <code>lionagi.config.settings</code>:</p> <ul> <li><code>LIONAGI_CHAT_PROVIDER</code> -- default: <code>\"openai\"</code></li> <li><code>LIONAGI_CHAT_MODEL</code> -- default: <code>\"gpt-4.1-mini\"</code></li> </ul> <p>Override by setting environment variables or passing <code>chat_model</code> to <code>Branch</code>.</p>"},{"location":"reference/api/#quick-reference","title":"Quick Reference","text":""},{"location":"reference/api/#common-patterns","title":"Common Patterns","text":"<p>Simple conversation:</p> <pre><code>from lionagi import Branch\n\nbranch = Branch(system=\"You are helpful.\")\nanswer = await branch.communicate(\"What is 2 + 2?\")\n</code></pre> <p>Structured output:</p> <pre><code>from pydantic import BaseModel\nfrom lionagi import Branch\n\nclass Answer(BaseModel):\n    value: int\n    explanation: str\n\nresult = await branch.communicate(\n    \"What is 2 + 2?\",\n    response_format=Answer,\n)\n</code></pre> <p>Tool use with ReAct:</p> <pre><code>def calculator(expression: str) -&gt; float:\n    \"\"\"Evaluate a math expression.\"\"\"\n    return eval(expression)\n\nbranch = Branch(system=\"You solve math problems.\", tools=[calculator])\nresult = await branch.ReAct({\"instruction\": \"What is 15% of 847?\"})\n</code></pre> <p>Graph workflow:</p> <pre><code>from lionagi import Session, Builder\n\nsession = Session()\nbuilder = Builder()\n\n# Parallel research\nids = [\n    builder.add_operation(\"communicate\", instruction=f\"Research {t}\")\n    for t in [\"AI safety\", \"AI alignment\", \"AI governance\"]\n]\n\n# Aggregate\nbuilder.add_aggregation(\n    \"communicate\",\n    source_node_ids=ids,\n    instruction=\"Synthesize a unified report\",\n)\n\nresult = await session.flow(builder.get_graph())\n</code></pre>"}]}