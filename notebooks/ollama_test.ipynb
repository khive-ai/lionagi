{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "\n",
    "client = AsyncOpenAI(\n",
    "    base_url=\"http://localhost:11434/v1/\",\n",
    "    # required but ignored\n",
    "    api_key=\"ollama\",\n",
    ")\n",
    "\n",
    "\n",
    "class TestModel(BaseModel):\n",
    "    answer: str\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "i = client.beta.chat.completions.stream(\n",
    "    model=\"llava\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n",
    "    ],\n",
    "    response_format=TestModel,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "async with i as stream:\n",
    "    async for chunk in stream:\n",
    "        results.append(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChunkEvent(type='chunk', chunk=ChatCompletionChunk(id='chatcmpl-425', choices=[Choice(delta=ChoiceDelta(content='{', function_call=None, refusal=None, role='assistant', tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1738171776, model='llava', object='chat.completion.chunk', service_tier=None, system_fingerprint='fp_ollama', usage=None), snapshot=ParsedChatCompletion[object](id='chatcmpl-425', choices=[ParsedChoice[object](finish_reason='stop', index=0, logprobs=None, message=ParsedChatCompletionMessage[object](content='{\\n  \"answer\": \">Paris<\" }\\n ', refusal=None, role='assistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistant', audio=None, function_call=None, tool_calls=None, parsed=TestModel(answer='>Paris<')))], created=1738171776, model='llava', object='chat.completion', service_tier=None, system_fingerprint='fp_ollama', usage=None)),\n",
       " ContentDeltaEvent(type='content.delta', delta='{', snapshot='{', parsed=None),\n",
       " ChunkEvent(type='chunk', chunk=ChatCompletionChunk(id='chatcmpl-425', choices=[Choice(delta=ChoiceDelta(content='\\n', function_call=None, refusal=None, role='assistant', tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1738171776, model='llava', object='chat.completion.chunk', service_tier=None, system_fingerprint='fp_ollama', usage=None), snapshot=ParsedChatCompletion[object](id='chatcmpl-425', choices=[ParsedChoice[object](finish_reason='stop', index=0, logprobs=None, message=ParsedChatCompletionMessage[object](content='{\\n  \"answer\": \">Paris<\" }\\n ', refusal=None, role='assistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistant', audio=None, function_call=None, tool_calls=None, parsed=TestModel(answer='>Paris<')))], created=1738171776, model='llava', object='chat.completion', service_tier=None, system_fingerprint='fp_ollama', usage=None)),\n",
       " ContentDeltaEvent(type='content.delta', delta='\\n', snapshot='{\\n', parsed={}),\n",
       " ChunkEvent(type='chunk', chunk=ChatCompletionChunk(id='chatcmpl-425', choices=[Choice(delta=ChoiceDelta(content=' ', function_call=None, refusal=None, role='assistant', tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1738171776, model='llava', object='chat.completion.chunk', service_tier=None, system_fingerprint='fp_ollama', usage=None), snapshot=ParsedChatCompletion[object](id='chatcmpl-425', choices=[ParsedChoice[object](finish_reason='stop', index=0, logprobs=None, message=ParsedChatCompletionMessage[object](content='{\\n  \"answer\": \">Paris<\" }\\n ', refusal=None, role='assistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistant', audio=None, function_call=None, tool_calls=None, parsed=TestModel(answer='>Paris<')))], created=1738171776, model='llava', object='chat.completion', service_tier=None, system_fingerprint='fp_ollama', usage=None)),\n",
       " ContentDeltaEvent(type='content.delta', delta=' ', snapshot='{\\n ', parsed={}),\n",
       " ChunkEvent(type='chunk', chunk=ChatCompletionChunk(id='chatcmpl-425', choices=[Choice(delta=ChoiceDelta(content=' \"', function_call=None, refusal=None, role='assistant', tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1738171776, model='llava', object='chat.completion.chunk', service_tier=None, system_fingerprint='fp_ollama', usage=None), snapshot=ParsedChatCompletion[object](id='chatcmpl-425', choices=[ParsedChoice[object](finish_reason='stop', index=0, logprobs=None, message=ParsedChatCompletionMessage[object](content='{\\n  \"answer\": \">Paris<\" }\\n ', refusal=None, role='assistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistant', audio=None, function_call=None, tool_calls=None, parsed=TestModel(answer='>Paris<')))], created=1738171776, model='llava', object='chat.completion', service_tier=None, system_fingerprint='fp_ollama', usage=None)),\n",
       " ContentDeltaEvent(type='content.delta', delta=' \"', snapshot='{\\n  \"', parsed={}),\n",
       " ChunkEvent(type='chunk', chunk=ChatCompletionChunk(id='chatcmpl-425', choices=[Choice(delta=ChoiceDelta(content='answer', function_call=None, refusal=None, role='assistant', tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1738171776, model='llava', object='chat.completion.chunk', service_tier=None, system_fingerprint='fp_ollama', usage=None), snapshot=ParsedChatCompletion[object](id='chatcmpl-425', choices=[ParsedChoice[object](finish_reason='stop', index=0, logprobs=None, message=ParsedChatCompletionMessage[object](content='{\\n  \"answer\": \">Paris<\" }\\n ', refusal=None, role='assistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistant', audio=None, function_call=None, tool_calls=None, parsed=TestModel(answer='>Paris<')))], created=1738171776, model='llava', object='chat.completion', service_tier=None, system_fingerprint='fp_ollama', usage=None)),\n",
       " ContentDeltaEvent(type='content.delta', delta='answer', snapshot='{\\n  \"answer', parsed={}),\n",
       " ChunkEvent(type='chunk', chunk=ChatCompletionChunk(id='chatcmpl-425', choices=[Choice(delta=ChoiceDelta(content='\":', function_call=None, refusal=None, role='assistant', tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1738171776, model='llava', object='chat.completion.chunk', service_tier=None, system_fingerprint='fp_ollama', usage=None), snapshot=ParsedChatCompletion[object](id='chatcmpl-425', choices=[ParsedChoice[object](finish_reason='stop', index=0, logprobs=None, message=ParsedChatCompletionMessage[object](content='{\\n  \"answer\": \">Paris<\" }\\n ', refusal=None, role='assistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistant', audio=None, function_call=None, tool_calls=None, parsed=TestModel(answer='>Paris<')))], created=1738171776, model='llava', object='chat.completion', service_tier=None, system_fingerprint='fp_ollama', usage=None)),\n",
       " ContentDeltaEvent(type='content.delta', delta='\":', snapshot='{\\n  \"answer\":', parsed={}),\n",
       " ChunkEvent(type='chunk', chunk=ChatCompletionChunk(id='chatcmpl-425', choices=[Choice(delta=ChoiceDelta(content=' ', function_call=None, refusal=None, role='assistant', tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1738171776, model='llava', object='chat.completion.chunk', service_tier=None, system_fingerprint='fp_ollama', usage=None), snapshot=ParsedChatCompletion[object](id='chatcmpl-425', choices=[ParsedChoice[object](finish_reason='stop', index=0, logprobs=None, message=ParsedChatCompletionMessage[object](content='{\\n  \"answer\": \">Paris<\" }\\n ', refusal=None, role='assistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistant', audio=None, function_call=None, tool_calls=None, parsed=TestModel(answer='>Paris<')))], created=1738171776, model='llava', object='chat.completion', service_tier=None, system_fingerprint='fp_ollama', usage=None)),\n",
       " ContentDeltaEvent(type='content.delta', delta=' ', snapshot='{\\n  \"answer\": ', parsed={}),\n",
       " ChunkEvent(type='chunk', chunk=ChatCompletionChunk(id='chatcmpl-425', choices=[Choice(delta=ChoiceDelta(content='\">', function_call=None, refusal=None, role='assistant', tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1738171776, model='llava', object='chat.completion.chunk', service_tier=None, system_fingerprint='fp_ollama', usage=None), snapshot=ParsedChatCompletion[object](id='chatcmpl-425', choices=[ParsedChoice[object](finish_reason='stop', index=0, logprobs=None, message=ParsedChatCompletionMessage[object](content='{\\n  \"answer\": \">Paris<\" }\\n ', refusal=None, role='assistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistant', audio=None, function_call=None, tool_calls=None, parsed=TestModel(answer='>Paris<')))], created=1738171776, model='llava', object='chat.completion', service_tier=None, system_fingerprint='fp_ollama', usage=None)),\n",
       " ContentDeltaEvent(type='content.delta', delta='\">', snapshot='{\\n  \"answer\": \">', parsed={}),\n",
       " ChunkEvent(type='chunk', chunk=ChatCompletionChunk(id='chatcmpl-425', choices=[Choice(delta=ChoiceDelta(content='Par', function_call=None, refusal=None, role='assistant', tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1738171776, model='llava', object='chat.completion.chunk', service_tier=None, system_fingerprint='fp_ollama', usage=None), snapshot=ParsedChatCompletion[object](id='chatcmpl-425', choices=[ParsedChoice[object](finish_reason='stop', index=0, logprobs=None, message=ParsedChatCompletionMessage[object](content='{\\n  \"answer\": \">Paris<\" }\\n ', refusal=None, role='assistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistant', audio=None, function_call=None, tool_calls=None, parsed=TestModel(answer='>Paris<')))], created=1738171776, model='llava', object='chat.completion', service_tier=None, system_fingerprint='fp_ollama', usage=None)),\n",
       " ContentDeltaEvent(type='content.delta', delta='Par', snapshot='{\\n  \"answer\": \">Par', parsed={}),\n",
       " ChunkEvent(type='chunk', chunk=ChatCompletionChunk(id='chatcmpl-425', choices=[Choice(delta=ChoiceDelta(content='is', function_call=None, refusal=None, role='assistant', tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1738171776, model='llava', object='chat.completion.chunk', service_tier=None, system_fingerprint='fp_ollama', usage=None), snapshot=ParsedChatCompletion[object](id='chatcmpl-425', choices=[ParsedChoice[object](finish_reason='stop', index=0, logprobs=None, message=ParsedChatCompletionMessage[object](content='{\\n  \"answer\": \">Paris<\" }\\n ', refusal=None, role='assistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistant', audio=None, function_call=None, tool_calls=None, parsed=TestModel(answer='>Paris<')))], created=1738171776, model='llava', object='chat.completion', service_tier=None, system_fingerprint='fp_ollama', usage=None)),\n",
       " ContentDeltaEvent(type='content.delta', delta='is', snapshot='{\\n  \"answer\": \">Paris', parsed={}),\n",
       " ChunkEvent(type='chunk', chunk=ChatCompletionChunk(id='chatcmpl-425', choices=[Choice(delta=ChoiceDelta(content='<', function_call=None, refusal=None, role='assistant', tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1738171776, model='llava', object='chat.completion.chunk', service_tier=None, system_fingerprint='fp_ollama', usage=None), snapshot=ParsedChatCompletion[object](id='chatcmpl-425', choices=[ParsedChoice[object](finish_reason='stop', index=0, logprobs=None, message=ParsedChatCompletionMessage[object](content='{\\n  \"answer\": \">Paris<\" }\\n ', refusal=None, role='assistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistant', audio=None, function_call=None, tool_calls=None, parsed=TestModel(answer='>Paris<')))], created=1738171776, model='llava', object='chat.completion', service_tier=None, system_fingerprint='fp_ollama', usage=None)),\n",
       " ContentDeltaEvent(type='content.delta', delta='<', snapshot='{\\n  \"answer\": \">Paris<', parsed={}),\n",
       " ChunkEvent(type='chunk', chunk=ChatCompletionChunk(id='chatcmpl-425', choices=[Choice(delta=ChoiceDelta(content='\"', function_call=None, refusal=None, role='assistant', tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1738171776, model='llava', object='chat.completion.chunk', service_tier=None, system_fingerprint='fp_ollama', usage=None), snapshot=ParsedChatCompletion[object](id='chatcmpl-425', choices=[ParsedChoice[object](finish_reason='stop', index=0, logprobs=None, message=ParsedChatCompletionMessage[object](content='{\\n  \"answer\": \">Paris<\" }\\n ', refusal=None, role='assistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistant', audio=None, function_call=None, tool_calls=None, parsed=TestModel(answer='>Paris<')))], created=1738171776, model='llava', object='chat.completion', service_tier=None, system_fingerprint='fp_ollama', usage=None)),\n",
       " ContentDeltaEvent(type='content.delta', delta='\"', snapshot='{\\n  \"answer\": \">Paris<\"', parsed={'answer': '>Paris<'}),\n",
       " ChunkEvent(type='chunk', chunk=ChatCompletionChunk(id='chatcmpl-425', choices=[Choice(delta=ChoiceDelta(content=' ', function_call=None, refusal=None, role='assistant', tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1738171776, model='llava', object='chat.completion.chunk', service_tier=None, system_fingerprint='fp_ollama', usage=None), snapshot=ParsedChatCompletion[object](id='chatcmpl-425', choices=[ParsedChoice[object](finish_reason='stop', index=0, logprobs=None, message=ParsedChatCompletionMessage[object](content='{\\n  \"answer\": \">Paris<\" }\\n ', refusal=None, role='assistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistant', audio=None, function_call=None, tool_calls=None, parsed=TestModel(answer='>Paris<')))], created=1738171776, model='llava', object='chat.completion', service_tier=None, system_fingerprint='fp_ollama', usage=None)),\n",
       " ContentDeltaEvent(type='content.delta', delta=' ', snapshot='{\\n  \"answer\": \">Paris<\" ', parsed={'answer': '>Paris<'}),\n",
       " ChunkEvent(type='chunk', chunk=ChatCompletionChunk(id='chatcmpl-425', choices=[Choice(delta=ChoiceDelta(content='}', function_call=None, refusal=None, role='assistant', tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1738171776, model='llava', object='chat.completion.chunk', service_tier=None, system_fingerprint='fp_ollama', usage=None), snapshot=ParsedChatCompletion[object](id='chatcmpl-425', choices=[ParsedChoice[object](finish_reason='stop', index=0, logprobs=None, message=ParsedChatCompletionMessage[object](content='{\\n  \"answer\": \">Paris<\" }\\n ', refusal=None, role='assistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistant', audio=None, function_call=None, tool_calls=None, parsed=TestModel(answer='>Paris<')))], created=1738171776, model='llava', object='chat.completion', service_tier=None, system_fingerprint='fp_ollama', usage=None)),\n",
       " ContentDeltaEvent(type='content.delta', delta='}', snapshot='{\\n  \"answer\": \">Paris<\" }', parsed={'answer': '>Paris<'}),\n",
       " ChunkEvent(type='chunk', chunk=ChatCompletionChunk(id='chatcmpl-425', choices=[Choice(delta=ChoiceDelta(content='\\n', function_call=None, refusal=None, role='assistant', tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1738171776, model='llava', object='chat.completion.chunk', service_tier=None, system_fingerprint='fp_ollama', usage=None), snapshot=ParsedChatCompletion[object](id='chatcmpl-425', choices=[ParsedChoice[object](finish_reason='stop', index=0, logprobs=None, message=ParsedChatCompletionMessage[object](content='{\\n  \"answer\": \">Paris<\" }\\n ', refusal=None, role='assistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistant', audio=None, function_call=None, tool_calls=None, parsed=TestModel(answer='>Paris<')))], created=1738171776, model='llava', object='chat.completion', service_tier=None, system_fingerprint='fp_ollama', usage=None)),\n",
       " ContentDeltaEvent(type='content.delta', delta='\\n', snapshot='{\\n  \"answer\": \">Paris<\" }\\n', parsed={'answer': '>Paris<'}),\n",
       " ChunkEvent(type='chunk', chunk=ChatCompletionChunk(id='chatcmpl-425', choices=[Choice(delta=ChoiceDelta(content=' ', function_call=None, refusal=None, role='assistant', tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1738171776, model='llava', object='chat.completion.chunk', service_tier=None, system_fingerprint='fp_ollama', usage=None), snapshot=ParsedChatCompletion[object](id='chatcmpl-425', choices=[ParsedChoice[object](finish_reason='stop', index=0, logprobs=None, message=ParsedChatCompletionMessage[object](content='{\\n  \"answer\": \">Paris<\" }\\n ', refusal=None, role='assistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistant', audio=None, function_call=None, tool_calls=None, parsed=TestModel(answer='>Paris<')))], created=1738171776, model='llava', object='chat.completion', service_tier=None, system_fingerprint='fp_ollama', usage=None)),\n",
       " ContentDeltaEvent(type='content.delta', delta=' ', snapshot='{\\n  \"answer\": \">Paris<\" }\\n ', parsed={'answer': '>Paris<'}),\n",
       " ChunkEvent(type='chunk', chunk=ChatCompletionChunk(id='chatcmpl-425', choices=[Choice(delta=ChoiceDelta(content='', function_call=None, refusal=None, role='assistant', tool_calls=None), finish_reason='stop', index=0, logprobs=None)], created=1738171776, model='llava', object='chat.completion.chunk', service_tier=None, system_fingerprint='fp_ollama', usage=None), snapshot=ParsedChatCompletion[object](id='chatcmpl-425', choices=[ParsedChoice[object](finish_reason='stop', index=0, logprobs=None, message=ParsedChatCompletionMessage[object](content='{\\n  \"answer\": \">Paris<\" }\\n ', refusal=None, role='assistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistantassistant', audio=None, function_call=None, tool_calls=None, parsed=TestModel(answer='>Paris<')))], created=1738171776, model='llava', object='chat.completion', service_tier=None, system_fingerprint='fp_ollama', usage=None)),\n",
       " ContentDeltaEvent(type='content.delta', delta='', snapshot='{\\n  \"answer\": \">Paris<\" }\\n ', parsed={'answer': '>Paris<'}),\n",
       " ContentDoneEvent[TestModel](type='content.done', content='{\\n  \"answer\": \">Paris<\" }\\n ', parsed=TestModel(answer='>Paris<'))]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParsedChatCompletionMessage[TestModel](content='{ \"answer\": \"Paris\" } ', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[], parsed=TestModel(answer='Paris'))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0].message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Endpoint returned 404: check your route/path.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 46\u001b[0m\n\u001b[1;32m     41\u001b[0m             logging\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAPI call failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     42\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m retry_in()\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "Cell \u001b[0;32mIn[1], line 26\u001b[0m, in \u001b[0;36mretry_in\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m session\u001b[38;5;241m.\u001b[39mpost(full_url, json\u001b[38;5;241m=\u001b[39mpayload) \u001b[38;5;28;01mas\u001b[39;00m response:\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# Check the status first\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m404\u001b[39m:\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;66;03m# Typically means the endpoint doesn't exist\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEndpoint returned 404: check your route/path.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# If the server actually returns JSON,\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m# this will parse it. Otherwise, it may raise ContentTypeError\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     response_json \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m response\u001b[38;5;241m.\u001b[39mjson()\n",
      "\u001b[0;31mValueError\u001b[0m: Endpoint returned 404: check your route/path."
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import logging\n",
    "import aiohttp\n",
    "\n",
    "\n",
    "async def retry_in():\n",
    "    payload = {\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": \"hello\"}],\n",
    "        \"model\": \"llama3.2\",\n",
    "    }\n",
    "\n",
    "    full_url = \"http://localhost:11434/api/chat\"  # <-- corrected\n",
    "\n",
    "    headers = {\n",
    "        \"Authorization\": \"...\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        try:\n",
    "            async with session.post(full_url, json=payload) as response:\n",
    "                # Check the status first\n",
    "                if response.status == 404:\n",
    "                    # Typically means the endpoint doesn't exist\n",
    "                    raise ValueError(\n",
    "                        \"Endpoint returned 404: check your route/path.\"\n",
    "                    )\n",
    "\n",
    "                # If the server actually returns JSON,\n",
    "                # this will parse it. Otherwise, it may raise ContentTypeError\n",
    "                response_json = await response.json()\n",
    "\n",
    "                # If successful and no \"error\" key:\n",
    "                if \"error\" not in response_json:\n",
    "                    return response_json\n",
    "\n",
    "        except asyncio.CancelledError:\n",
    "            logging.warning(\"API call canceled by external request.\")\n",
    "            raise\n",
    "\n",
    "        except aiohttp.ClientError as e:\n",
    "            logging.error(f\"API call failed: {e}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "result = await retry_in()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await retry_in()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The capital of France is Paris. \n"
     ]
    }
   ],
   "source": [
    "from lionagi import iModel\n",
    "\n",
    "from lionagi.service.endpoints.chat_completion import (\n",
    "    EndPoint,\n",
    "    ChatCompletionEndPoint,\n",
    ")\n",
    "\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "client = AsyncOpenAI(\n",
    "    base_url=\"http://localhost:11434/v1/\",\n",
    "    # required but ignored\n",
    "    api_key=\"ollama\",\n",
    ")\n",
    "\n",
    "response = await client.chat.completions.create(\n",
    "    model=\"llava\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=' The capital of France is Paris. ', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lionagi import iModel\n",
    "\n",
    "# gpt4o = iModel(\n",
    "#     provider=\"openai\",\n",
    "#     model=\"gpt-4o\",\n",
    "#     # base_url=\"http://localhost:11434/api\",\n",
    "#     endpoint=\"chat\",\n",
    "#     # requires_api_key=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# api_call = gpt4o.create_api_calling(messages=messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OllamaChatCompletionEndPoint(ChatCompletionEndPoint):\n",
    "\n",
    "    async def _invoke(\n",
    "        self,\n",
    "    ): ...\n",
    "\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAT_COMPLETION_CONFIG = {\n",
    "    \"provider\": \"ollama\",\n",
    "    \"base_url\": \"http://localhost:11434/v1/api\",\n",
    "    \"endpoint\": \"chat\",\n",
    "    \"method\": \"post\",\n",
    "    \"openai_compatible\": True,\n",
    "    \"is_invokeable\": True,\n",
    "    \"requires_tokens\": True,\n",
    "    \"is_streamable\": True,\n",
    "    \"requires_api_key\": False,\n",
    "    \"required_kwargs\": {\n",
    "        \"messages\",\n",
    "        \"model\",\n",
    "    },\n",
    "    \"deprecated_kwargs\": {\n",
    "        \"max_tokens\",\n",
    "        \"function_call\",\n",
    "        \"functions\",\n",
    "    },\n",
    "    \"optional_kwargs\": {\n",
    "        \"store\",\n",
    "        \"reasoning_effort\",\n",
    "        \"metadata\",\n",
    "        \"frequency_penalty\",\n",
    "        \"logit_bias\",\n",
    "        \"logprobs\",\n",
    "        \"top_logprobs\",\n",
    "        \"max_completion_tokens\",\n",
    "        \"n\",\n",
    "        \"modalities\",\n",
    "        \"prediction\",\n",
    "        \"audio\",\n",
    "        \"presence_penalty\",\n",
    "        \"response_format\",\n",
    "        \"seed\",\n",
    "        \"service_tier\",\n",
    "        \"stop\",\n",
    "        \"stream\",\n",
    "        \"stream_options\",\n",
    "        \"temperature\",\n",
    "        \"top_p\",\n",
    "        \"tools\",\n",
    "        \"tool_choice\",\n",
    "        \"parallel_tool_calls\",\n",
    "        \"user\",\n",
    "    },\n",
    "    \"allowed_roles\": [\"user\", \"assistant\", \"system\", \"developer\", \"tool\"],\n",
    "}\n",
    "\n",
    "\n",
    "ollama_endpoint = ChatCompletionEndPoint(CHAT_COMPLETION_CONFIG)\n",
    "\n",
    "ollama = iModel(\n",
    "    provider=\"ollama\",\n",
    "    model=\"deepseek-r1:1.5b\",\n",
    "    endpoint=ollama_endpoint,\n",
    "    # invoke_with_endpoint=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_call = ollama.create_api_calling(messages=messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [{'role': 'user', 'content': 'What is the capital of France?'}],\n",
       " 'model': 'deepseek-r1:1.5b'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_call.payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:API call failed: 404, message='Attempt to decode JSON with unexpected mimetype: text/plain', url='http://localhost:11434/v1/api/chat'\n"
     ]
    }
   ],
   "source": [
    "result = await api_call.invoke()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
