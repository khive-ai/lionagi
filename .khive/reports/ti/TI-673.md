---
title: Lionagi Refactoring Test Information (Issue #673)
by: khive-implementer
created: 2025-05-19
updated: 2025-05-19
version: 1.0
doc_type: TI
output_subdir: ti
description: Test Information for the refactoring of the lionagi package (Issue #673).
date: 2025-05-19
author: "@khive-implementer"
status: Draft
related_issue: 673
research_report: RR-673.md
technical_design: TDS-673.md
implementation_plan: IP-673.md
---

# Test Information: `lionagi` Package Refactoring

## 1. Overview

This document outlines the testing strategy for the refactored `lionagi` package as specified in the Technical Design Specification (TDS-673) and Implementation Plan (IP-673). The testing approach follows Test-Driven Development (TDD) principles and aims to ensure that the refactored package meets all requirements with high test coverage.

## 2. Testing Approach

### 2.1 Test-Driven Development (TDD)

The implementation will follow the TDD cycle:
1. **Red**: Write failing tests for the new functionality
2. **Green**: Implement the functionality to make the tests pass
3. **Refactor**: Improve the code while maintaining passing tests

### 2.2 Test Coverage Goals

- Overall test coverage: >80%
- Core components: >90%
- Public API: 100%
- Error handling: >85%

### 2.3 Testing Tools

- **pytest**: Primary testing framework
- **pytest-cov**: For measuring test coverage
- **pytest-asyncio**: For testing asynchronous code
- **unittest.mock**: For mocking dependencies
- **hypothesis**: For property-based testing where appropriate

## 3. Test Categories

### 3.1 Unit Tests

Unit tests will focus on testing individual components in isolation with mocked dependencies.

#### 3.1.1 Models Tests

- **Base Models**:
  - Test initialization with valid and invalid data
  - Test validation rules
  - Test default values
  - Test model methods

- **Pydapter Integration**:
  - Test `Adaptable` mixin functionality
  - Test serialization to various formats (JSON, TOML, etc.)
  - Test deserialization from various formats
  - Test async adaptation methods

#### 3.1.2 Core Components Tests

- **Orchestrator**:
  - Test conversation flow management
  - Test message processing
  - Test tool call handling
  - Test error handling

- **State Manager**:
  - Test session creation and management
  - Test branch creation and switching
  - Test state persistence and retrieval
  - Test error handling

- **Service Interface**:
  - Test request preparation
  - Test response processing
  - Test error handling
  - Test integration with `lionfuncs.network.adapters`

- **Tool Manager**:
  - Test tool registration
  - Test schema generation
  - Test tool execution
  - Test error handling

#### 3.1.3 Public API Tests

- Test all public API functions and classes
- Test with various input combinations
- Test error cases
- Test configuration options

### 3.2 Integration Tests

Integration tests will focus on testing the interaction between components.

#### 3.2.1 Component Integration Tests

- Test interaction between Orchestrator and State Manager
- Test interaction between Orchestrator and Service Interface
- Test interaction between Orchestrator and Tool Manager
- Test interaction between Service Interface and `lionfuncs.network.adapters`

#### 3.2.2 End-to-End Workflow Tests

- Test complete chat workflow
- Test chat with tool use workflow
- Test branch management workflow
- Test error handling in workflows

### 3.3 Regression Tests

- Test that key functionality from the original `lionagi` is preserved where appropriate
- Test that known edge cases are handled correctly

## 4. Test Implementation Details

### 4.1 Directory Structure

```
tests/
├── __init__.py
├── conftest.py  # Common fixtures
│
├── unit/
│   ├── __init__.py
│   │
│   ├── models/
│   │   ├── __init__.py
│   │   ├── test_base.py
│   │   ├── test_message.py
│   │   ├── test_session.py
│   │   └── ...
│   │
│   ├── core/
│   │   ├── __init__.py
│   │   ├── test_orchestrator.py
│   │   ├── test_state_manager.py
│   │   ├── test_service_interface.py
│   │   ├── test_tool_manager.py
│   │   └── test_utils.py
│   │
│   └── tools/
│       ├── __init__.py
│       └── test_web_search_tool.py
│
├── integration/
│   ├── __init__.py
│   ├── test_component_interactions.py
│   └── test_workflows.py
│
└── regression/
    ├── __init__.py
    └── test_regression.py
```

### 4.2 Test Fixtures

#### 4.2.1 Model Fixtures

```python
@pytest.fixture
def message_data():
    return {
        "role": "user",
        "content": "Hello, world!",
        "metadata": {"source": "test"}
    }

@pytest.fixture
def message(message_data):
    from lionagi.models.message import Message
    return Message(**message_data)

@pytest.fixture
def branch_with_messages():
    from lionagi.models.message import Message
    from lionagi.models.session import Branch
    
    messages = [
        Message(role="user", content="Hello"),
        Message(role="assistant", content="Hi there!")
    ]
    
    return Branch(messages=messages)

@pytest.fixture
def session_with_branches(branch_with_messages):
    from lionagi.models.session import Session
    
    branch_id = branch_with_messages.id
    branches = {branch_id: branch_with_messages}
    
    return Session(branches=branches, current_branch_id=branch_id)
```

#### 4.2.2 Mock Fixtures

```python
@pytest.fixture
def mock_lionfuncs_network_adapter():
    with patch("lionfuncs.network.adapters.OpenAIAdapter") as mock_adapter:
        mock_adapter.call_llm.return_value = {
            "choices": [
                {
                    "message": {
                        "role": "assistant",
                        "content": "This is a mock response."
                    }
                }
            ]
        }
        yield mock_adapter

@pytest.fixture
def mock_tool_function():
    async def tool_func(query: str) -> str:
        return f"Result for: {query}"
    
    return tool_func
```

### 4.3 Example Test Cases

#### 4.3.1 Model Test Example

```python
def test_message_creation(message_data):
    from lionagi.models.message import Message
    
    message = Message(**message_data)
    
    assert message.role == message_data["role"]
    assert message.content == message_data["content"]
    assert message.metadata == message_data["metadata"]
    assert message.id is not None
    assert message.timestamp is not None

def test_message_adaptation(message):
    # Test adaptation to JSON
    json_data = message.adapt_to(obj_key="json")
    assert isinstance(json_data, str)
    
    # Test adaptation from JSON
    from lionagi.models.message import Message
    loaded_message = Message.adapt_from(json_data, obj_key="json")
    
    assert loaded_message.id == message.id
    assert loaded_message.role == message.role
    assert loaded_message.content == message.content
```

#### 4.3.2 Core Component Test Example

```python
async def test_orchestrator_process_message(session_with_branches, mock_lionfuncs_network_adapter):
    from lionagi.core.orchestrator import Orchestrator
    from lionagi.core.service_interface import ServiceInterface
    
    service_interface = ServiceInterface(adapter=mock_lionfuncs_network_adapter)
    orchestrator = Orchestrator(service_interface=service_interface)
    
    result = await orchestrator.process_message(
        session=session_with_branches,
        message_content="Hello, AI!",
        message_role="user"
    )
    
    # Check that a new message was added to the current branch
    current_branch = session_with_branches.branches[session_with_branches.current_branch_id]
    assert len(current_branch.messages) == 3  # Original 2 + new user message
    
    # Check that the new message has the correct content
    assert current_branch.messages[-1].role == "user"
    assert current_branch.messages[-1].content == "Hello, AI!"
    
    # Check that the result is the assistant's response
    assert result.role == "assistant"
    assert result.content == "This is a mock response."
```

#### 4.3.3 Tool Test Example

```python
async def test_tool_execution(mock_tool_function):
    from lionagi.core.tool_manager import ToolManager
    from lionagi.models.message import ToolCallRequest
    
    # Create a tool manager and register the tool
    tool_manager = ToolManager()
    tool_manager.register_tool("search", mock_tool_function)
    
    # Create a tool call request
    tool_call = ToolCallRequest(
        function_name="search",
        arguments={"query": "test query"}
    )
    
    # Execute the tool
    result = await tool_manager.execute_tool_call(tool_call)
    
    # Check the result
    assert result.request_id == tool_call.id
    assert result.result == "Result for: test query"
    assert not result.is_error
```

## 5. Test Execution Strategy

### 5.1 Continuous Testing During Development

- Run unit tests for each component as it's developed
- Run integration tests as components are integrated
- Monitor test coverage and add tests for uncovered code paths

### 5.2 Pre-commit Testing

- Run linters and formatters
- Run all tests to ensure nothing is broken
- Check test coverage

### 5.3 CI/CD Integration

- Configure CI to run tests on each commit
- Configure CI to check test coverage
- Configure CI to run linters and formatters

## 6. Test Data Management

### 6.1 Test Data Sources

- Synthetic data generated within tests
- Fixtures for common test scenarios
- Mock responses for external services

### 6.2 Test Data Privacy

- No real API keys or sensitive data in tests
- Use environment variables or configuration files for any external service configuration

## 7. Conclusion

This test information document outlines a comprehensive testing strategy for the refactored `lionagi` package. By following this strategy and implementing the described tests, we will ensure that the refactored package meets all requirements with high test coverage and maintains high quality.
